<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv AI Security Papers</title>
    <style>
        body { font-family: Verdana, sans-serif; margin: 0; padding: 0; background-color: #eee; }
        .container { max-width: 950px; margin: 0 auto; padding: 20px; background-color: #fff; }
        .header { background-color: #cee3f8; border-bottom: 1px solid #5f99cf; padding: 10px 20px; margin-bottom: 20px; }
        .header h1 { margin: 0; font-size: 20px; color: #369; }
        .filter-options { display: flex; justify-content: space-between; margin-bottom: 20px; padding: 10px; background-color: #f8f8f8; border: 1px solid #ddd; }
        .filter-options button { color: #369; cursor: pointer; padding: 5px 10px; background: none; border: none; }
        .filter-options button.active { font-weight: bold; background-color: #e2e2e2; border-radius: 3px; }
        .paper-row { padding: 10px; border-bottom: 1px solid #ddd; line-height: 1.4; }
        .paper-main { display: flex; align-items: center; cursor: pointer; }
        .rank { flex: 0 0 30px; color: #888; text-align: right; padding-right: 10px; font-size: 18px; }
        .votes { flex: 0 0 70px; text-align: center; padding: 0 10px; display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .votes strong { color: #1DA1F2; font-size: 15px; }
        .votes a { color: inherit; transition: transform 0.2s; display: flex; flex-direction: column; align-items: center; }
        .votes a:hover { transform: scale(1.1); }
        .paper-content { flex: 1; display: flex; flex-direction: column; }
        .paper-title { color: #0000ff; text-decoration: none; font-weight: bold; font-size: 16px; }
        .paper-meta { color: #888; font-size: 12px; margin-top: 4px; }
        .paper-details { margin-top: 10px; padding: 10px; background-color: #f9f9f9; border-radius: 5px; display: none; }
        .paper-details.show { display: block; }
        .abstract { font-size: 14px; line-height: 1.5; margin-top: 10px; white-space: pre-line; }
        .footer { text-align: center; margin-top: 20px; font-size: 12px; color: #888; padding: 10px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ArXiv AI Security Papers</h1>
        </div>
        <div class="filter-options">
            <div>
                <button onclick="sortPapers('date')" id="sort-date">Recent</button>
                <button onclick="sortPapers('citations')" id="sort-citations">Most Cited</button>
                <button onclick="sortPapers('tweets')" class="active" id="sort-tweets">Most Tweeted</button>
            </div>
            <div>
                <button onclick="expandAll()" id="expand-all">Expand All</button>
                <button onclick="collapseAll()" id="collapse-all">Collapse All</button>
                <a href="list.html" style="margin-left: 20px; background-color: #5f99cf; color: white; padding: 5px 10px; text-decoration: none; border-radius: 3px;">See Previous Renders</a>
            </div>
        </div>
        
        <div id="papers-container"></div>
        
        <div class="footer">
            Generated on 2025-04-14 12:10:36 | Database contains 95 papers total.
            <div style="margin-top: 10px;">
                <a href="list.html" style="color: #369; text-decoration: none;">View render history</a>
            </div>
        </div>
    </div>
    
    <script>
    const papers = [{"arxiv_id": "2503.16426", "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding", "authors": ["Keyan Chen", "Chenyang Liu", "Bowen Chen", "Wenyuan Li", "Zhengxia Zou", "Zhenwei Shi"], "published": "2025-03-20T17:59:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16426v1", "categories": ["cs.CV"], "abstract": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's)."}, {"arxiv_id": "2503.16419", "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", " Shaochen", " Zhong", "Hanjie Chen", "Xia Hu"], "published": "2025-03-20T17:59:38Z", "citations": 0, "tweets": 15, "paper_link": "http://arxiv.org/abs/2503.16419v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."}, {"arxiv_id": "2503.16418", "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity", "authors": ["Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Hao Kang", "Xin Lu"], "published": "2025-03-20T17:59:34Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2503.16418v1", "categories": ["cs.CV", "cs.LG"], "abstract": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community."}, {"arxiv_id": "2503.16416", "title": "Survey on Evaluation of LLM-based Agents", "authors": ["Asaf Yehudai", "Lilach Eden", "Alan Li", "Guy Uziel", "Yilun Zhao", "Roy Bar-Haim", "Arman Cohan", "Michal Shmueli-Scheuer"], "published": "2025-03-20T17:59:23Z", "citations": 0, "tweets": 12, "paper_link": "http://arxiv.org/abs/2503.16416v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch."}, {"arxiv_id": "2503.16412", "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation", "authors": ["Ananta R. Bhattarai", "Xingzhe He", "Alla Sheffer", "Helge Rhodin"], "published": "2025-03-20T17:59:12Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16412v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation."}, {"arxiv_id": "2503.16413", "title": "M3: 3D-Spatial MultiModal Memory", "authors": ["Xueyan Zou", "Yuchen Song", "Ri-Zhao Qiu", "Xuanbin Peng", "Jianglong Ye", "Sifei Liu", "Xiaolong Wang"], "published": "2025-03-20T17:59:12Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16413v1", "categories": ["cs.CV", "cs.RO"], "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation."}, {"arxiv_id": "2503.16402", "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination", "authors": ["Yifan Sun", "Han Wang", "Dongbai Li", "Gang Wang", "Huan Zhang"], "published": "2025-03-20T17:55:04Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16402v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment."}, {"arxiv_id": "2503.16401", "title": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them", "authors": ["Guanyu Chen", "Peiyang Wang", "Tianren Zhang", "Feng Chen"], "published": "2025-03-20T17:54:42Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.16401v1", "categories": ["cs.LG"], "abstract": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning."}, {"arxiv_id": "2503.16399", "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World", "authors": ["Chen Chen", "Zhirui Wang", "Taowei Sheng", "Yi Jiang", "Yundu Li", "Peirui Cheng", "Luning Zhang", "Kaiqiang Chen", "Yanfeng Hu", "Xue Yang", "Xian Sun"], "published": "2025-03-20T17:54:29Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16399v1", "categories": ["cs.CV", "cs.AI"], "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ."}, {"arxiv_id": "2503.16392", "title": "Graph of Effort: Quantifying Risk of AI Usage for Vulnerability\n  Assessment", "authors": ["Anket Mehra", "Andreas Aßmuth", "Malte Prieß"], "published": "2025-03-20T17:52:42Z", "citations": 0, "tweets": 7, "paper_link": "http://arxiv.org/abs/2503.16392v1", "categories": ["cs.CR", "cs.AI", "cs.DC"], "abstract": "With AI-based software becoming widely available, the risk of exploiting its\ncapabilities, such as high automation and complex pattern recognition, could\nsignificantly increase. An AI used offensively to attack non-AI assets is\nreferred to as offensive AI.\n  Current research explores how offensive AI can be utilized and how its usage\ncan be classified. Additionally, methods for threat modeling are being\ndeveloped for AI-based assets within organizations. However, there are gaps\nthat need to be addressed. Firstly, there is a need to quantify the factors\ncontributing to the AI threat. Secondly, there is a requirement to create\nthreat models that analyze the risk of being attacked by AI for vulnerability\nassessment across all assets of an organization. This is particularly crucial\nand challenging in cloud environments, where sophisticated infrastructure and\naccess control landscapes are prevalent. The ability to quantify and further\nanalyze the threat posed by offensive AI enables analysts to rank\nvulnerabilities and prioritize the implementation of proactive countermeasures.\n  To address these gaps, this paper introduces the Graph of Effort, an\nintuitive, flexible, and effective threat modeling method for analyzing the\neffort required to use offensive AI for vulnerability exploitation by an\nadversary. While the threat model is functional and provides valuable support,\nits design choices need further empirical validation in future work."}, {"arxiv_id": "2503.16390", "title": "Phonons in Electron Crystals with Berry Curvature", "authors": ["Junkai Dong", "Ophelia Evelyn Sommer", "Tomohiro Soejima", "Daniel E. Parker", "Ashvin Vishwanath"], "published": "2025-03-20T17:49:59Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16390v1", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "abstract": "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology."}, {"arxiv_id": "2503.16385", "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation", "authors": ["Yijia Luo", "Yulin Song", "Xingyao Zhang", "Jiaheng Liu", "Weixun Wang", "GengRu Chen", "Wenbo Su", "Bo Zheng"], "published": "2025-03-20T17:46:38Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16385v1", "categories": ["cs.AI"], "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."}, {"arxiv_id": "2503.16376", "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images", "authors": ["Leyang Wang", "Joice Lin"], "published": "2025-03-20T17:39:06Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16376v1", "categories": ["cs.CV"], "abstract": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG."}, {"arxiv_id": "2503.16366", "title": "Dynamic Metasurface-Backed Luneburg Lens for Multiplexed Backscatter\n  Communication", "authors": ["Samuel Kim", "Tim Sleasman", "Avrami Rakovsky", "Ra'id Awadallah", "David B. Shrekenhamer"], "published": "2025-03-20T17:22:11Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.16366v1", "categories": ["physics.optics", "eess.SP"], "abstract": "Backscatter communications is attractive for its low power requirements due\nto the lack of actively radiating components; however, commonly used devices\nare typically limited in range and functionality. Here, we design and\ndemonstrate a flattened Luneburg lens combined with a spatially-tunable dynamic\nmetasurface to create a low-power backscatter communicator. The Luneburg lens\nis a spherically-symmetric lens that focuses a collimated beam from any\ndirection, enabling a wide field-of-view with no aberrations. By applying\nquasi-conformal transformation optics (QCTO), we design a flattened Luneburg\nlens to facilitate its seamless interface with the planar metasurface. The\ngradient index of the Luneburg lens is realized through additive manufacturing.\nWe show that the flattened Luneburg lens with a reflective surface at the\nflattened focal plane is able to achieve diffraction-limited retroreflection,\nenabling long-range backscatter communication. When an interrogator transmits\ntowards the metasurface-backed Luneburg lens, the device can modulate the\nreflected signal phase across a wide field of regard to communicate data. We\nexperimentally show that the spatial control over the metasurface allows\ndifferent bit streams to be simultaneously communicated in different\ndirections. Additionally, we show that the device is able to prevent\neavesdroppers from receiving information, thus securing communications."}, {"arxiv_id": "2503.16356", "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners", "authors": ["Yunzhi Yao", "Jizhan Fang", "Jia-Chen Gu", "Ningyu Zhang", "Shumin Deng", "Huajun Chen", "Nanyun Peng"], "published": "2025-03-20T17:14:34Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16356v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."}, {"arxiv_id": "2503.16350", "title": "An Evaluation Tool for Backbone Extraction Techniques in Weighted\n  Complex Networks", "authors": ["Ali Yassin", "Abbas Haidar", "Hocine Cherifi", "Hamida Seba", "Olivier Togni"], "published": "2025-03-20T17:08:35Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16350v1", "categories": ["cs.SI", "cs.SE"], "abstract": "Networks are essential for analyzing complex systems. However, their growing\nsize necessitates backbone extraction techniques aimed at reducing their size\nwhile retaining critical features. In practice, selecting, implementing, and\nevaluating the most suitable backbone extraction method may be challenging.\nThis paper introduces netbone, a Python package designed for assessing the\nperformance of backbone extraction techniques in weighted networks. Its\ncomparison framework is the standout feature of netbone. Indeed, the tool\nincorporates state-of-the-art backbone extraction techniques. Furthermore, it\nprovides a comprehensive suite of evaluation metrics allowing users to evaluate\ndifferent backbones techniques. We illustrate the flexibility and effectiveness\nof netbone through the US air transportation network analysis. We compare the\nperformance of different backbone extraction techniques using the evaluation\nmetrics. We also show how users can integrate a new backbone extraction method\ninto the comparison framework. netbone is publicly available as an open-source\ntool, ensuring its accessibility to researchers and practitioners. Promoting\nstandardized evaluation practices contributes to the advancement of backbone\nextraction techniques and fosters reproducibility and comparability in research\nefforts. We anticipate that netbone will serve as a valuable resource for\nresearchers and practitioners enabling them to make informed decisions when\nselecting backbone extraction techniques to gain insights into the structural\nand functional properties of complex systems."}, {"arxiv_id": "2503.16345", "title": "Finite-coupling spectrum of O(N) model in AdS", "authors": ["Jonáš Dujava", "Petr Vaško"], "published": "2025-03-20T17:01:22Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16345v1", "categories": ["hep-th"], "abstract": "We determine the scaling dimensions in the boundary $\\mathsf{CFT}_{d}$\ncorresponding to the $\\mathsf{O}(N)$ model in $\\mathsf{EAdS}_{d+1}$. The\n$\\mathsf{CFT}$ data accessible to the 4-point boundary correlator of\nfundamental fields are extracted in $d=2$ and $d=4$, at a finite coupling, and\nto the leading nontrivial order in the $1/N$ expansion. We focus on the\nnon-singlet sectors, namely the anti-symmetric and symmetric traceless\nirreducible representations of the $\\mathsf{O}(N)$ group, extending the\nprevious results that considered only the singlet sector. Studying the\nnon-singlet sector requires an understanding of the crossed-channel diagram\ncontributions to the $s$-channel conformal block decomposition. Building upon\nan existing computation, we present general formulas in $d=2$ and $d=4$ for the\ncontribution of a $t$-channel conformal block to the anomalous dimensions of\n$s$-channel double-twist operators, derived for external scalar operators with\nequal scaling dimensions. Up to some technical details, this eventually leads\nto the complete picture of $1/N$ corrections to the $\\mathsf{CFT}$ data in the\ninteracting theory."}, {"arxiv_id": "2503.16335", "title": "Enhancing Software Quality Assurance with an Adaptive Differential\n  Evolution based Quantum Variational Autoencoder-Transformer Model", "authors": ["Seshu Babu Barma", "Mohanakrishnan Hariharan", "Satish Arvapalli"], "published": "2025-03-20T16:55:38Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16335v1", "categories": ["cs.AI", "cs.ET"], "abstract": "An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%."}, {"arxiv_id": "2503.16334", "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "authors": ["Ying Shen", "Lifu Huang"], "published": "2025-03-20T16:55:26Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16334v1", "categories": ["cs.CL"], "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."}, {"arxiv_id": "2503.16326", "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence", "authors": ["Long Yuan", "Fengran Mo", "Kaiyu Huang", "Wenjie Wang", "Wangyuxuan Zhai", "Xiaoyu Zhu", "You Li", "Jinan Xu", "Jian-Yun Nie"], "published": "2025-03-20T16:45:48Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16326v1", "categories": ["cs.AI"], "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication."}, {"arxiv_id": "2503.17363", "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural\n  Language Self-Critique", "authors": ["Yansi Li", "Jiahao Xu", "Tian Liang", "Xingyu Chen", "Zhiwei He", "Qiuzhi Liu", "Rui Wang", "Zhuosheng Zhang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published": "2025-03-21T17:59:55Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2503.17363v1", "categories": ["cs.CL"], "abstract": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field."}, {"arxiv_id": "2503.17357", "title": "Filtered Rayleigh-Ritz is all you need", "authors": ["Ryan Abbott", "Daniel C. Hackett", "George T. Fleming", "Dimitra A. Pefkou", "Michael L. Wagman"], "published": "2025-03-21T17:58:21Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.17357v1", "categories": ["hep-lat", "cs.NA", "math.NA"], "abstract": "Recent work has shown that the (block) Lanczos algorithm can be used to\nextract approximate energy spectra and matrix elements from (matrices of)\ncorrelation functions in quantum field theory, and identified exact\ncoincidences between Lanczos analysis methods and others. In this work, we note\nanother coincidence: the Lanczos algorithm is equivalent to the well-known\nRayleigh-Ritz method applied to Krylov subspaces. Rayleigh-Ritz provides\noptimal eigenvalue approximations within subspaces; we find that spurious-state\nfiltering allows these optimality guarantees to be retained in the presence of\nstatistical noise. We explore the relation between Lanczos and Prony's method,\ntheir block generalizations, generalized pencil of functions (GPOF), and\nmethods based on the generalized eigenvalue problem (GEVP), and find they all\nfall into a larger \"Prony-Ritz equivalence class\", identified as all methods\nwhich solve a finite-dimensional spectrum exactly given sufficient correlation\nfunction (matrix) data. This equivalence allows simpler and more numerically\nstable implementations of (block) Lanczos analyses."}, {"arxiv_id": "2503.17352", "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "published": "2025-03-21T17:52:43Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.17352v1", "categories": ["cs.CV", "cs.CL"], "abstract": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker."}, {"arxiv_id": "2503.17351", "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging\n  Photoplethysmography", "authors": ["Vineet R. Shenoy", "Shaoju Wu", "Armand Comas", "Tim K. Marks", "Suhas Lohit", "Hassan Mansour"], "published": "2025-03-21T17:52:33Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.17351v1", "categories": ["cs.CV"], "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."}, {"arxiv_id": "2503.17349", "title": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language\n  Models", "authors": ["Jianing Qi", "Jiawei Liu", "Hao Tang", "Zhigang Zhu"], "published": "2025-03-21T17:51:14Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.17349v1", "categories": ["cs.CV"], "abstract": "Vision-Language Models (VLMs) excel at identifying and describing objects but\nstruggle with spatial reasoning such as accurately understanding the relative\npositions of objects. Inspired by the dual-pathway (ventral-dorsal) model of\nhuman vision, we investigate why VLMs fail spatial tasks despite strong object\nrecognition capabilities. Our interpretability-driven analysis reveals a\ncritical underlying cause: vision embeddings in VLMs are treated primarily as\nsemantic ``bag-of-tokens,\" overshadowing subtle yet crucial positional cues due\nto their disproportionately large embedding norms. We validate this insight\nthrough extensive diagnostic experiments, demonstrating minimal performance\nimpact when token orders or fine-grained spatial details are removed. Guided by\nthese findings, we propose simple, interpretable interventions, including\nnormalizing vision embedding norms and extracting mid-layer spatially rich\nfeatures, to restore spatial awareness. Empirical results on both our synthetic\ndata and standard benchmarks demonstrate improved spatial reasoning\ncapabilities, highlighting the value of interpretability-informed design\nchoices. Our study not only uncovers fundamental limitations in current VLM\narchitectures but also provides actionable insights for enhancing structured\nperception of visual scenes."}, {"arxiv_id": "2503.18944", "title": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation", "authors": ["Karim Abou Zeid", "Kadir Yilmaz", "Daan de Geus", "Alexander Hermans", "David Adrian", "Timm Linder", "Bastian Leibe"], "published": "2025-03-24T17:59:11Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.18944v1", "categories": ["cs.CV"], "abstract": "Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets."}, {"arxiv_id": "2503.18943", "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding", "authors": ["Mingze Xu", "Mingfei Gao", "Shiyu Li", "Jiasen Lu", "Zhe Gan", "Zhengfeng Lai", "Meng Cao", "Kai Kang", "Yinfei Yang", "Afshin Dehghan"], "published": "2025-03-24T17:59:07Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.18943v1", "categories": ["cs.CV"], "abstract": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks."}, {"arxiv_id": "2503.18942", "title": "Video-T1: Test-Time Scaling for Video Generation", "authors": ["Fangfu Liu", "Hanyang Wang", "Yimo Cai", "Kaiyan Zhang", "Xiaohang Zhan", "Yueqi Duan"], "published": "2025-03-24T17:59:04Z", "citations": 0, "tweets": 11, "paper_link": "http://arxiv.org/abs/2503.18942v1", "categories": ["cs.CV", "cs.AI"], "abstract": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"}, {"arxiv_id": "2503.18941", "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval", "authors": ["Hongru Cai", "Yongqi Li", "Ruifeng Yuan", "Wenjie Wang", "Zhen Zhang", "Wenjie Li", "Tat-Seng Chua"], "published": "2025-03-24T17:59:03Z", "citations": 0, "tweets": 26, "paper_link": "http://arxiv.org/abs/2503.18941v1", "categories": ["cs.IR", "cs.CL"], "abstract": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems."}, {"arxiv_id": "2503.18938", "title": "AdaWorld: Learning Adaptable World Models with Latent Actions", "authors": ["Shenyuan Gao", "Siyuan Zhou", "Yilun Du", "Jun Zhang", "Chuang Gan"], "published": "2025-03-24T17:58:15Z", "citations": 0, "tweets": 9, "paper_link": "http://arxiv.org/abs/2503.18938v1", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "abstract": "World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning."}, {"arxiv_id": "2503.19910", "title": "CoLLM: A Large Language Model for Composed Image Retrieval", "authors": ["Chuong Huynh", "Jinyu Yang", "Ashish Tawari", "Mubarak Shah", "Son Tran", "Raffay Hamid", "Trishul Chilimbi", "Abhinav Shrivastava"], "published": "2025-03-25T17:59:50Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.19910v1", "categories": ["cs.CV", "cs.IR"], "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field."}, {"arxiv_id": "2503.19909", "title": "In the Magma chamber: Update and challenges in ground-truth\n  vulnerabilities revival for automatic input generator comparison", "authors": ["Timothée Riom", "Sabine Houy", "Bruno Kreyssig", "Alexandre Bartel"], "published": "2025-03-25T17:59:27Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.19909v1", "categories": ["cs.SE", "cs.CR"], "abstract": "Fuzzing is a well-established technique for detecting bugs and\nvulnerabilities. With the surge of fuzzers and fuzzer platforms being developed\nsuch as AFL and OSSFuzz rises the necessity to benchmark these tools'\nperformance. A common problem is that vulnerability benchmarks are based on\nbugs in old software releases. For this very reason, Magma introduced the\nnotion of forward-porting to reintroduce vulnerable code in current software\nreleases. While their results are promising, the state-of-the-art lacks an\nupdate on the maintainability of this approach over time. Indeed, adding the\nvulnerable code to a recent software version might either break its\nfunctionality or make the vulnerable code no longer reachable. We characterise\nthe challenges with forward-porting by reassessing the portability of Magma's\nCVEs four years after its release and manually reintroducing the\nvulnerabilities in the current software versions. We find the straightforward\nprocess efficient for 17 of the 32 CVEs in our study. We further investigate\nwhy a trivial forward-porting process fails in the 15 other CVEs. This involves\nidentifying the commits breaking the forward-porting process and reverting them\nin addition to the bug fix. While we manage to complete the process for nine of\nthese CVEs, we provide an update on all 15 and explain the challenges we have\nbeen confronted with in this process. Thereby, we give the basis for future\nwork towards a sustainable forward-ported fuzzing benchmark."}, {"arxiv_id": "2503.19906", "title": "AvatarArtist: Open-Domain 4D Avatarization", "authors": ["Hongyu Liu", "Xuan Wang", "Ziyu Wan", "Yue Ma", "Jingye Chen", "Yanbo Fan", "Yujun Shen", "Yibing Song", "Qifeng Chen"], "published": "2025-03-25T17:59:03Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.19906v1", "categories": ["cs.CV"], "abstract": "This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies.."}, {"arxiv_id": "2503.19903", "title": "Scaling Vision Pre-Training to 4K Resolution", "authors": ["Baifeng Shi", "Boyi Li", "Han Cai", "Yao Lu", "Sifei Liu", "Marco Pavone", "Jan Kautz", "Song Han", "Trevor Darrell", "Pavlo Molchanov", "Hongxu Yin"], "published": "2025-03-25T17:58:37Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.19903v1", "categories": ["cs.CV"], "abstract": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL."}, {"arxiv_id": "2503.19902", "title": "ICE: Intrinsic Concept Extraction from a Single Image via Diffusion\n  Models", "authors": ["Fernando Julio Cendra", "Kai Han"], "published": "2025-03-25T17:58:29Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.19902v1", "categories": ["cs.CV"], "abstract": "The inherent ambiguity in defining visual concepts poses significant\nchallenges for modern generative models, such as the diffusion-based\nText-to-Image (T2I) models, in accurately learning concepts from a single\nimage. Existing methods lack a systematic way to reliably extract the\ninterpretable underlying intrinsic concepts. To address this challenge, we\npresent ICE, short for Intrinsic Concept Extraction, a novel framework that\nexclusively utilizes a T2I model to automatically and systematically extract\nintrinsic concepts from a single image. ICE consists of two pivotal stages. In\nthe first stage, ICE devises an automatic concept localization module to\npinpoint relevant text-based concepts and their corresponding masks within the\nimage. This critical stage streamlines concept initialization and provides\nprecise guidance for subsequent analysis. The second stage delves deeper into\neach identified mask, decomposing the object-level concepts into intrinsic\nconcepts and general concepts. This decomposition allows for a more granular\nand interpretable breakdown of visual elements. Our framework demonstrates\nsuperior performance on intrinsic concept extraction from a single image in an\nunsupervised manner. Project page: https://visual-ai.github.io/ice"}, {"arxiv_id": "2503.20786", "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "authors": ["Sondos Mahmoud Bsharat", "Mukul Ranjan", "Aidar Myrzakhan", "Jiacheng Liu", "Bowei Guo", "Shengkun Tang", "Zhuang Liu", "Yuanzhi Li", "Zhiqiang Shen"], "published": "2025-03-26T17:59:56Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.20786v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU."}, {"arxiv_id": "2503.20784", "title": "FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with\n  Feature Banks", "authors": ["Jinwei Li", "Huan-ang Gao", "Wenyi Li", "Haohan Chi", "Chenyu Liu", "Chenxi Du", "Yiqian Liu", "Mingju Gao", "Guiyu Zhang", "Zongzheng Zhang", "Li Yi", "Yao Yao", "Jingwei Zhao", "Hongyang Li", "Yikai Wang", "Hao Zhao"], "published": "2025-03-26T17:59:31Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2503.20784v1", "categories": ["cs.CV"], "abstract": "With the rapid advancements in diffusion models and 3D generation techniques,\ndynamic 3D content generation has become a crucial research area. However,\nachieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal\nconsistency remains a challenging task. Inspired by recent findings that\npretrained diffusion features capture rich correspondences, we propose FB-4D, a\nnovel 4D generation framework that integrates a Feature Bank mechanism to\nenhance both spatial and temporal consistency in generated frames. In FB-4D, we\nstore features extracted from previous frames and fuse them into the process of\ngenerating subsequent frames, ensuring consistent characteristics across both\ntime and multiple views. To ensure a compact representation, the Feature Bank\nis updated by a proposed dynamic merging mechanism. Leveraging this Feature\nBank, we demonstrate for the first time that generating additional reference\nsequences through multiple autoregressive iterations can continuously improve\ngeneration performance. Experimental results show that FB-4D significantly\noutperforms existing methods in terms of rendering quality, spatial-temporal\nconsistency, and robustness. It surpasses all multi-view generation tuning-free\napproaches by a large margin and achieves performance on par with\ntraining-based methods."}, {"arxiv_id": "2503.20783", "title": "Understanding R1-Zero-Like Training: A Critical Perspective", "authors": ["Zichen Liu", "Changyu Chen", "Wenjun Li", "Penghui Qi", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "published": "2025-03-26T17:59:14Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2503.20783v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero."}, {"arxiv_id": "2503.20776", "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields", "authors": ["Shijie Zhou", "Hui Ren", "Yijia Weng", "Shuwang Zhang", "Zhen Wang", "Dejia Xu", "Zhiwen Fan", "Suya You", "Zhangyang Wang", "Leonidas Guibas", "Achuta Kadambi"], "published": "2025-03-26T17:56:16Z", "citations": 0, "tweets": 13, "paper_link": "http://arxiv.org/abs/2503.20776v1", "categories": ["cs.CV"], "abstract": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."}, {"arxiv_id": "2503.20757", "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search", "authors": ["Yunhai Hu", "Yilun Zhao", "Chen Zhao", "Arman Cohan"], "published": "2025-03-26T17:46:08Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.20757v1", "categories": ["cs.CL"], "abstract": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."}, {"arxiv_id": "2503.21774", "title": "Optimal Stepsize for Diffusion Sampling", "authors": ["Jianning Pei", "Han Hu", "Shuyang Gu"], "published": "2025-03-27T17:59:46Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.21774v1", "categories": ["cs.CV"], "abstract": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps."}, {"arxiv_id": "2503.21760", "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "published": "2025-03-27T17:57:28Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.21760v1", "categories": ["cs.CL"], "abstract": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks."}, {"arxiv_id": "2503.21757", "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck", "authors": ["Adrian Bulat", "Yassine Ouali", "Georgios Tzimiropoulos"], "published": "2025-03-27T17:57:07Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.21757v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality."}, {"arxiv_id": "2503.21755", "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness", "authors": ["Dian Zheng", "Ziqi Huang", "Hongbo Liu", "Kai Zou", "Yinan He", "Fan Zhang", "Yuanhan Zhang", "Jingwen He", "Wei-Shi Zheng", "Yu Qiao", "Ziwei Liu"], "published": "2025-03-27T17:57:01Z", "citations": 0, "tweets": 11, "paper_link": "http://arxiv.org/abs/2503.21755v1", "categories": ["cs.CV"], "abstract": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."}, {"arxiv_id": "2503.21748", "title": "Extracting energy via bosonic Gaussian operations", "authors": ["Frank Ernesto Quintela Rodriguez", "Francesco Anna Mele", "Salvatore Francesco Emanuele Oliviero", "Vittorio Giovannetti", "Ludovico Lami", "Vasco Cavina"], "published": "2025-03-27T17:56:00Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.21748v1", "categories": ["quant-ph"], "abstract": "Quantum thermodynamics is often formulated as a theory with constrained\naccess to operations and resources. In this manuscript, we find a closed\nformula for the Gaussian ergotropy, i.e. the maximum energy that can be\nextracted from bosonic systems governed by quadratic Hamiltonians by means of\nGaussian unitaries only. This formula resembles the well-known eigenvalue-based\nexpression for the standard ergotropy, but is instead formulated using\nsymplectic eigenvalues. We further prove that the Gaussian ergotropy is\nadditive, indicating that the multiple-copy scenario does not benefit from\nGaussian entangling operations. Extending our analysis to the relationship\nbetween ergotropic and entropic functions, we establish bounds linking entropic\nmeasures of Gaussianity to extractable work. Finally, we generalise our\nframework to open systems by studying the optimal state preparation that\nminimises the energy output in a Gaussian channel."}, {"arxiv_id": "2503.22678", "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions", "authors": ["Mohammad Almansoori", "Komal Kumar", "Hisham Cholakkal"], "published": "2025-03-28T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.22678v1", "categories": ["cs.CL"], "abstract": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}."}, {"arxiv_id": "2503.22676", "title": "TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D\n  Gaussian Splatting", "authors": [" Boyang", " Yu", "Yanlin Jin", "Ashok Veeraraghavan", "Akshat Dave", "Guha Balakrishnan"], "published": "2025-03-28T17:59:43Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.22676v1", "categories": ["cs.CV"], "abstract": "We present TranSplat, a 3D scene rendering algorithm that enables realistic\ncross-scene object transfer (from a source to a target scene) based on the\nGaussian Splatting framework. Our approach addresses two critical challenges:\n(1) precise 3D object extraction from the source scene, and (2) faithful\nrelighting of the transferred object in the target scene without explicit\nmaterial property estimation. TranSplat fits a splatting model to the source\nscene, using 2D object masks to drive fine-grained 3D segmentation. Following\nuser-guided insertion of the object into the target scene, along with automatic\nrefinement of position and orientation, TranSplat derives per-Gaussian radiance\ntransfer functions via spherical harmonic analysis to adapt the object's\nappearance to match the target scene's lighting environment. This relighting\nstrategy does not require explicitly estimating physical scene properties such\nas BRDFs. Evaluated on several synthetic and real-world scenes and objects,\nTranSplat yields excellent 3D object extractions and relighting performance\ncompared to recent baseline methods and visually convincing cross-scene object\ntransfers. We conclude by discussing the limitations of the approach."}, {"arxiv_id": "2503.22674", "title": "QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?", "authors": ["Belinda Z. Li", "Been Kim", "Zi Wang"], "published": "2025-03-28T17:58:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.22674v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities."}, {"arxiv_id": "2503.22656", "title": "Differential equation quantum solvers: engineering measurements to\n  reduce cost", "authors": ["Annie Paine", "Casper Gyurik", "Antonio Andrea Gentile"], "published": "2025-03-28T17:43:35Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.22656v1", "categories": ["quant-ph", "cs.LG"], "abstract": "Quantum computers have been proposed as a solution for efficiently solving\nnon-linear differential equations (DEs), a fundamental task across diverse\ntechnological and scientific domains. However, a crucial milestone in this\nregard is to design protocols that are hardware-aware, making efficient use of\nlimited available quantum resources. We focus here on promising variational\nmethods derived from scientific machine learning: differentiable quantum\ncircuits (DQC), addressing specifically their cost in number of circuit\nevaluations. Reducing the number of quantum circuit evaluations is particularly\nvaluable in hybrid quantum/classical protocols, where the time required to\ninterface and run quantum hardware at each cycle can impact the total wall-time\nmuch more than relatively inexpensive classical post-processing overhead. Here,\nwe propose and test two sample-efficient protocols for solving non-linear DEs,\nachieving exponential savings in quantum circuit evaluations. These protocols\nare based on redesigning the extraction of information from DQC in a\n``measure-first\" approach, by introducing engineered cost operators similar to\nthe randomized-measurement toolbox (i.e. classical shadows). In benchmark\nsimulations on one and two-dimensional DEs, we report up to $\\sim$ 100 fold\nreductions in circuit evaluations. Our protocols thus hold the promise to\nunlock larger and more challenging non-linear differential equation\ndemonstrations with existing quantum hardware."}, {"arxiv_id": "2503.22655", "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training", "authors": ["Xiaomin Yu", "Pengxiang Ding", "Wenjie Zhang", "Siteng Huang", "Songyang Gao", "Chengwei Qin", "Kejian Wu", "Zhaoxin Fan", "Ziyue Qiao", "Donglin Wang"], "published": "2025-03-28T17:43:00Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.22655v1", "categories": ["cs.AI", "cs.CV", "cs.MM"], "abstract": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git."}, {"arxiv_id": "2503.24389", "title": "SU-YOLO: Spiking Neural Network for Efficient Underwater Object\n  Detection", "authors": ["Chenyang Li", "Wenxuan Liu", "Guoqiang Gong", "Xiaobo Ding", "Xian Zhong"], "published": "2025-03-31T17:59:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.24389v1", "categories": ["cs.CV", "cs.NE"], "abstract": "Underwater object detection is critical for oceanic research and industrial\nsafety inspections. However, the complex optical environment and the limited\nresources of underwater equipment pose significant challenges to achieving high\naccuracy and low power consumption. To address these issues, we propose Spiking\nUnderwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the\nlightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a\nnovel spike-based underwater image denoising method based solely on integer\naddition, which enhances the quality of feature maps with minimal computational\noverhead. In addition, we introduce Separated Batch Normalization (SeBN), a\ntechnique that normalizes feature maps independently across multiple time steps\nand is optimized for integration with residual structures to capture the\ntemporal dynamics of SNNs more effectively. The redesigned spiking residual\nblocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO\narchitecture to mitigate spike degradation and enhance the model's feature\nextraction capabilities. Experimental results on URPC2019 underwater dataset\ndemonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an\nenergy consumption of 2.98 mJ, surpassing mainstream SNN models in both\ndetection accuracy and computational efficiency. These results underscore the\npotential of SNNs for engineering applications. The code is available in\nhttps://github.com/lwxfight/snn-underwater."}, {"arxiv_id": "2503.24377", "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models", "authors": ["Rui Wang", "Hongru Wang", "Boyang Xue", "Jianhui Pang", "Shudong Liu", "Yi Chen", "Jiahao Qiu", "Derek Fai Wong", "Heng Ji", "Kam-Fai Wong"], "published": "2025-03-31T17:58:07Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.24377v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field."}, {"arxiv_id": "2503.24376", "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "published": "2025-03-31T17:55:23Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2503.24376v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."}, {"arxiv_id": "2503.24370", "title": "Effectively Controlling Reasoning Models through Thinking Intervention", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Prateek Mittal"], "published": "2025-03-31T17:50:13Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.24370v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs."}, {"arxiv_id": "2503.24368", "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image\n  Segmentation", "authors": ["Xiaoran Zhang", "Eric Z. Chen", "Lin Zhao", "Xiao Chen", "Yikang Liu", "Boris Maihe", "James S. Duncan", "Terrence Chen", "Shanhui Sun"], "published": "2025-03-31T17:47:42Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.24368v1", "categories": ["cs.CV"], "abstract": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications."}, {"arxiv_id": "2504.01951", "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through\n  Online Shopping Data", "authors": ["Massimiliano Luca", "Ciro Beneduce", "Bruno Lepri", "Jacopo Staiano"], "published": "2025-04-02T17:56:08Z", "citations": 0, "tweets": 13, "paper_link": "http://arxiv.org/abs/2504.01951v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "abstract": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies."}, {"arxiv_id": "2504.01943", "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "authors": ["Wasi Uddin Ahmad", "Sean Narenthiran", "Somshubra Majumdar", "Aleksander Ficek", "Siddhartha Jain", "Jocelyn Huang", "Vahid Noroozi", "Boris Ginsburg"], "published": "2025-04-02T17:50:31Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.01943v1", "categories": ["cs.CL"], "abstract": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community."}, {"arxiv_id": "2504.01935", "title": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning\n  Length?", "authors": ["Celine Lee", "Alexander M. Rush", "Keyon Vafa"], "published": "2025-04-02T17:45:58Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.01935v1", "categories": ["cs.AI"], "abstract": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements."}, {"arxiv_id": "2504.01933", "title": "Hessian-aware Training for Enhancing DNNs Resilience to Parameter\n  Corruptions", "authors": ["Tahmid Hasan Prato", "Seijoon Kim", "Lizhong Chen", "Sanghyun Hong"], "published": "2025-04-02T17:42:31Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.01933v1", "categories": ["cs.CR", "cs.LG"], "abstract": "Deep neural networks are not resilient to parameter corruptions: even a\nsingle-bitwise error in their parameters in memory can cause an accuracy drop\nof over 10%, and in the worst cases, up to 99%. This susceptibility poses great\nchallenges in deploying models on computing platforms, where adversaries can\ninduce bit-flips through software or bitwise corruptions may occur naturally.\nMost prior work addresses this issue with hardware or system-level approaches,\nsuch as integrating additional hardware components to verify a model's\nintegrity at inference. However, these methods have not been widely deployed as\nthey require infrastructure or platform-wide modifications.\n  In this paper, we propose a new approach to addressing this issue: training\nmodels to be more resilient to bitwise corruptions to their parameters. Our\napproach, Hessian-aware training, promotes models with $flatter$ loss surfaces.\nWe show that, while there have been training methods, designed to improve\ngeneralization through Hessian-based approaches, they do not enhance resilience\nto parameter corruptions. In contrast, models trained with our method\ndemonstrate increased resilience to parameter corruptions, particularly with a\n20$-$50% reduction in the number of bits whose individual flipping leads to a\n90$-$100% accuracy drop. Moreover, we show the synergy between ours and\nexisting hardware and system-level defenses."}, {"arxiv_id": "2504.01931", "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents\n  with Dynamic Evaluation and Selection", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Jindong Gu", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Hamid Palangi", "Tomas Pfister"], "published": "2025-04-02T17:40:47Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.01931v1", "categories": ["cs.CL"], "abstract": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization."}, {"arxiv_id": "2504.02823", "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage\n  Security Inspection", "authors": ["Divya Velayudhan", "Abdelfatah Ahmed", "Mohamad Alansari", "Neha Gour", "Abderaouf Behouch", "Taimur Hassan", "Syed Talal Wasim", "Nabil Maalej", "Muzammal Naseer", "Juergen Gall", "Mohammed Bennamoun", "Ernesto Damiani", "Naoufel Werghi"], "published": "2025-04-03T17:59:12Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.02823v1", "categories": ["cs.CV", "eess.IV"], "abstract": "Advancements in Computer-Aided Screening (CAS) systems are essential for\nimproving the detection of security threats in X-ray baggage scans. However,\ncurrent datasets are limited in representing real-world, sophisticated threats\nand concealment tactics, and existing approaches are constrained by a\nclosed-set paradigm with predefined labels. To address these challenges, we\nintroduce STCray, the first multimodal X-ray baggage security dataset,\ncomprising 46,642 image-caption paired scans across 21 threat categories,\ngenerated using an X-ray scanner for airport security. STCray is meticulously\ndeveloped with our specialized protocol that ensures domain-aware, coherent\ncaptions, that lead to the multi-modal instruction following data in X-ray\nbaggage security. This allows us to train a domain-aware visual AI assistant\nnamed STING-BEE that supports a range of vision-language tasks, including scene\ncomprehension, referring threat localization, visual grounding, and visual\nquestion answering (VQA), establishing novel baselines for multi-modal learning\nin X-ray baggage security. Further, STING-BEE shows state-of-the-art\ngeneralization in cross-domain settings. Code, data, and models are available\nat https://divs1159.github.io/STING-BEE/."}, {"arxiv_id": "2504.02821", "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models", "authors": ["Mateusz Pach", "Shyamgopal Karthik", "Quentin Bouniot", "Serge Belongie", "Zeynep Akata"], "published": "2025-04-03T17:58:35Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.02821v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs."}, {"arxiv_id": "2504.02810", "title": "Generative Evaluation of Complex Reasoning in Large Language Models", "authors": ["Haowei Lin", "Xiangyu Wang", "Ruilin Yan", "Baizhou Huang", "Haotian Ye", "Jianhua Zhu", "Zihao Wang", "James Zou", "Jianzhu Ma", "Yitao Liang"], "published": "2025-04-03T17:54:18Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.02810v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."}, {"arxiv_id": "2504.02807", "title": "MegaMath: Pushing the Limits of Open Math Corpora", "authors": ["Fan Zhou", "Zengzhi Wang", "Nikhil Ranjan", "Zhoujun Cheng", "Liping Tang", "Guowei He", "Zhengzhong Liu", "Eric P. Xing"], "published": "2025-04-03T17:52:07Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.02807v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets."}, {"arxiv_id": "2504.02801", "title": "F-ViTA: Foundation Model Guided Visible to Thermal Translation", "authors": ["Jay N. Paranjape", "Celso de Melo", "Vishal M. Patel"], "published": "2025-04-03T17:47:06Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.02801v1", "categories": ["cs.CV"], "abstract": "Thermal imaging is crucial for scene understanding, particularly in low-light\nand nighttime conditions. However, collecting large thermal datasets is costly\nand labor-intensive due to the specialized equipment required for infrared\nimage capture. To address this challenge, researchers have explored\nvisible-to-thermal image translation. Most existing methods rely on Generative\nAdversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a\nstyle transfer problem. As a result, these approaches attempt to learn both the\nmodality distribution shift and underlying physical principles from limited\ntraining data. In this paper, we propose F-ViTA, a novel approach that\nleverages the general world knowledge embedded in foundation models to guide\nthe diffusion process for improved translation. Specifically, we condition an\nInstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation\nmodels such as SAM and Grounded DINO. This allows the model to learn meaningful\ncorrelations between scene objects and their thermal signatures in infrared\nimagery. Extensive experiments on five public datasets demonstrate that F-ViTA\noutperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes\nwell to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared\n(LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the\nsame visible image. Code: https://github.com/JayParanjape/F-ViTA/tree/master."}, {"arxiv_id": "2504.03635", "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling\n  Law for Reasoning", "authors": ["Xinyi Wang", "Shawn Tan", "Mingyu Jin", "William Yang Wang", "Rameswar Panda", "Yikang Shen"], "published": "2025-04-04T17:57:22Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.03635v1", "categories": ["cs.AI", "cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."}, {"arxiv_id": "2504.03622", "title": "Align to Structure: Aligning Large Language Models with Structural\n  Information", "authors": ["Zae Myung Kim", "Anand Ramachandran", "Farideh Tavazoee", "Joo-Kyung Kim", "Oleg Rokhlenko", "Dongyeop Kang"], "published": "2025-04-04T17:40:04Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.03622v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align."}, {"arxiv_id": "2504.03621", "title": "VISTA-OCR: Towards generative and interactive end to end OCR models", "authors": ["Laziz Hamdi", "Amine Tamasna", "Pascal Boisson", "Thierry Paquet"], "published": "2025-04-04T17:39:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.03621v1", "categories": ["cs.CV"], "abstract": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance."}, {"arxiv_id": "2504.03620", "title": "Quantum Search with In-Place Queries", "authors": ["Blake Holman", "Ronak Ramachandran", "Justin Yirka"], "published": "2025-04-04T17:37:42Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.03620v1", "categories": ["quant-ph", "cs.CC", "cs.DS"], "abstract": "Quantum query complexity is typically characterized in terms of XOR queries\n|x,y> to |x,y+f(x)> or phase queries, which ensure that even queries to\nnon-invertible functions are unitary. When querying a permutation, another\nnatural model is unitary: in-place queries |x> to |f(x)>.\n  Some problems are known to require exponentially fewer in-place queries than\nXOR queries, but no separation has been shown in the opposite direction. A\ncandidate for such a separation was the problem of inverting a permutation over\nN elements. This task, equivalent to unstructured search in the context of\npermutations, is solvable with $O(\\sqrt{N})$ XOR queries but was conjectured to\nrequire $\\Omega(N)$ in-place queries.\n  We refute this conjecture by designing a quantum algorithm for Permutation\nInversion using $O(\\sqrt{N})$ in-place queries. Our algorithm achieves the same\nspeedup as Grover's algorithm despite the inability to efficiently uncompute\nqueries or perform straightforward oracle-controlled reflections.\n  Nonetheless, we show that there are indeed problems which require fewer XOR\nqueries than in-place queries. We introduce a subspace-conversion problem\ncalled Function Erasure that requires 1 XOR query and $\\Theta(\\sqrt{N})$\nin-place queries. Then, we build on a recent extension of the quantum adversary\nmethod to characterize exact conditions for a decision problem to exhibit such\na separation, and we propose a candidate problem."}, {"arxiv_id": "2504.03616", "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task", "authors": ["Leonardo Ranaldi", "Barry Haddow", "Alexandra Birch"], "published": "2025-04-04T17:35:43Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.03616v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages."}, {"arxiv_id": "2504.05294", "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "published": "2025-04-07T17:49:23Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.05294v1", "categories": ["cs.CL"], "abstract": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations."}, {"arxiv_id": "2504.05289", "title": "Direct Measurement of the Singlet Lifetime and Photoexcitation Behavior\n  of the Boron Vacancy Center in Hexagonal Boron Nitride", "authors": ["Richard A. Escalante", "Andrew J. Beling", "Niko Reed", "Justin Welter", "John Blanchard", "Daniel G. Ang", "Cecilia Campos", "Edwin Coronel", "Klaus Krambrock", "Alexander S. Leal", "Paras N. Prasad", "Ronald L. Walsworth"], "published": "2025-04-07T17:42:43Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.05289v1", "categories": ["quant-ph"], "abstract": "Optically active electronic spin defects in van der Waals (vdW) materials\nprovide a promising platform for quantum sensing, as they can enable shorter\nstandoff distances compared to defects in diamond, leading to sensitivity\nadvantages. Perhaps the most studied defect in a vdW material is the negatively\ncharged boron vacancy center ($V^{-}_{B}$) in hexagonal boron nitride (hBN).\nHowever, many of the $V^{-}_{B}$ electronic and spin transition rates and\nbranching ratios are not fully known. Here, we use time-resolved\nphotoluminescence (PL) measurements with a nanosecond rise-time 515 nm laser to\ndetermine directly the singlet state lifetime of a $V^{-}_{B}$ ensemble in\nneutron-irradiated, sub-micron-size flakes of hBN. We perform this measurement\non 16 different hBN flakes at room temperature and obtain an average lifetime\nof 15(3) ns. Additionally, we probe the PL dynamics of thermal and optically\npolarized electronic spin distributions of the $V^{-}_{B}$ ensemble in a single\nsub-micron hBN flake, and fit our results to a 9-level model to extract the\nelectronic transition rates. Lastly, we present PL measurements that\npotentially indicate optically-induced conversion of $V^{-}_{B}$ to another\nelectronic state, or possibly the neutral charge state (V$^{0}_{B}$), in\nneutron-irradiated hBN flakes of size $>$ 1 $\\mu$m."}, {"arxiv_id": "2504.05287", "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception", "authors": ["Hui Zhang", "Zijian Wu", "Linyi Huang", "Sammy Christen", "Jie Song"], "published": "2025-04-07T17:38:19Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.05287v1", "categories": ["cs.RO"], "abstract": "Robust grasping of various objects from single-view perception is fundamental\nfor dexterous robots. Previous works often rely on fully observable objects,\nexpert demonstrations, or static grasping poses, which restrict their\ngeneralization ability and adaptability to external disturbances. In this\npaper, we present a reinforcement-learning-based framework that enables\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\nsingle-view perception, while performing adaptive motions to external\ndisturbances. We utilize a hand-centric object representation for shape feature\nextraction that emphasizes interaction-relevant local shapes, enhancing\nrobustness to shape variance and uncertainty. To enable effective hand\nadaptation to disturbances with limited observations, we propose a mixed\ncurriculum learning strategy, which first utilizes imitation learning to\ndistill a policy trained with privileged real-time visual-tactile feedback, and\ngradually transfers to reinforcement learning to learn adaptive motions under\ndisturbances caused by observation noises and dynamic randomization. Our\nexperiments demonstrate strong generalization in grasping unseen objects with\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\nmethod to various disturbances, including unobserved object movement and\nexternal forces, through both quantitative and qualitative evaluations. Project\nPage: https://zdchan.github.io/Robust_DexGrasp/"}, {"arxiv_id": "2504.05278", "title": "The challenge of uncertainty quantification of large language models in\n  medicine", "authors": ["Zahra Atf", "Seyed Amir Ahmad Safavi-Naini", "Peter R. Lewis", "Aref Mahjoubfar", "Nariman Naderi", "Thomas R. Savage", "Ali Soroush"], "published": "2025-04-07T17:24:11Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.05278v1", "categories": ["cs.AI"], "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge."}, {"arxiv_id": "2504.05276", "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented\n  Generation", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "published": "2025-04-07T17:17:41Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.05276v1", "categories": ["cs.CL"], "abstract": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains."}, {"arxiv_id": "2504.06266", "title": "Constraining the [CII] luminosity function from the power spectrum of\n  line intensity maps at redshift 3.6", "authors": ["Elena Marcuzzo", "Cristiano Porciani", "Emilio Romano-Díaz", "Prachi Khatri"], "published": "2025-04-08T17:59:59Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.06266v1", "categories": ["astro-ph.CO", "astro-ph.GA"], "abstract": "Forthcoming measurements of the line-intensity-mapping power spectrum (PS)\nare expected to set precious constraints on several quantities of astrophysical\nand cosmological interest. Our study targets the [CII] luminosity function (LF)\nat high redshift, which is still highly uncertain, in particular at the faint\nend. As an example of future opportunities, we present forecasts for the Deep\nSpectroscopic Survey (DSS) that will be conducted with the Fred Young\nSubmillimeter Telescope at $z \\simeq 3.6$ and also make predictions for\neventual $10\\times$ wider and/or $\\sqrt{10}\\times$ more sensitive surveys. The\nhalo-occupation properties of [CII] emitters in the MARIGOLD simulations\nprovide us with the motivation to abundance match two versions of the ALPINE LF\nagainst the halo mass function. We employ the resulting luminosity-mass\nrelation within the halo model to predict the expected PS signal and its\nuncertainty. Finally, we use Bayesian inference to analyse mock PS data and\nforecast what constraints could be achieved on the first two moments of the LF\nand on Schechter fits. Depending on the actual LF, the DSS will measure the\nclustering and shot-noise amplitudes of the PS with a signal-to-noise ratio of\n$\\sim 3$ or higher. However, degeneracies with the bias parameter and\nredshift-space distortions make it unfeasible to extract the first moment of\nthe LF. Even the widest and most sensitive survey we consider can only\nconstrain it with a $50\\%$ uncertainty. By jointly fitting the PS and the LF,\nwe directly constrain Schechter-function parameters. We find that the\nnormalisation and the cutoff luminosity are precisely and accurately measured\nwhile the faint-end slope remains highly uncertain (unless the true value\napproaches $-2$). Overall, increasing the survey sensitivity at fixed sky\ncoverage yields greater improvements than covering a larger area at fixed\nsensitivity."}, {"arxiv_id": "2504.06265", "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning\n  through Bayesian Optimization", "authors": ["Bojana Ranković", "Philippe Schwaller"], "published": "2025-04-08T17:59:57Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.06265v1", "categories": ["cs.LG", "cs.AI"], "abstract": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization."}, {"arxiv_id": "2504.06261", "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "authors": ["Gleb Rodionov", "Roman Garipov", "Alina Shutova", "George Yakushev", "Vage Egiazarian", "Anton Sinitsin", "Denis Kuznedelev", "Dan Alistarh"], "published": "2025-04-08T17:59:41Z", "citations": 0, "tweets": 29, "paper_link": "http://arxiv.org/abs/2504.06261v1", "categories": ["cs.LG", "cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."}, {"arxiv_id": "2504.06260", "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "published": "2025-04-08T17:59:39Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.06260v1", "categories": ["cs.AI", "cs.CL", "cs.NA", "math.NA"], "abstract": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench"}, {"arxiv_id": "2504.06257", "title": "PainNet: Statistical Relation Network with Episode-Based Training for\n  Pain Estimation", "authors": ["Mina Bishay", "Graham Page", "Mohammad Mavadati"], "published": "2025-04-08T17:58:52Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.06257v1", "categories": ["cs.CV"], "abstract": "Despite the span in estimating pain from facial expressions, limited works\nhave focused on estimating the sequence-level pain, which is reported by\npatients and used commonly in clinics. In this paper, we introduce a novel\nStatistical Relation Network, referred to as PainNet, designed for the\nestimation of the sequence-level pain. PainNet employs two key modules, the\nembedding and the relation modules, for comparing pairs of pain videos, and\nproducing relation scores indicating if each pair belongs to the same pain\ncategory or not. At the core of the embedding module is a statistical layer\nmounted on the top of a RNN for extracting compact video-level features. The\nstatistical layer is implemented as part of the deep architecture. Doing so,\nallows combining multiple training stages used in previous research, into a\nsingle end-to-end training stage. PainNet is trained using the episode-based\ntraining scheme, which involves comparing a query video with a set of videos\nrepresenting the different pain categories. Experimental results show the\nbenefit of using the statistical layer and the episode-based training in the\nproposed model. Furthermore, PainNet outperforms the state-of-the-art results\non self-reported pain estimation."}, {"arxiv_id": "2504.07097", "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\n  Learning", "authors": ["Nikhil Shivakumar Nayak", "Krishnateja Killamsetty", "Ligong Han", "Abhishek Bhandwaldar", "Prateek Chanda", "Kai Xu", "Hao Wang", "Aldo Pareja", "Oleg Silkin", "Mustafa Eyceoz", "Akash Srivastava"], "published": "2025-04-09T17:59:42Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.07097v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.PR", "stat.ML", "68T50", "I.2.0; G.3"], "abstract": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models."}, {"arxiv_id": "2504.07089", "title": "OmniCaptioner: One Captioner to Rule Them All", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "published": "2025-04-09T17:58:58Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2504.07089v1", "categories": ["cs.CV", "cs.CL"], "abstract": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."}, {"arxiv_id": "2504.07087", "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on\n  Textualized Knowledge Graphs", "authors": ["Elan Markowitz", "Krupa Galiya", "Greg Ver Steeg", "Aram Galstyan"], "published": "2025-04-09T17:58:47Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2504.07087v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abstract": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks."}, {"arxiv_id": "2504.07080", "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning", "authors": ["Atharva Pandey", "Kshitij Dubey", "Rahul Sharma", "Amit Sharma"], "published": "2025-04-09T17:53:55Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.07080v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains."}, {"arxiv_id": "2504.07070", "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large\n  Language Models", "authors": ["Zhouhang Xie", "Junda Wu", "Yiran Shen", "Yu Xia", "Xintong Li", "Aaron Chang", "Ryan Rossi", "Sachin Kumar", "Bodhisattwa Prasad Majumder", "Jingbo Shang", "Prithviraj Ammanabrolu", "Julian McAuley"], "published": "2025-04-09T17:39:58Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.07070v1", "categories": ["cs.CL"], "abstract": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field."}, {"arxiv_id": "2504.07964", "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing", "authors": ["Zhongyang Li", "Ziyue Li", "Tianyi Zhou"], "published": "2025-04-10T17:59:56Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.07964v1", "categories": ["cs.LG"], "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE."}, {"arxiv_id": "2504.07956", "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning", "authors": ["Yukun Qi", "Yiming Zhao", "Yu Zeng", "Xikun Bao", "Wenxuan Huang", "Lin Chen", "Zehui Chen", "Jie Zhao", "Zhongang Qi", "Feng Zhao"], "published": "2025-04-10T17:59:03Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2504.07956v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task."}, {"arxiv_id": "2504.07951", "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models", "authors": ["Mustafa Shukor", "Enrico Fini", "Victor Guilherme Turrisi da Costa", "Matthieu Cord", "Joshua Susskind", "Alaaeldin El-Nouby"], "published": "2025-04-10T17:57:28Z", "citations": 0, "tweets": 9, "paper_link": "http://arxiv.org/abs/2504.07951v1", "categories": ["cs.CV"], "abstract": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."}, {"arxiv_id": "2504.07938", "title": "Development of a Quantum-Resistant File Transfer System with Blockchain\n  Audit Trail", "authors": ["Ernesto Sola-Thomas", "Masudul H Imtiaz"], "published": "2025-04-10T17:51:14Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.07938v1", "categories": ["cs.CR"], "abstract": "This paper presents a condensed system architecture for a file transfer\nsolution that leverages post quantum cryptography and blockchain to secure data\nagainst quantum threats. The architecture integrates NIST standardized\nalgorithms CRYSTALS Kyber for encryption and CRYSTALS Dilithium for digital\nsignatures with an immutable blockchain ledger to provide an auditable,\ndecentralized storage mechanism. The system is modular, comprising a Sender\nmodule for secure encryption and signing, a central User Storage module for\ndecryption, reencryption, and blockchain logging, and a Requestor module for\nauthenticated data access. We include detailed pseudocode, analyze security\nrisks, and offer performance insights to demonstrate the system's robustness,\nscalability, and transparency."}, {"arxiv_id": "2504.07936", "title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path\n  Towards Human-AI Synergy", "authors": ["Jordi Linares-Pellicer", "Juan Izquierdo-Domenech", "Isabel Ferri-Molla", "Carlos Aliaga-Torro"], "published": "2025-04-10T17:50:17Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2504.07936v1", "categories": ["cs.AI"], "abstract": "Generative AI presents a profound challenge to traditional notions of human\nuniqueness, particularly in creativity. Fueled by neural network based\nfoundation models, these systems demonstrate remarkable content generation\ncapabilities, sparking intense debates about authorship, copyright, and\nintelligence itself. This paper argues that generative AI represents an\nalternative form of intelligence and creativity, operating through mathematical\npattern synthesis rather than biological understanding or verbatim replication.\nThe fundamental differences between artificial and biological neural networks\nreveal AI learning as primarily statistical pattern extraction from vast\ndatasets crystallized forms of collective human knowledge scraped from the\ninternet. This perspective complicates copyright theft narratives and\nhighlights practical challenges in attributing AI outputs to individual\nsources. Rather than pursuing potentially futile legal restrictions, we\nadvocate for human AI synergy. By embracing generative AI as a complementary\ntool alongside human intuition, context, and ethical judgment, society can\nunlock unprecedented innovation, democratize creative expression, and address\ncomplex challenges. This collaborative approach, grounded in realistic\nunderstanding of AIs capabilities and limitations, offers the most promising\npath forward. Additionally, recognizing these models as products of collective\nhuman knowledge raises ethical questions about accessibility ensuring equitable\naccess to these tools could prevent widening societal divides and leverage\ntheir full potential for collective benefit."}, {"arxiv_id": "2504.08729", "title": "Steering CLIP's vision transformer with sparse autoencoders", "authors": ["Sonia Joseph", "Praneet Suresh", "Ethan Goldfarb", "Lorenz Hufe", "Yossi Gandelsman", "Robert Graham", "Danilo Bzdok", "Wojciech Samek", "Blake Aaron Richards"], "published": "2025-04-11T17:56:09Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2504.08729v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks."}, {"arxiv_id": "2504.08728", "title": "End-to-End Demonstration of Quantum Generative Adversarial Networks for\n  Steel Microstructure Image Augmentation on a Trapped-Ion Quantum Computer", "authors": ["Samwel Sekwao", "Jason Iaconis", "Claudio Girotto", "Martin Roetteler", "Minwoo Kang", "Donghwi Kim", "Seunghyo Noh", "Woomin Kyoung", "Kyujin Shin"], "published": "2025-04-11T17:55:58Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.08728v1", "categories": ["quant-ph"], "abstract": "Generative adversarial networks (GANs) are a machine learning technique\ncapable of producing high-quality synthetic images. In the field of materials\nscience, when a crystallographic dataset includes inadequate or\ndifficult-to-obtain images, synthetic images can be used for image augmentation\nto mitigate data scarcity and streamline the preparation of datasets for\nhigh-throughput analysis. We integrate quantum computing with GANs into a\nhybrid quantum-classical GAN to generate complex 5-channel electron backscatter\ndiffraction (EBSD) images of two distinct microstructure phases of steel. By\ntraining a quantum circuit at the input layer of a large classical Wasserstein\nGAN (WGAN) model, we mitigate mode collapse and achieve higher image quality\ncompared to a baseline classical GAN. We generate images from both ferrite and\nbainite microstructure phases in an end-to-end workflow. With respect to\nmaximum mean discrepancy score, we find that the hybrid quantum-classical WGAN\nimproves over classical Bernoulli GANs in 70% of samples. As the quantum\ncomputer is part of the training procedure, our method has potential to scale\nto larger number of qubits. Our results indicate that the WGAN model based on\nthe quantum circuit ansatz may be effectively leveraged to enhance the quality\nof synthetic EBSD images on both quantum simulators and actual quantum\nhardware."}, {"arxiv_id": "2504.08727", "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images", "authors": ["Boyang Deng", "Songyou Peng", "Kyle Genova", "Gordon Wetzstein", "Noah Snavely", "Leonidas Guibas", "Thomas Funkhouser"], "published": "2025-04-11T17:55:45Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.08727v1", "categories": ["cs.CV", "cs.AI", "cs.CY"], "abstract": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."}, {"arxiv_id": "2504.08725", "title": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation", "authors": ["Dayu Yang", "Antoine Simoulin", "Xin Qian", "Xiaoyi Liu", "Yuwei Cao", "Zhaopu Teng", "Grey Yang"], "published": "2025-04-11T17:50:08Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.08725v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "abstract": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."}, {"arxiv_id": "2504.08714", "title": "Generating Fine Details of Entity Interactions", "authors": ["Xinyi Gu", "Jiayuan Mao"], "published": "2025-04-11T17:24:58Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.08714v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation."}];
    let currentSort = 'date';
    
    function formatDate(dateString) {
        if (!dateString) return "Unknown date";
        const match = dateString.match(/(\d{4}-\d{2}-\d{2})/);
        return match ? match[1] : "Unknown date";
    }
    
    function formatAuthors(authors, full = false) {
        if (!authors || authors.length === 0) return "Unknown authors";
        if (!full && authors.length > 3) {
            return authors.slice(0, 3).join(', ') + ' et al.';
        }
        return authors.join(', ');
    }
    
    function formatCategories(categories) {
        if (!categories || categories.length === 0) return "";
        return categories.join(', ');
    }
    
    function toggleDetails(id) {
        const details = document.getElementById(`paper-details-${id}`);
        if (details.classList.contains('show')) {
            details.classList.remove('show');
        } else {
            details.classList.add('show');
        }
    }
    
    function expandAll() {
        document.querySelectorAll('.paper-details').forEach(el => {
            el.classList.add('show');
        });
    }
    
    function collapseAll() {
        document.querySelectorAll('.paper-details').forEach(el => {
            el.classList.remove('show');
        });
    }
    
    function sortPapers(sortMethod) {
        document.getElementById('sort-date').classList.remove('active');
        document.getElementById('sort-citations').classList.remove('active');
        document.getElementById('sort-tweets').classList.remove('active');
        document.getElementById('sort-' + sortMethod).classList.add('active');
        
        let sortedPapers = [...papers];
        if (sortMethod === 'date') {
            sortedPapers.sort((a, b) => (b.published || '').localeCompare(a.published || ''));
        } else if (sortMethod === 'citations') {
            sortedPapers.sort((a, b) => (b.citations || 0) - (a.citations || 0));
        } else if (sortMethod === 'tweets') {
            sortedPapers.sort((a, b) => (b.tweets || 0) - (a.tweets || 0));
        }
        
        currentSort = sortMethod;
        
        let html = '';
        sortedPapers.forEach((paper, index) => {
            html += `
            <div class="paper-row">
                <div class="paper-main" onclick="toggleDetails(${index})">
                    <div class="rank">${index + 1}</div>
                    <div class="votes">
                        <a href="https://x.com/search?q=${paper.arxiv_id}&src=typed_query&f=top" target="_blank" style="text-decoration:none" onclick="event.stopPropagation()">
                            <strong>${paper.tweets}</strong>
                            <span>tweets</span>
                        </a>
                    </div>
                    <div class="paper-content">
                        <a href="${paper.paper_link}" class="paper-title" target="_blank" onclick="event.stopPropagation()">${paper.title}</a>
                        <div class="paper-meta">
                            ${formatDate(paper.published)} | ${formatAuthors(paper.authors)} | 📚 ${paper.citations} citations
                        </div>
                    </div>
                </div>
                <div class="paper-details" id="paper-details-${index}">
                    <div><strong>Categories:</strong> ${formatCategories(paper.categories)}</div>
                    <div class="abstract">${paper.abstract || 'No abstract available'}</div>
                </div>
            </div>
            `;
        });
        
        document.getElementById('papers-container').innerHTML = html;
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        sortPapers('tweets');
    });
    </script>
</body>
</html>