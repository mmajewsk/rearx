<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv AI Security Papers</title>
    <style>
        body { font-family: Verdana, sans-serif; margin: 0; padding: 0; background-color: #eee; }
        .container { max-width: 950px; margin: 0 auto; padding: 20px; background-color: #fff; }
        .header { background-color: #cee3f8; border-bottom: 1px solid #5f99cf; padding: 10px 20px; margin-bottom: 20px; }
        .header h1 { margin: 0; font-size: 20px; color: #369; }
        .filter-options { display: flex; justify-content: space-between; margin-bottom: 20px; padding: 10px; background-color: #f8f8f8; border: 1px solid #ddd; }
        .filter-options button { color: #369; cursor: pointer; padding: 5px 10px; background: none; border: none; }
        .filter-options button.active { font-weight: bold; background-color: #e2e2e2; border-radius: 3px; }
        .paper-row { padding: 10px; border-bottom: 1px solid #ddd; line-height: 1.4; }
        .paper-main { display: flex; align-items: center; cursor: pointer; }
        .rank { flex: 0 0 30px; color: #888; text-align: right; padding-right: 10px; font-size: 18px; }
        .votes { flex: 0 0 70px; text-align: center; padding: 0 10px; display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .votes strong { color: #1DA1F2; font-size: 15px; }
        .votes a { color: inherit; transition: transform 0.2s; display: flex; flex-direction: column; align-items: center; }
        .votes a:hover { transform: scale(1.1); }
        .paper-content { flex: 1; display: flex; flex-direction: column; }
        .paper-title { color: #0000ff; text-decoration: none; font-weight: bold; font-size: 16px; }
        .paper-meta { color: #888; font-size: 12px; margin-top: 4px; }
        .paper-details { margin-top: 10px; padding: 10px; background-color: #f9f9f9; border-radius: 5px; display: none; }
        .paper-details.show { display: block; }
        .abstract { font-size: 14px; line-height: 1.5; margin-top: 10px; white-space: pre-line; }
        .footer { text-align: center; margin-top: 20px; font-size: 12px; color: #888; padding: 10px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ArXiv AI Security Papers</h1>
        </div>
        <div class="filter-options">
            <div>
                <button onclick="sortPapers('date')" id="sort-date">Recent</button>
                <button onclick="sortPapers('citations')" id="sort-citations">Most Cited</button>
                <button onclick="sortPapers('tweets')" class="active" id="sort-tweets">Most Tweeted</button>
            </div>
            <div>
                <button onclick="expandAll()" id="expand-all">Expand All</button>
                <button onclick="collapseAll()" id="collapse-all">Collapse All</button>
                <a href="list.html" style="margin-left: 20px; background-color: #5f99cf; color: white; padding: 5px 10px; text-decoration: none; border-radius: 3px;">See Previous Renders</a>
            </div>
        </div>
        
        <div id="papers-container"></div>
        
        <div class="footer">
            Generated on 2025-06-15 00:30:06 | Database contains 313 papers total.
            <div style="margin-top: 10px;">
                <a href="list.html" style="color: #369; text-decoration: none;">View render history</a>
            </div>
        </div>
    </div>
    
    <script>
    const papers = [{"arxiv_id": "2503.16426", "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding", "authors": ["Keyan Chen", "Chenyang Liu", "Bowen Chen", "Wenyuan Li", "Zhengxia Zou", "Zhenwei Shi"], "published": "2025-03-20T17:59:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16426v1", "categories": ["cs.CV"], "abstract": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's)."}, {"arxiv_id": "2503.16419", "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", " Shaochen", " Zhong", "Hanjie Chen", "Xia Hu"], "published": "2025-03-20T17:59:38Z", "citations": 0, "tweets": 15, "paper_link": "http://arxiv.org/abs/2503.16419v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."}, {"arxiv_id": "2503.16418", "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity", "authors": ["Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Hao Kang", "Xin Lu"], "published": "2025-03-20T17:59:34Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2503.16418v1", "categories": ["cs.CV", "cs.LG"], "abstract": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community."}, {"arxiv_id": "2503.16416", "title": "Survey on Evaluation of LLM-based Agents", "authors": ["Asaf Yehudai", "Lilach Eden", "Alan Li", "Guy Uziel", "Yilun Zhao", "Roy Bar-Haim", "Arman Cohan", "Michal Shmueli-Scheuer"], "published": "2025-03-20T17:59:23Z", "citations": 0, "tweets": 12, "paper_link": "http://arxiv.org/abs/2503.16416v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch."}, {"arxiv_id": "2503.16412", "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation", "authors": ["Ananta R. Bhattarai", "Xingzhe He", "Alla Sheffer", "Helge Rhodin"], "published": "2025-03-20T17:59:12Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16412v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation."}, {"arxiv_id": "2503.16413", "title": "M3: 3D-Spatial MultiModal Memory", "authors": ["Xueyan Zou", "Yuchen Song", "Ri-Zhao Qiu", "Xuanbin Peng", "Jianglong Ye", "Sifei Liu", "Xiaolong Wang"], "published": "2025-03-20T17:59:12Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16413v1", "categories": ["cs.CV", "cs.RO"], "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation."}, {"arxiv_id": "2503.16402", "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination", "authors": ["Yifan Sun", "Han Wang", "Dongbai Li", "Gang Wang", "Huan Zhang"], "published": "2025-03-20T17:55:04Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16402v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment."}, {"arxiv_id": "2503.16401", "title": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them", "authors": ["Guanyu Chen", "Peiyang Wang", "Tianren Zhang", "Feng Chen"], "published": "2025-03-20T17:54:42Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.16401v1", "categories": ["cs.LG"], "abstract": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning."}, {"arxiv_id": "2503.16399", "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World", "authors": ["Chen Chen", "Zhirui Wang", "Taowei Sheng", "Yi Jiang", "Yundu Li", "Peirui Cheng", "Luning Zhang", "Kaiqiang Chen", "Yanfeng Hu", "Xue Yang", "Xian Sun"], "published": "2025-03-20T17:54:29Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16399v1", "categories": ["cs.CV", "cs.AI"], "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ."}, {"arxiv_id": "2503.16392", "title": "Graph of Effort: Quantifying Risk of AI Usage for Vulnerability\n  Assessment", "authors": ["Anket Mehra", "Andreas Aßmuth", "Malte Prieß"], "published": "2025-03-20T17:52:42Z", "citations": 0, "tweets": 7, "paper_link": "http://arxiv.org/abs/2503.16392v1", "categories": ["cs.CR", "cs.AI", "cs.DC"], "abstract": "With AI-based software becoming widely available, the risk of exploiting its\ncapabilities, such as high automation and complex pattern recognition, could\nsignificantly increase. An AI used offensively to attack non-AI assets is\nreferred to as offensive AI.\n  Current research explores how offensive AI can be utilized and how its usage\ncan be classified. Additionally, methods for threat modeling are being\ndeveloped for AI-based assets within organizations. However, there are gaps\nthat need to be addressed. Firstly, there is a need to quantify the factors\ncontributing to the AI threat. Secondly, there is a requirement to create\nthreat models that analyze the risk of being attacked by AI for vulnerability\nassessment across all assets of an organization. This is particularly crucial\nand challenging in cloud environments, where sophisticated infrastructure and\naccess control landscapes are prevalent. The ability to quantify and further\nanalyze the threat posed by offensive AI enables analysts to rank\nvulnerabilities and prioritize the implementation of proactive countermeasures.\n  To address these gaps, this paper introduces the Graph of Effort, an\nintuitive, flexible, and effective threat modeling method for analyzing the\neffort required to use offensive AI for vulnerability exploitation by an\nadversary. While the threat model is functional and provides valuable support,\nits design choices need further empirical validation in future work."}, {"arxiv_id": "2503.16390", "title": "Phonons in Electron Crystals with Berry Curvature", "authors": ["Junkai Dong", "Ophelia Evelyn Sommer", "Tomohiro Soejima", "Daniel E. Parker", "Ashvin Vishwanath"], "published": "2025-03-20T17:49:59Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16390v1", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "abstract": "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology."}, {"arxiv_id": "2503.16385", "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation", "authors": ["Yijia Luo", "Yulin Song", "Xingyao Zhang", "Jiaheng Liu", "Weixun Wang", "GengRu Chen", "Wenbo Su", "Bo Zheng"], "published": "2025-03-20T17:46:38Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16385v1", "categories": ["cs.AI"], "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."}, {"arxiv_id": "2503.16376", "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images", "authors": ["Leyang Wang", "Joice Lin"], "published": "2025-03-20T17:39:06Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16376v1", "categories": ["cs.CV"], "abstract": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG."}, {"arxiv_id": "2503.16366", "title": "Dynamic Metasurface-Backed Luneburg Lens for Multiplexed Backscatter\n  Communication", "authors": ["Samuel Kim", "Tim Sleasman", "Avrami Rakovsky", "Ra'id Awadallah", "David B. Shrekenhamer"], "published": "2025-03-20T17:22:11Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.16366v1", "categories": ["physics.optics", "eess.SP"], "abstract": "Backscatter communications is attractive for its low power requirements due\nto the lack of actively radiating components; however, commonly used devices\nare typically limited in range and functionality. Here, we design and\ndemonstrate a flattened Luneburg lens combined with a spatially-tunable dynamic\nmetasurface to create a low-power backscatter communicator. The Luneburg lens\nis a spherically-symmetric lens that focuses a collimated beam from any\ndirection, enabling a wide field-of-view with no aberrations. By applying\nquasi-conformal transformation optics (QCTO), we design a flattened Luneburg\nlens to facilitate its seamless interface with the planar metasurface. The\ngradient index of the Luneburg lens is realized through additive manufacturing.\nWe show that the flattened Luneburg lens with a reflective surface at the\nflattened focal plane is able to achieve diffraction-limited retroreflection,\nenabling long-range backscatter communication. When an interrogator transmits\ntowards the metasurface-backed Luneburg lens, the device can modulate the\nreflected signal phase across a wide field of regard to communicate data. We\nexperimentally show that the spatial control over the metasurface allows\ndifferent bit streams to be simultaneously communicated in different\ndirections. Additionally, we show that the device is able to prevent\neavesdroppers from receiving information, thus securing communications."}, {"arxiv_id": "2503.16356", "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners", "authors": ["Yunzhi Yao", "Jizhan Fang", "Jia-Chen Gu", "Ningyu Zhang", "Shumin Deng", "Huajun Chen", "Nanyun Peng"], "published": "2025-03-20T17:14:34Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16356v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."}, {"arxiv_id": "2503.16350", "title": "An Evaluation Tool for Backbone Extraction Techniques in Weighted\n  Complex Networks", "authors": ["Ali Yassin", "Abbas Haidar", "Hocine Cherifi", "Hamida Seba", "Olivier Togni"], "published": "2025-03-20T17:08:35Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16350v1", "categories": ["cs.SI", "cs.SE"], "abstract": "Networks are essential for analyzing complex systems. However, their growing\nsize necessitates backbone extraction techniques aimed at reducing their size\nwhile retaining critical features. In practice, selecting, implementing, and\nevaluating the most suitable backbone extraction method may be challenging.\nThis paper introduces netbone, a Python package designed for assessing the\nperformance of backbone extraction techniques in weighted networks. Its\ncomparison framework is the standout feature of netbone. Indeed, the tool\nincorporates state-of-the-art backbone extraction techniques. Furthermore, it\nprovides a comprehensive suite of evaluation metrics allowing users to evaluate\ndifferent backbones techniques. We illustrate the flexibility and effectiveness\nof netbone through the US air transportation network analysis. We compare the\nperformance of different backbone extraction techniques using the evaluation\nmetrics. We also show how users can integrate a new backbone extraction method\ninto the comparison framework. netbone is publicly available as an open-source\ntool, ensuring its accessibility to researchers and practitioners. Promoting\nstandardized evaluation practices contributes to the advancement of backbone\nextraction techniques and fosters reproducibility and comparability in research\nefforts. We anticipate that netbone will serve as a valuable resource for\nresearchers and practitioners enabling them to make informed decisions when\nselecting backbone extraction techniques to gain insights into the structural\nand functional properties of complex systems."}, {"arxiv_id": "2503.16345", "title": "Finite-coupling spectrum of O(N) model in AdS", "authors": ["Jonáš Dujava", "Petr Vaško"], "published": "2025-03-20T17:01:22Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16345v1", "categories": ["hep-th"], "abstract": "We determine the scaling dimensions in the boundary $\\mathsf{CFT}_{d}$\ncorresponding to the $\\mathsf{O}(N)$ model in $\\mathsf{EAdS}_{d+1}$. The\n$\\mathsf{CFT}$ data accessible to the 4-point boundary correlator of\nfundamental fields are extracted in $d=2$ and $d=4$, at a finite coupling, and\nto the leading nontrivial order in the $1/N$ expansion. We focus on the\nnon-singlet sectors, namely the anti-symmetric and symmetric traceless\nirreducible representations of the $\\mathsf{O}(N)$ group, extending the\nprevious results that considered only the singlet sector. Studying the\nnon-singlet sector requires an understanding of the crossed-channel diagram\ncontributions to the $s$-channel conformal block decomposition. Building upon\nan existing computation, we present general formulas in $d=2$ and $d=4$ for the\ncontribution of a $t$-channel conformal block to the anomalous dimensions of\n$s$-channel double-twist operators, derived for external scalar operators with\nequal scaling dimensions. Up to some technical details, this eventually leads\nto the complete picture of $1/N$ corrections to the $\\mathsf{CFT}$ data in the\ninteracting theory."}, {"arxiv_id": "2503.16335", "title": "Enhancing Software Quality Assurance with an Adaptive Differential\n  Evolution based Quantum Variational Autoencoder-Transformer Model", "authors": ["Seshu Babu Barma", "Mohanakrishnan Hariharan", "Satish Arvapalli"], "published": "2025-03-20T16:55:38Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16335v1", "categories": ["cs.AI", "cs.ET"], "abstract": "An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%."}, {"arxiv_id": "2503.16334", "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "authors": ["Ying Shen", "Lifu Huang"], "published": "2025-03-20T16:55:26Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16334v1", "categories": ["cs.CL"], "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."}, {"arxiv_id": "2503.16326", "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence", "authors": ["Long Yuan", "Fengran Mo", "Kaiyu Huang", "Wenjie Wang", "Wangyuxuan Zhai", "Xiaoyu Zhu", "You Li", "Jinan Xu", "Jian-Yun Nie"], "published": "2025-03-20T16:45:48Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16326v1", "categories": ["cs.AI"], "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication."}, {"arxiv_id": "2503.17363", "title": "Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural\n  Language Self-Critique", "authors": ["Yansi Li", "Jiahao Xu", "Tian Liang", "Xingyu Chen", "Zhiwei He", "Qiuzhi Liu", "Rui Wang", "Zhuosheng Zhang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published": "2025-03-21T17:59:55Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2503.17363v1", "categories": ["cs.CL"], "abstract": "Enhancing the reasoning capabilities of large language models (LLMs),\nparticularly for complex tasks requiring multi-step logical deductions, remains\na significant challenge. Traditional inference time scaling methods utilize\nscalar reward signals from process reward models to evaluate candidate\nreasoning steps, but these scalar rewards lack the nuanced qualitative\ninformation essential for understanding and justifying each step. In this\npaper, we propose a novel inference-time scaling approach -- stepwise natural\nlanguage self-critique (PANEL), which employs self-generated natural language\ncritiques as feedback to guide the step-level search process. By generating\nrich, human-readable critiques for each candidate reasoning step, PANEL retains\nessential qualitative information, facilitating better-informed decision-making\nduring inference. This approach bypasses the need for task-specific verifiers\nand the associated training overhead, making it broadly applicable across\ndiverse tasks. Experimental results on challenging reasoning benchmarks,\nincluding AIME and GPQA, demonstrate that PANEL significantly enhances\nreasoning performance, outperforming traditional scalar reward-based methods.\nOur code is available at https://github.com/puddingyeah/PANEL to support and\nencourage future research in this promising field."}, {"arxiv_id": "2503.17357", "title": "Filtered Rayleigh-Ritz is all you need", "authors": ["Ryan Abbott", "Daniel C. Hackett", "George T. Fleming", "Dimitra A. Pefkou", "Michael L. Wagman"], "published": "2025-03-21T17:58:21Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.17357v1", "categories": ["hep-lat", "cs.NA", "math.NA"], "abstract": "Recent work has shown that the (block) Lanczos algorithm can be used to\nextract approximate energy spectra and matrix elements from (matrices of)\ncorrelation functions in quantum field theory, and identified exact\ncoincidences between Lanczos analysis methods and others. In this work, we note\nanother coincidence: the Lanczos algorithm is equivalent to the well-known\nRayleigh-Ritz method applied to Krylov subspaces. Rayleigh-Ritz provides\noptimal eigenvalue approximations within subspaces; we find that spurious-state\nfiltering allows these optimality guarantees to be retained in the presence of\nstatistical noise. We explore the relation between Lanczos and Prony's method,\ntheir block generalizations, generalized pencil of functions (GPOF), and\nmethods based on the generalized eigenvalue problem (GEVP), and find they all\nfall into a larger \"Prony-Ritz equivalence class\", identified as all methods\nwhich solve a finite-dimensional spectrum exactly given sufficient correlation\nfunction (matrix) data. This equivalence allows simpler and more numerically\nstable implementations of (block) Lanczos analyses."}, {"arxiv_id": "2503.17352", "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "published": "2025-03-21T17:52:43Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.17352v1", "categories": ["cs.CV", "cs.CL"], "abstract": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker."}, {"arxiv_id": "2503.17351", "title": "Time-Series U-Net with Recurrence for Noise-Robust Imaging\n  Photoplethysmography", "authors": ["Vineet R. Shenoy", "Shaoju Wu", "Armand Comas", "Tim K. Marks", "Suhas Lohit", "Hassan Mansour"], "published": "2025-03-21T17:52:33Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.17351v1", "categories": ["cs.CV"], "abstract": "Remote estimation of vital signs enables health monitoring for situations in\nwhich contact-based devices are either not available, too intrusive, or too\nexpensive. In this paper, we present a modular, interpretable pipeline for\npulse signal estimation from video of the face that achieves state-of-the-art\nresults on publicly available datasets.Our imaging photoplethysmography (iPPG)\nsystem consists of three modules: face and landmark detection, time-series\nextraction, and pulse signal/pulse rate estimation. Unlike many deep learning\nmethods that make use of a single black-box model that maps directly from input\nvideo to output signal or heart rate, our modular approach enables each of the\nthree parts of the pipeline to be interpreted individually. The pulse signal\nestimation module, which we call TURNIP (Time-Series U-Net with Recurrence for\nNoise-Robust Imaging Photoplethysmography), allows the system to faithfully\nreconstruct the underlying pulse signal waveform and uses it to measure heart\nrate and pulse rate variability metrics, even in the presence of motion. When\nparts of the face are occluded due to extreme head poses, our system explicitly\ndetects such \"self-occluded\" regions and maintains estimation robustness\ndespite the missing information. Our algorithm provides reliable heart rate\nestimates without the need for specialized sensors or contact with the skin,\noutperforming previous iPPG methods on both color (RGB) and near-infrared (NIR)\ndatasets."}, {"arxiv_id": "2503.17349", "title": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language\n  Models", "authors": ["Jianing Qi", "Jiawei Liu", "Hao Tang", "Zhigang Zhu"], "published": "2025-03-21T17:51:14Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.17349v1", "categories": ["cs.CV"], "abstract": "Vision-Language Models (VLMs) excel at identifying and describing objects but\nstruggle with spatial reasoning such as accurately understanding the relative\npositions of objects. Inspired by the dual-pathway (ventral-dorsal) model of\nhuman vision, we investigate why VLMs fail spatial tasks despite strong object\nrecognition capabilities. Our interpretability-driven analysis reveals a\ncritical underlying cause: vision embeddings in VLMs are treated primarily as\nsemantic ``bag-of-tokens,\" overshadowing subtle yet crucial positional cues due\nto their disproportionately large embedding norms. We validate this insight\nthrough extensive diagnostic experiments, demonstrating minimal performance\nimpact when token orders or fine-grained spatial details are removed. Guided by\nthese findings, we propose simple, interpretable interventions, including\nnormalizing vision embedding norms and extracting mid-layer spatially rich\nfeatures, to restore spatial awareness. Empirical results on both our synthetic\ndata and standard benchmarks demonstrate improved spatial reasoning\ncapabilities, highlighting the value of interpretability-informed design\nchoices. Our study not only uncovers fundamental limitations in current VLM\narchitectures but also provides actionable insights for enhancing structured\nperception of visual scenes."}, {"arxiv_id": "2503.18944", "title": "DINO in the Room: Leveraging 2D Foundation Models for 3D Segmentation", "authors": ["Karim Abou Zeid", "Kadir Yilmaz", "Daan de Geus", "Alexander Hermans", "David Adrian", "Timm Linder", "Bastian Leibe"], "published": "2025-03-24T17:59:11Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.18944v1", "categories": ["cs.CV"], "abstract": "Vision foundation models (VFMs) trained on large-scale image datasets provide\nhigh-quality features that have significantly advanced 2D visual recognition.\nHowever, their potential in 3D vision remains largely untapped, despite the\ncommon availability of 2D images alongside 3D point cloud datasets. While\nsignificant research has been dedicated to 2D-3D fusion, recent\nstate-of-the-art 3D methods predominantly focus on 3D data, leaving the\nintegration of VFMs into 3D models underexplored. In this work, we challenge\nthis trend by introducing DITR, a simple yet effective approach that extracts\n2D foundation model features, projects them to 3D, and finally injects them\ninto a 3D point cloud segmentation model. DITR achieves state-of-the-art\nresults on both indoor and outdoor 3D semantic segmentation benchmarks. To\nenable the use of VFMs even when images are unavailable during inference, we\nfurther propose to distill 2D foundation models into a 3D backbone as a\npretraining task. By initializing the 3D backbone with knowledge distilled from\n2D VFMs, we create a strong basis for downstream 3D segmentation tasks,\nultimately boosting performance across various datasets."}, {"arxiv_id": "2503.18943", "title": "SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language\n  Models for Long-Form Video Understanding", "authors": ["Mingze Xu", "Mingfei Gao", "Shiyu Li", "Jiasen Lu", "Zhe Gan", "Zhengfeng Lai", "Meng Cao", "Kai Kang", "Yinfei Yang", "Afshin Dehghan"], "published": "2025-03-24T17:59:07Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.18943v1", "categories": ["cs.CV"], "abstract": "We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of\nvideo large language models (LLMs) offering a token-efficient solution for\nlong-form video understanding. This model family employs the two-stream\nSlowFast mechanism, enabling efficient modeling of long-range temporal context\nto meet the demand for lightweight, mobile-friendly Video LLMs. We provide\nmodels ranging from 1B to 7B parameters, optimized through a streamlined\ntraining pipeline and a high-quality data mixture composed of publicly\navailable datasets. Experimental results demonstrate that SF-LLaVA-1.5 achieves\ncompetitive performance on a wide range of video and image benchmarks, with\nrobust results across all model sizes. Notably, SF-LLaVA-1.5 achieves\nstate-of-the-art results in long-form video understanding (e.g., LongVideoBench\nand MLVU) and excels at small scales (1B and 3B) across various video\nbenchmarks."}, {"arxiv_id": "2503.18942", "title": "Video-T1: Test-Time Scaling for Video Generation", "authors": ["Fangfu Liu", "Hanyang Wang", "Yimo Cai", "Kaiyan Zhang", "Xiaohang Zhan", "Yueqi Duan"], "published": "2025-03-24T17:59:04Z", "citations": 0, "tweets": 11, "paper_link": "http://arxiv.org/abs/2503.18942v1", "categories": ["cs.CV", "cs.AI"], "abstract": "With the scale capability of increasing training data, model size, and\ncomputational cost, video generation has achieved impressive results in digital\ncreation, enabling users to express creativity across various domains.\nRecently, researchers in Large Language Models (LLMs) have expanded the scaling\nto test-time, which can significantly improve LLM performance by using more\ninference-time computation. Instead of scaling up video foundation models\nthrough expensive training costs, we explore the power of Test-Time Scaling\n(TTS) in video generation, aiming to answer the question: if a video generation\nmodel is allowed to use non-trivial amount of inference-time compute, how much\ncan it improve generation quality given a challenging text prompt. In this\nwork, we reinterpret the test-time scaling of video generation as a searching\nproblem to sample better trajectories from Gaussian noise space to the target\nvideo distribution. Specifically, we build the search space with test-time\nverifiers to provide feedback and heuristic algorithms to guide searching\nprocess. Given a text prompt, we first explore an intuitive linear search\nstrategy by increasing noise candidates at inference time. As full-step\ndenoising all frames simultaneously requires heavy test-time computation costs,\nwe further design a more efficient TTS method for video generation called\nTree-of-Frames (ToF) that adaptively expands and prunes video branches in an\nautoregressive manner. Extensive experiments on text-conditioned video\ngeneration benchmarks demonstrate that increasing test-time compute\nconsistently leads to significant improvements in the quality of videos.\nProject page: https://liuff19.github.io/Video-T1"}, {"arxiv_id": "2503.18941", "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval", "authors": ["Hongru Cai", "Yongqi Li", "Ruifeng Yuan", "Wenjie Wang", "Zhen Zhang", "Wenjie Li", "Tat-Seng Chua"], "published": "2025-03-24T17:59:03Z", "citations": 0, "tweets": 26, "paper_link": "http://arxiv.org/abs/2503.18941v1", "categories": ["cs.IR", "cs.CL"], "abstract": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems."}, {"arxiv_id": "2503.18938", "title": "AdaWorld: Learning Adaptable World Models with Latent Actions", "authors": ["Shenyuan Gao", "Siyuan Zhou", "Yilun Du", "Jun Zhang", "Chuang Gan"], "published": "2025-03-24T17:58:15Z", "citations": 0, "tweets": 9, "paper_link": "http://arxiv.org/abs/2503.18938v1", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "abstract": "World models aim to learn action-controlled prediction models and have proven\nessential for the development of intelligent agents. However, most existing\nworld models rely heavily on substantial action-labeled data and costly\ntraining, making it challenging to adapt to novel environments with\nheterogeneous actions through limited interactions. This limitation can hinder\ntheir applicability across broader domains. To overcome this challenge, we\npropose AdaWorld, an innovative world model learning approach that enables\nefficient adaptation. The key idea is to incorporate action information during\nthe pretraining of world models. This is achieved by extracting latent actions\nfrom videos in a self-supervised manner, capturing the most critical\ntransitions between frames. We then develop an autoregressive world model that\nconditions on these latent actions. This learning paradigm enables highly\nadaptable world models, facilitating efficient transfer and learning of new\nactions even with limited interactions and finetuning. Our comprehensive\nexperiments across multiple environments demonstrate that AdaWorld achieves\nsuperior performance in both simulation quality and visual planning."}, {"arxiv_id": "2503.19910", "title": "CoLLM: A Large Language Model for Composed Image Retrieval", "authors": ["Chuong Huynh", "Jinyu Yang", "Ashish Tawari", "Mubarak Shah", "Son Tran", "Raffay Hamid", "Trishul Chilimbi", "Abhinav Shrivastava"], "published": "2025-03-25T17:59:50Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.19910v1", "categories": ["cs.CV", "cs.IR"], "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images\nbased on a multimodal query. Typical training data consists of triplets\ncontaining a reference image, a textual description of desired modifications,\nand the target image, which are expensive and time-consuming to acquire. The\nscarcity of CIR datasets has led to zero-shot approaches utilizing synthetic\ntriplets or leveraging vision-language models (VLMs) with ubiquitous\nweb-crawled image-caption pairs. However, these methods have significant\nlimitations: synthetic triplets suffer from limited scale, lack of diversity,\nand unnatural modification text, while image-caption pairs hinder joint\nembedding learning of the multimodal query due to the absence of triplet data.\nMoreover, existing approaches struggle with complex and nuanced modification\ntexts that demand sophisticated fusion and understanding of vision and language\nmodalities. We present CoLLM, a one-stop framework that effectively addresses\nthese limitations. Our approach generates triplets on-the-fly from\nimage-caption pairs, enabling supervised training without manual annotation. We\nleverage Large Language Models (LLMs) to generate joint embeddings of reference\nimages and modification texts, facilitating deeper multimodal fusion.\nAdditionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset\ncomprising 3.4M samples, and refine existing CIR benchmarks (CIRR and\nFashion-IQ) to enhance evaluation reliability. Experimental results demonstrate\nthat CoLLM achieves state-of-the-art performance across multiple CIR benchmarks\nand settings. MTCIR yields competitive results, with up to 15% performance\nimprovement. Our refined benchmarks provide more reliable evaluation metrics\nfor CIR models, contributing to the advancement of this important field."}, {"arxiv_id": "2503.19909", "title": "In the Magma chamber: Update and challenges in ground-truth\n  vulnerabilities revival for automatic input generator comparison", "authors": ["Timothée Riom", "Sabine Houy", "Bruno Kreyssig", "Alexandre Bartel"], "published": "2025-03-25T17:59:27Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.19909v1", "categories": ["cs.SE", "cs.CR"], "abstract": "Fuzzing is a well-established technique for detecting bugs and\nvulnerabilities. With the surge of fuzzers and fuzzer platforms being developed\nsuch as AFL and OSSFuzz rises the necessity to benchmark these tools'\nperformance. A common problem is that vulnerability benchmarks are based on\nbugs in old software releases. For this very reason, Magma introduced the\nnotion of forward-porting to reintroduce vulnerable code in current software\nreleases. While their results are promising, the state-of-the-art lacks an\nupdate on the maintainability of this approach over time. Indeed, adding the\nvulnerable code to a recent software version might either break its\nfunctionality or make the vulnerable code no longer reachable. We characterise\nthe challenges with forward-porting by reassessing the portability of Magma's\nCVEs four years after its release and manually reintroducing the\nvulnerabilities in the current software versions. We find the straightforward\nprocess efficient for 17 of the 32 CVEs in our study. We further investigate\nwhy a trivial forward-porting process fails in the 15 other CVEs. This involves\nidentifying the commits breaking the forward-porting process and reverting them\nin addition to the bug fix. While we manage to complete the process for nine of\nthese CVEs, we provide an update on all 15 and explain the challenges we have\nbeen confronted with in this process. Thereby, we give the basis for future\nwork towards a sustainable forward-ported fuzzing benchmark."}, {"arxiv_id": "2503.19906", "title": "AvatarArtist: Open-Domain 4D Avatarization", "authors": ["Hongyu Liu", "Xuan Wang", "Ziyu Wan", "Yue Ma", "Jingye Chen", "Yanbo Fan", "Yujun Shen", "Yibing Song", "Qifeng Chen"], "published": "2025-03-25T17:59:03Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.19906v1", "categories": ["cs.CV"], "abstract": "This work focuses on open-domain 4D avatarization, with the purpose of\ncreating a 4D avatar from a portrait image in an arbitrary style. We select\nparametric triplanes as the intermediate 4D representation and propose a\npractical training paradigm that takes advantage of both generative adversarial\nnetworks (GANs) and diffusion models. Our design stems from the observation\nthat 4D GANs excel at bridging images and triplanes without supervision yet\nusually face challenges in handling diverse data distributions. A robust 2D\ndiffusion prior emerges as the solution, assisting the GAN in transferring its\nexpertise across various domains. The synergy between these experts permits the\nconstruction of a multi-domain image-triplane dataset, which drives the\ndevelopment of a general 4D avatar creator. Extensive experiments suggest that\nour model, AvatarArtist, is capable of producing high-quality 4D avatars with\nstrong robustness to various source image domains. The code, the data, and the\nmodels will be made publicly available to facilitate future studies.."}, {"arxiv_id": "2503.19903", "title": "Scaling Vision Pre-Training to 4K Resolution", "authors": ["Baifeng Shi", "Boyi Li", "Han Cai", "Yao Lu", "Sifei Liu", "Marco Pavone", "Jan Kautz", "Song Han", "Trevor Darrell", "Pavlo Molchanov", "Hongxu Yin"], "published": "2025-03-25T17:58:37Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.19903v1", "categories": ["cs.CV"], "abstract": "High-resolution perception of visual details is crucial for daily tasks.\nCurrent vision pre-training, however, is still limited to low resolutions\n(e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images.\nWe introduce PS3 that scales CLIP-style vision pre-training to 4K resolution\nwith a near-constant cost. Instead of contrastive learning on global image\nrepresentation, PS3 is pre-trained by selectively processing local regions and\ncontrasting them with local detailed captions, enabling high-resolution\nrepresentation learning with greatly reduced computational overhead. The\npre-trained PS3 is able to both encode the global image at low resolution and\nselectively process local high-resolution regions based on their saliency or\nrelevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the\nresulting model, named VILA-HD, significantly improves high-resolution visual\nperception compared to baselines without high-resolution vision pre-training\nsuch as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks\nappealing scaling properties of VILA-HD, including scaling up resolution for\nfree and scaling up test-time compute for better performance. Compared to state\nof the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL\nacross multiple benchmarks and achieves better efficiency than latest token\npruning approaches. Finally, we find current benchmarks do not require\n4K-resolution perception, which motivates us to propose 4KPro, a new benchmark\nof image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs,\nincluding a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x\nspeedup over Qwen2-VL."}, {"arxiv_id": "2503.19902", "title": "ICE: Intrinsic Concept Extraction from a Single Image via Diffusion\n  Models", "authors": ["Fernando Julio Cendra", "Kai Han"], "published": "2025-03-25T17:58:29Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.19902v1", "categories": ["cs.CV"], "abstract": "The inherent ambiguity in defining visual concepts poses significant\nchallenges for modern generative models, such as the diffusion-based\nText-to-Image (T2I) models, in accurately learning concepts from a single\nimage. Existing methods lack a systematic way to reliably extract the\ninterpretable underlying intrinsic concepts. To address this challenge, we\npresent ICE, short for Intrinsic Concept Extraction, a novel framework that\nexclusively utilizes a T2I model to automatically and systematically extract\nintrinsic concepts from a single image. ICE consists of two pivotal stages. In\nthe first stage, ICE devises an automatic concept localization module to\npinpoint relevant text-based concepts and their corresponding masks within the\nimage. This critical stage streamlines concept initialization and provides\nprecise guidance for subsequent analysis. The second stage delves deeper into\neach identified mask, decomposing the object-level concepts into intrinsic\nconcepts and general concepts. This decomposition allows for a more granular\nand interpretable breakdown of visual elements. Our framework demonstrates\nsuperior performance on intrinsic concept extraction from a single image in an\nunsupervised manner. Project page: https://visual-ai.github.io/ice"}, {"arxiv_id": "2503.20786", "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "authors": ["Sondos Mahmoud Bsharat", "Mukul Ranjan", "Aidar Myrzakhan", "Jiacheng Liu", "Bowei Guo", "Shengkun Tang", "Zhuang Liu", "Yuanzhi Li", "Zhiqiang Shen"], "published": "2025-03-26T17:59:56Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.20786v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU."}, {"arxiv_id": "2503.20784", "title": "FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with\n  Feature Banks", "authors": ["Jinwei Li", "Huan-ang Gao", "Wenyi Li", "Haohan Chi", "Chenyu Liu", "Chenxi Du", "Yiqian Liu", "Mingju Gao", "Guiyu Zhang", "Zongzheng Zhang", "Li Yi", "Yao Yao", "Jingwei Zhao", "Hongyang Li", "Yikai Wang", "Hao Zhao"], "published": "2025-03-26T17:59:31Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2503.20784v1", "categories": ["cs.CV"], "abstract": "With the rapid advancements in diffusion models and 3D generation techniques,\ndynamic 3D content generation has become a crucial research area. However,\nachieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal\nconsistency remains a challenging task. Inspired by recent findings that\npretrained diffusion features capture rich correspondences, we propose FB-4D, a\nnovel 4D generation framework that integrates a Feature Bank mechanism to\nenhance both spatial and temporal consistency in generated frames. In FB-4D, we\nstore features extracted from previous frames and fuse them into the process of\ngenerating subsequent frames, ensuring consistent characteristics across both\ntime and multiple views. To ensure a compact representation, the Feature Bank\nis updated by a proposed dynamic merging mechanism. Leveraging this Feature\nBank, we demonstrate for the first time that generating additional reference\nsequences through multiple autoregressive iterations can continuously improve\ngeneration performance. Experimental results show that FB-4D significantly\noutperforms existing methods in terms of rendering quality, spatial-temporal\nconsistency, and robustness. It surpasses all multi-view generation tuning-free\napproaches by a large margin and achieves performance on par with\ntraining-based methods."}, {"arxiv_id": "2503.20783", "title": "Understanding R1-Zero-Like Training: A Critical Perspective", "authors": ["Zichen Liu", "Changyu Chen", "Wenjun Li", "Penghui Qi", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "published": "2025-03-26T17:59:14Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2503.20783v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero."}, {"arxiv_id": "2503.20776", "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields", "authors": ["Shijie Zhou", "Hui Ren", "Yijia Weng", "Shuwang Zhang", "Zhen Wang", "Dejia Xu", "Zhiwen Fan", "Suya You", "Zhangyang Wang", "Leonidas Guibas", "Achuta Kadambi"], "published": "2025-03-26T17:56:16Z", "citations": 0, "tweets": 13, "paper_link": "http://arxiv.org/abs/2503.20776v1", "categories": ["cs.CV"], "abstract": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."}, {"arxiv_id": "2503.20757", "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search", "authors": ["Yunhai Hu", "Yilun Zhao", "Chen Zhao", "Arman Cohan"], "published": "2025-03-26T17:46:08Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.20757v1", "categories": ["cs.CL"], "abstract": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."}, {"arxiv_id": "2503.21774", "title": "Optimal Stepsize for Diffusion Sampling", "authors": ["Jianning Pei", "Han Hu", "Shuyang Gu"], "published": "2025-03-27T17:59:46Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.21774v1", "categories": ["cs.CV"], "abstract": "Diffusion models achieve remarkable generation quality but suffer from\ncomputational intensive sampling due to suboptimal step discretization. While\nexisting works focus on optimizing denoising directions, we address the\nprincipled design of stepsize schedules. This paper proposes Optimal Stepsize\nDistillation, a dynamic programming framework that extracts theoretically\noptimal schedules by distilling knowledge from reference trajectories. By\nreformulating stepsize optimization as recursive error minimization, our method\nguarantees global discretization bounds through optimal substructure\nexploitation. Crucially, the distilled schedules demonstrate strong robustness\nacross architectures, ODE solvers, and noise schedules. Experiments show 10x\naccelerated text-to-image generation while preserving 99.4% performance on\nGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps."}, {"arxiv_id": "2503.21760", "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "published": "2025-03-27T17:57:28Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.21760v1", "categories": ["cs.CL"], "abstract": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks."}, {"arxiv_id": "2503.21757", "title": "Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck", "authors": ["Adrian Bulat", "Yassine Ouali", "Georgios Tzimiropoulos"], "published": "2025-03-27T17:57:07Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.21757v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "In this work, we aim to compress the vision tokens of a Large Vision Language\nModel (LVLM) into a representation that is simultaneously suitable for (a)\ngenerative and (b) discriminative tasks, (c) is nearly lossless, and (d) is\nstorage-efficient. We propose a novel compression approach, called Fwd2Bot,\nthat uses the LVLM itself to compress the visual information in a task-agnostic\nmanner. At the core of Fwd2bot there exists a \"double-forward pass\" training\nstrategy, whereby, during the first forward pass, the LLM (of the LVLM) creates\na bottleneck by condensing the visual information into a small number of\nsummary tokens. Then, using the same LLM, the second forward pass processes the\nlanguage instruction(s) alongside the summary tokens, used as a direct\nreplacement for the image ones. The training signal is provided by two losses:\nan autoregressive one applied after the second pass that provides a direct\noptimization objective for compression, and a contrastive loss, applied after\nthe first pass, that further boosts the representation strength, especially for\ndiscriminative tasks. The training is further enhanced by stage-specific\nadapters. We accompany the proposed method by an in-depth ablation study.\nOverall, Fwd2Bot results in highly-informative compressed representations\nsuitable for both generative and discriminative tasks. For generative tasks, we\noffer a 2x higher compression rate without compromising the generative\ncapabilities, setting a new state-of-the-art result. For discriminative tasks,\nwe set a new state-of-the-art on image retrieval and compositionality."}, {"arxiv_id": "2503.21755", "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness", "authors": ["Dian Zheng", "Ziqi Huang", "Hongbo Liu", "Kai Zou", "Yinan He", "Fan Zhang", "Yuanhan Zhang", "Jingwen He", "Wei-Shi Zheng", "Yu Qiao", "Ziwei Liu"], "published": "2025-03-27T17:57:01Z", "citations": 0, "tweets": 11, "paper_link": "http://arxiv.org/abs/2503.21755v1", "categories": ["cs.CV"], "abstract": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness."}, {"arxiv_id": "2503.21748", "title": "Extracting energy via bosonic Gaussian operations", "authors": ["Frank Ernesto Quintela Rodriguez", "Francesco Anna Mele", "Salvatore Francesco Emanuele Oliviero", "Vittorio Giovannetti", "Ludovico Lami", "Vasco Cavina"], "published": "2025-03-27T17:56:00Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.21748v1", "categories": ["quant-ph"], "abstract": "Quantum thermodynamics is often formulated as a theory with constrained\naccess to operations and resources. In this manuscript, we find a closed\nformula for the Gaussian ergotropy, i.e. the maximum energy that can be\nextracted from bosonic systems governed by quadratic Hamiltonians by means of\nGaussian unitaries only. This formula resembles the well-known eigenvalue-based\nexpression for the standard ergotropy, but is instead formulated using\nsymplectic eigenvalues. We further prove that the Gaussian ergotropy is\nadditive, indicating that the multiple-copy scenario does not benefit from\nGaussian entangling operations. Extending our analysis to the relationship\nbetween ergotropic and entropic functions, we establish bounds linking entropic\nmeasures of Gaussianity to extractable work. Finally, we generalise our\nframework to open systems by studying the optimal state preparation that\nminimises the energy output in a Gaussian channel."}, {"arxiv_id": "2503.22678", "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical\n  Interactions", "authors": ["Mohammad Almansoori", "Komal Kumar", "Hisham Cholakkal"], "published": "2025-03-28T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.22678v1", "categories": ["cs.CL"], "abstract": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}."}, {"arxiv_id": "2503.22676", "title": "TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D\n  Gaussian Splatting", "authors": [" Boyang", " Yu", "Yanlin Jin", "Ashok Veeraraghavan", "Akshat Dave", "Guha Balakrishnan"], "published": "2025-03-28T17:59:43Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.22676v1", "categories": ["cs.CV"], "abstract": "We present TranSplat, a 3D scene rendering algorithm that enables realistic\ncross-scene object transfer (from a source to a target scene) based on the\nGaussian Splatting framework. Our approach addresses two critical challenges:\n(1) precise 3D object extraction from the source scene, and (2) faithful\nrelighting of the transferred object in the target scene without explicit\nmaterial property estimation. TranSplat fits a splatting model to the source\nscene, using 2D object masks to drive fine-grained 3D segmentation. Following\nuser-guided insertion of the object into the target scene, along with automatic\nrefinement of position and orientation, TranSplat derives per-Gaussian radiance\ntransfer functions via spherical harmonic analysis to adapt the object's\nappearance to match the target scene's lighting environment. This relighting\nstrategy does not require explicitly estimating physical scene properties such\nas BRDFs. Evaluated on several synthetic and real-world scenes and objects,\nTranSplat yields excellent 3D object extractions and relighting performance\ncompared to recent baseline methods and visually convincing cross-scene object\ntransfers. We conclude by discussing the limitations of the approach."}, {"arxiv_id": "2503.22674", "title": "QuestBench: Can LLMs ask the right question to acquire information in\n  reasoning tasks?", "authors": ["Belinda Z. Li", "Been Kim", "Zi Wang"], "published": "2025-03-28T17:58:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.22674v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities."}, {"arxiv_id": "2503.22656", "title": "Differential equation quantum solvers: engineering measurements to\n  reduce cost", "authors": ["Annie Paine", "Casper Gyurik", "Antonio Andrea Gentile"], "published": "2025-03-28T17:43:35Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.22656v1", "categories": ["quant-ph", "cs.LG"], "abstract": "Quantum computers have been proposed as a solution for efficiently solving\nnon-linear differential equations (DEs), a fundamental task across diverse\ntechnological and scientific domains. However, a crucial milestone in this\nregard is to design protocols that are hardware-aware, making efficient use of\nlimited available quantum resources. We focus here on promising variational\nmethods derived from scientific machine learning: differentiable quantum\ncircuits (DQC), addressing specifically their cost in number of circuit\nevaluations. Reducing the number of quantum circuit evaluations is particularly\nvaluable in hybrid quantum/classical protocols, where the time required to\ninterface and run quantum hardware at each cycle can impact the total wall-time\nmuch more than relatively inexpensive classical post-processing overhead. Here,\nwe propose and test two sample-efficient protocols for solving non-linear DEs,\nachieving exponential savings in quantum circuit evaluations. These protocols\nare based on redesigning the extraction of information from DQC in a\n``measure-first\" approach, by introducing engineered cost operators similar to\nthe randomized-measurement toolbox (i.e. classical shadows). In benchmark\nsimulations on one and two-dimensional DEs, we report up to $\\sim$ 100 fold\nreductions in circuit evaluations. Our protocols thus hold the promise to\nunlock larger and more challenging non-linear differential equation\ndemonstrations with existing quantum hardware."}, {"arxiv_id": "2503.22655", "title": "Unicorn: Text-Only Data Synthesis for Vision Language Model Training", "authors": ["Xiaomin Yu", "Pengxiang Ding", "Wenjie Zhang", "Siteng Huang", "Songyang Gao", "Chengwei Qin", "Kejian Wu", "Zhaoxin Fan", "Ziyue Qiao", "Donglin Wang"], "published": "2025-03-28T17:43:00Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.22655v1", "categories": ["cs.AI", "cs.CV", "cs.MM"], "abstract": "Training vision-language models (VLMs) typically requires large-scale,\nhigh-quality image-text pairs, but collecting or synthesizing such data is\ncostly. In contrast, text data is abundant and inexpensive, prompting the\nquestion: can high-quality multimodal training data be synthesized purely from\ntext? To tackle this, we propose a cross-integrated three-stage multimodal data\nsynthesis framework, which generates two datasets: Unicorn-1.2M and\nUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we\nconstruct 1.2M semantically diverse high-quality captions by expanding sparse\ncaption seeds using large language models (LLMs). In Stage 2:\nInstruction-Tuning Data Generation, we further process 471K captions into\nmulti-turn instruction-tuning tasks to support complex reasoning. Finally, in\nStage 3: Modality Representation Transfer, these textual captions\nrepresentations are transformed into visual representations, resulting in\ndiverse synthetic image representations. This three-stage process enables us to\nconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for\ninstruction-tuning, without relying on real images. By eliminating the\ndependency on real images while maintaining data quality and diversity, our\nframework offers a cost-effective and scalable solution for VLMs training. Code\nis available at https://github.com/Yu-xm/Unicorn.git."}, {"arxiv_id": "2503.24389", "title": "SU-YOLO: Spiking Neural Network for Efficient Underwater Object\n  Detection", "authors": ["Chenyang Li", "Wenxuan Liu", "Guoqiang Gong", "Xiaobo Ding", "Xian Zhong"], "published": "2025-03-31T17:59:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.24389v1", "categories": ["cs.CV", "cs.NE"], "abstract": "Underwater object detection is critical for oceanic research and industrial\nsafety inspections. However, the complex optical environment and the limited\nresources of underwater equipment pose significant challenges to achieving high\naccuracy and low power consumption. To address these issues, we propose Spiking\nUnderwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the\nlightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a\nnovel spike-based underwater image denoising method based solely on integer\naddition, which enhances the quality of feature maps with minimal computational\noverhead. In addition, we introduce Separated Batch Normalization (SeBN), a\ntechnique that normalizes feature maps independently across multiple time steps\nand is optimized for integration with residual structures to capture the\ntemporal dynamics of SNNs more effectively. The redesigned spiking residual\nblocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO\narchitecture to mitigate spike degradation and enhance the model's feature\nextraction capabilities. Experimental results on URPC2019 underwater dataset\ndemonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an\nenergy consumption of 2.98 mJ, surpassing mainstream SNN models in both\ndetection accuracy and computational efficiency. These results underscore the\npotential of SNNs for engineering applications. The code is available in\nhttps://github.com/lwxfight/snn-underwater."}, {"arxiv_id": "2503.24377", "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for\n  Large Language Models", "authors": ["Rui Wang", "Hongru Wang", "Boyang Xue", "Jianhui Pang", "Shudong Liu", "Yi Chen", "Jiahao Qiu", "Derek Fai Wong", "Heng Ji", "Kam-Fai Wong"], "published": "2025-03-31T17:58:07Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.24377v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field."}, {"arxiv_id": "2503.24376", "title": "Exploring the Effect of Reinforcement Learning on Video Understanding:\n  Insights from SEED-Bench-R1", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "published": "2025-03-31T17:55:23Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2503.24376v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals."}, {"arxiv_id": "2503.24370", "title": "Effectively Controlling Reasoning Models through Thinking Intervention", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Prateek Mittal"], "published": "2025-03-31T17:50:13Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.24370v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs."}, {"arxiv_id": "2503.24368", "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image\n  Segmentation", "authors": ["Xiaoran Zhang", "Eric Z. Chen", "Lin Zhao", "Xiao Chen", "Yikang Liu", "Boris Maihe", "James S. Duncan", "Terrence Chen", "Shanhui Sun"], "published": "2025-03-31T17:47:42Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.24368v1", "categories": ["cs.CV"], "abstract": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications."}, {"arxiv_id": "2504.01951", "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through\n  Online Shopping Data", "authors": ["Massimiliano Luca", "Ciro Beneduce", "Bruno Lepri", "Jacopo Staiano"], "published": "2025-04-02T17:56:08Z", "citations": 0, "tweets": 13, "paper_link": "http://arxiv.org/abs/2504.01951v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "abstract": "With the wide and cross-domain adoption of Large Language Models, it becomes\ncrucial to assess to which extent the statistical correlations in training\ndata, which underlie their impressive performance, hide subtle and potentially\ntroubling biases. Gender bias in LLMs has been widely investigated from the\nperspectives of works, hobbies, and emotions typically associated with a\nspecific gender. In this study, we introduce a novel perspective. We\ninvestigate whether LLMs can predict an individual's gender based solely on\nonline shopping histories and whether these predictions are influenced by\ngender biases and stereotypes. Using a dataset of historical online purchases\nfrom users in the United States, we evaluate the ability of six LLMs to\nclassify gender and we then analyze their reasoning and products-gender\nco-occurrences. Results indicate that while models can infer gender with\nmoderate accuracy, their decisions are often rooted in stereotypical\nassociations between product categories and gender. Furthermore, explicit\ninstructions to avoid bias reduce the certainty of model predictions, but do\nnot eliminate stereotypical patterns. Our findings highlight the persistent\nnature of gender biases in LLMs and emphasize the need for robust\nbias-mitigation strategies."}, {"arxiv_id": "2504.01943", "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "authors": ["Wasi Uddin Ahmad", "Sean Narenthiran", "Somshubra Majumdar", "Aleksander Ficek", "Siddhartha Jain", "Jocelyn Huang", "Vahid Noroozi", "Boris Ginsburg"], "published": "2025-04-02T17:50:31Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.01943v1", "categories": ["cs.CL"], "abstract": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community."}, {"arxiv_id": "2504.01935", "title": "Critical Thinking: Which Kinds of Complexity Govern Optimal Reasoning\n  Length?", "authors": ["Celine Lee", "Alexander M. Rush", "Keyon Vafa"], "published": "2025-04-02T17:45:58Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.01935v1", "categories": ["cs.AI"], "abstract": "Large language models (LLMs) often benefit from verbalized reasoning at\ninference time, but it remains unclear which aspects of task difficulty these\nextra reasoning tokens address. To investigate this question, we formalize a\nframework using deterministic finite automata (DFAs). DFAs offer a formalism\nthrough which we can characterize task complexity through measurable properties\nsuch as run length (number of reasoning steps required) and state-space size\n(decision complexity). We first show that across different tasks and models of\ndifferent sizes and training paradigms, there exists an optimal amount of\nreasoning tokens such that the probability of producing a correct solution is\nmaximized. We then investigate which properties of complexity govern this\ncritical length: we find that task instances with longer corresponding\nunderlying DFA runs (i.e. demand greater latent state-tracking requirements)\ncorrelate with longer reasoning lengths, but, surprisingly, that DFA size (i.e.\nstate-space complexity) does not. We then demonstrate an implication of these\nfindings: being able to predict the optimal number of reasoning tokens for new\nproblems and filtering out non-optimal length answers results in consistent\naccuracy improvements."}, {"arxiv_id": "2504.01933", "title": "Hessian-aware Training for Enhancing DNNs Resilience to Parameter\n  Corruptions", "authors": ["Tahmid Hasan Prato", "Seijoon Kim", "Lizhong Chen", "Sanghyun Hong"], "published": "2025-04-02T17:42:31Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.01933v1", "categories": ["cs.CR", "cs.LG"], "abstract": "Deep neural networks are not resilient to parameter corruptions: even a\nsingle-bitwise error in their parameters in memory can cause an accuracy drop\nof over 10%, and in the worst cases, up to 99%. This susceptibility poses great\nchallenges in deploying models on computing platforms, where adversaries can\ninduce bit-flips through software or bitwise corruptions may occur naturally.\nMost prior work addresses this issue with hardware or system-level approaches,\nsuch as integrating additional hardware components to verify a model's\nintegrity at inference. However, these methods have not been widely deployed as\nthey require infrastructure or platform-wide modifications.\n  In this paper, we propose a new approach to addressing this issue: training\nmodels to be more resilient to bitwise corruptions to their parameters. Our\napproach, Hessian-aware training, promotes models with $flatter$ loss surfaces.\nWe show that, while there have been training methods, designed to improve\ngeneralization through Hessian-based approaches, they do not enhance resilience\nto parameter corruptions. In contrast, models trained with our method\ndemonstrate increased resilience to parameter corruptions, particularly with a\n20$-$50% reduction in the number of bits whose individual flipping leads to a\n90$-$100% accuracy drop. Moreover, we show the synergy between ours and\nexisting hardware and system-level defenses."}, {"arxiv_id": "2504.01931", "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents\n  with Dynamic Evaluation and Selection", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Jindong Gu", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Hamid Palangi", "Tomas Pfister"], "published": "2025-04-02T17:40:47Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.01931v1", "categories": ["cs.CL"], "abstract": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization."}, {"arxiv_id": "2504.02823", "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage\n  Security Inspection", "authors": ["Divya Velayudhan", "Abdelfatah Ahmed", "Mohamad Alansari", "Neha Gour", "Abderaouf Behouch", "Taimur Hassan", "Syed Talal Wasim", "Nabil Maalej", "Muzammal Naseer", "Juergen Gall", "Mohammed Bennamoun", "Ernesto Damiani", "Naoufel Werghi"], "published": "2025-04-03T17:59:12Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.02823v1", "categories": ["cs.CV", "eess.IV"], "abstract": "Advancements in Computer-Aided Screening (CAS) systems are essential for\nimproving the detection of security threats in X-ray baggage scans. However,\ncurrent datasets are limited in representing real-world, sophisticated threats\nand concealment tactics, and existing approaches are constrained by a\nclosed-set paradigm with predefined labels. To address these challenges, we\nintroduce STCray, the first multimodal X-ray baggage security dataset,\ncomprising 46,642 image-caption paired scans across 21 threat categories,\ngenerated using an X-ray scanner for airport security. STCray is meticulously\ndeveloped with our specialized protocol that ensures domain-aware, coherent\ncaptions, that lead to the multi-modal instruction following data in X-ray\nbaggage security. This allows us to train a domain-aware visual AI assistant\nnamed STING-BEE that supports a range of vision-language tasks, including scene\ncomprehension, referring threat localization, visual grounding, and visual\nquestion answering (VQA), establishing novel baselines for multi-modal learning\nin X-ray baggage security. Further, STING-BEE shows state-of-the-art\ngeneralization in cross-domain settings. Code, data, and models are available\nat https://divs1159.github.io/STING-BEE/."}, {"arxiv_id": "2504.02821", "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models", "authors": ["Mateusz Pach", "Shyamgopal Karthik", "Quentin Bouniot", "Serge Belongie", "Zeynep Akata"], "published": "2025-04-03T17:58:35Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.02821v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance\ninterpretability and steerability in Large Language Models (LLMs). In this\nwork, we extend the application of SAEs to Vision-Language Models (VLMs), such\nas CLIP, and introduce a comprehensive framework for evaluating monosemanticity\nin vision representations. Our experimental results reveal that SAEs trained on\nVLMs significantly enhance the monosemanticity of individual neurons while also\nexhibiting hierarchical representations that align well with expert-defined\nstructures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that\napplying SAEs to intervene on a CLIP vision encoder, directly steer output from\nmultimodal LLMs (e.g., LLaVA) without any modifications to the underlying\nmodel. These findings emphasize the practicality and efficacy of SAEs as an\nunsupervised approach for enhancing both the interpretability and control of\nVLMs."}, {"arxiv_id": "2504.02810", "title": "Generative Evaluation of Complex Reasoning in Large Language Models", "authors": ["Haowei Lin", "Xiangyu Wang", "Ruilin Yan", "Baizhou Huang", "Haotian Ye", "Jianhua Zhu", "Zihao Wang", "James Zou", "Jianzhu Ma", "Yitao Liang"], "published": "2025-04-03T17:54:18Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.02810v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."}, {"arxiv_id": "2504.02807", "title": "MegaMath: Pushing the Limits of Open Math Corpora", "authors": ["Fan Zhou", "Zengzhi Wang", "Nikhil Ranjan", "Zhoujun Cheng", "Liping Tang", "Guowei He", "Zhengzhong Liu", "Eric P. Xing"], "published": "2025-04-03T17:52:07Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.02807v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets."}, {"arxiv_id": "2504.02801", "title": "F-ViTA: Foundation Model Guided Visible to Thermal Translation", "authors": ["Jay N. Paranjape", "Celso de Melo", "Vishal M. Patel"], "published": "2025-04-03T17:47:06Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.02801v1", "categories": ["cs.CV"], "abstract": "Thermal imaging is crucial for scene understanding, particularly in low-light\nand nighttime conditions. However, collecting large thermal datasets is costly\nand labor-intensive due to the specialized equipment required for infrared\nimage capture. To address this challenge, researchers have explored\nvisible-to-thermal image translation. Most existing methods rely on Generative\nAdversarial Networks (GANs) or Diffusion Models (DMs), treating the task as a\nstyle transfer problem. As a result, these approaches attempt to learn both the\nmodality distribution shift and underlying physical principles from limited\ntraining data. In this paper, we propose F-ViTA, a novel approach that\nleverages the general world knowledge embedded in foundation models to guide\nthe diffusion process for improved translation. Specifically, we condition an\nInstructPix2Pix Diffusion Model with zero-shot masks and labels from foundation\nmodels such as SAM and Grounded DINO. This allows the model to learn meaningful\ncorrelations between scene objects and their thermal signatures in infrared\nimagery. Extensive experiments on five public datasets demonstrate that F-ViTA\noutperforms state-of-the-art (SOTA) methods. Furthermore, our model generalizes\nwell to out-of-distribution (OOD) scenarios and can generate Long-Wave Infrared\n(LWIR), Mid-Wave Infrared (MWIR), and Near-Infrared (NIR) translations from the\nsame visible image. Code: https://github.com/JayParanjape/F-ViTA/tree/master."}, {"arxiv_id": "2504.03635", "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling\n  Law for Reasoning", "authors": ["Xinyi Wang", "Shawn Tan", "Mingyu Jin", "William Yang Wang", "Rameswar Panda", "Yikang Shen"], "published": "2025-04-04T17:57:22Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.03635v1", "categories": ["cs.AI", "cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."}, {"arxiv_id": "2504.03622", "title": "Align to Structure: Aligning Large Language Models with Structural\n  Information", "authors": ["Zae Myung Kim", "Anand Ramachandran", "Farideh Tavazoee", "Joo-Kyung Kim", "Oleg Rokhlenko", "Dongyeop Kang"], "published": "2025-04-04T17:40:04Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.03622v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align."}, {"arxiv_id": "2504.03621", "title": "VISTA-OCR: Towards generative and interactive end to end OCR models", "authors": ["Laziz Hamdi", "Amine Tamasna", "Pascal Boisson", "Thierry Paquet"], "published": "2025-04-04T17:39:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.03621v1", "categories": ["cs.CV"], "abstract": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance."}, {"arxiv_id": "2504.03620", "title": "Quantum Search with In-Place Queries", "authors": ["Blake Holman", "Ronak Ramachandran", "Justin Yirka"], "published": "2025-04-04T17:37:42Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.03620v1", "categories": ["quant-ph", "cs.CC", "cs.DS"], "abstract": "Quantum query complexity is typically characterized in terms of XOR queries\n|x,y> to |x,y+f(x)> or phase queries, which ensure that even queries to\nnon-invertible functions are unitary. When querying a permutation, another\nnatural model is unitary: in-place queries |x> to |f(x)>.\n  Some problems are known to require exponentially fewer in-place queries than\nXOR queries, but no separation has been shown in the opposite direction. A\ncandidate for such a separation was the problem of inverting a permutation over\nN elements. This task, equivalent to unstructured search in the context of\npermutations, is solvable with $O(\\sqrt{N})$ XOR queries but was conjectured to\nrequire $\\Omega(N)$ in-place queries.\n  We refute this conjecture by designing a quantum algorithm for Permutation\nInversion using $O(\\sqrt{N})$ in-place queries. Our algorithm achieves the same\nspeedup as Grover's algorithm despite the inability to efficiently uncompute\nqueries or perform straightforward oracle-controlled reflections.\n  Nonetheless, we show that there are indeed problems which require fewer XOR\nqueries than in-place queries. We introduce a subspace-conversion problem\ncalled Function Erasure that requires 1 XOR query and $\\Theta(\\sqrt{N})$\nin-place queries. Then, we build on a recent extension of the quantum adversary\nmethod to characterize exact conditions for a decision problem to exhibit such\na separation, and we propose a candidate problem."}, {"arxiv_id": "2504.03616", "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task", "authors": ["Leonardo Ranaldi", "Barry Haddow", "Alexandra Birch"], "published": "2025-04-04T17:35:43Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.03616v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages."}, {"arxiv_id": "2504.05294", "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward\n  Hacking in Explanations", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "published": "2025-04-07T17:49:23Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.05294v1", "categories": ["cs.CL"], "abstract": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations."}, {"arxiv_id": "2504.05289", "title": "Direct Measurement of the Singlet Lifetime and Photoexcitation Behavior\n  of the Boron Vacancy Center in Hexagonal Boron Nitride", "authors": ["Richard A. Escalante", "Andrew J. Beling", "Niko Reed", "Justin Welter", "John Blanchard", "Daniel G. Ang", "Cecilia Campos", "Edwin Coronel", "Klaus Krambrock", "Alexander S. Leal", "Paras N. Prasad", "Ronald L. Walsworth"], "published": "2025-04-07T17:42:43Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.05289v1", "categories": ["quant-ph"], "abstract": "Optically active electronic spin defects in van der Waals (vdW) materials\nprovide a promising platform for quantum sensing, as they can enable shorter\nstandoff distances compared to defects in diamond, leading to sensitivity\nadvantages. Perhaps the most studied defect in a vdW material is the negatively\ncharged boron vacancy center ($V^{-}_{B}$) in hexagonal boron nitride (hBN).\nHowever, many of the $V^{-}_{B}$ electronic and spin transition rates and\nbranching ratios are not fully known. Here, we use time-resolved\nphotoluminescence (PL) measurements with a nanosecond rise-time 515 nm laser to\ndetermine directly the singlet state lifetime of a $V^{-}_{B}$ ensemble in\nneutron-irradiated, sub-micron-size flakes of hBN. We perform this measurement\non 16 different hBN flakes at room temperature and obtain an average lifetime\nof 15(3) ns. Additionally, we probe the PL dynamics of thermal and optically\npolarized electronic spin distributions of the $V^{-}_{B}$ ensemble in a single\nsub-micron hBN flake, and fit our results to a 9-level model to extract the\nelectronic transition rates. Lastly, we present PL measurements that\npotentially indicate optically-induced conversion of $V^{-}_{B}$ to another\nelectronic state, or possibly the neutral charge state (V$^{0}_{B}$), in\nneutron-irradiated hBN flakes of size $>$ 1 $\\mu$m."}, {"arxiv_id": "2504.05287", "title": "RobustDexGrasp: Robust Dexterous Grasping of General Objects from\n  Single-view Perception", "authors": ["Hui Zhang", "Zijian Wu", "Linyi Huang", "Sammy Christen", "Jie Song"], "published": "2025-04-07T17:38:19Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.05287v1", "categories": ["cs.RO"], "abstract": "Robust grasping of various objects from single-view perception is fundamental\nfor dexterous robots. Previous works often rely on fully observable objects,\nexpert demonstrations, or static grasping poses, which restrict their\ngeneralization ability and adaptability to external disturbances. In this\npaper, we present a reinforcement-learning-based framework that enables\nzero-shot dynamic dexterous grasping of a wide range of unseen objects from\nsingle-view perception, while performing adaptive motions to external\ndisturbances. We utilize a hand-centric object representation for shape feature\nextraction that emphasizes interaction-relevant local shapes, enhancing\nrobustness to shape variance and uncertainty. To enable effective hand\nadaptation to disturbances with limited observations, we propose a mixed\ncurriculum learning strategy, which first utilizes imitation learning to\ndistill a policy trained with privileged real-time visual-tactile feedback, and\ngradually transfers to reinforcement learning to learn adaptive motions under\ndisturbances caused by observation noises and dynamic randomization. Our\nexperiments demonstrate strong generalization in grasping unseen objects with\nrandom poses, achieving success rates of 97.0% across 247,786 simulated objects\nand 94.6% across 512 real objects. We also demonstrate the robustness of our\nmethod to various disturbances, including unobserved object movement and\nexternal forces, through both quantitative and qualitative evaluations. Project\nPage: https://zdchan.github.io/Robust_DexGrasp/"}, {"arxiv_id": "2504.05278", "title": "The challenge of uncertainty quantification of large language models in\n  medicine", "authors": ["Zahra Atf", "Seyed Amir Ahmad Safavi-Naini", "Peter R. Lewis", "Aref Mahjoubfar", "Nariman Naderi", "Thomas R. Savage", "Ali Soroush"], "published": "2025-04-07T17:24:11Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.05278v1", "categories": ["cs.AI"], "abstract": "This study investigates uncertainty quantification in large language models\n(LLMs) for medical applications, emphasizing both technical innovations and\nphilosophical implications. As LLMs become integral to clinical\ndecision-making, accurately communicating uncertainty is crucial for ensuring\nreliable, safe, and ethical AI-assisted healthcare. Our research frames\nuncertainty not as a barrier but as an essential part of knowledge that invites\na dynamic and reflective approach to AI design. By integrating advanced\nprobabilistic methods such as Bayesian inference, deep ensembles, and Monte\nCarlo dropout with linguistic analysis that computes predictive and semantic\nentropy, we propose a comprehensive framework that manages both epistemic and\naleatoric uncertainties. The framework incorporates surrogate modeling to\naddress limitations of proprietary APIs, multi-source data integration for\nbetter context, and dynamic calibration via continual and meta-learning.\nExplainability is embedded through uncertainty maps and confidence metrics to\nsupport user trust and clinical interpretability. Our approach supports\ntransparent and ethical decision-making aligned with Responsible and Reflective\nAI principles. Philosophically, we advocate accepting controlled ambiguity\ninstead of striving for absolute predictability, recognizing the inherent\nprovisionality of medical knowledge."}, {"arxiv_id": "2504.05276", "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented\n  Generation", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "published": "2025-04-07T17:17:41Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.05276v1", "categories": ["cs.CL"], "abstract": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains."}, {"arxiv_id": "2504.06266", "title": "Constraining the [CII] luminosity function from the power spectrum of\n  line intensity maps at redshift 3.6", "authors": ["Elena Marcuzzo", "Cristiano Porciani", "Emilio Romano-Díaz", "Prachi Khatri"], "published": "2025-04-08T17:59:59Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.06266v1", "categories": ["astro-ph.CO", "astro-ph.GA"], "abstract": "Forthcoming measurements of the line-intensity-mapping power spectrum (PS)\nare expected to set precious constraints on several quantities of astrophysical\nand cosmological interest. Our study targets the [CII] luminosity function (LF)\nat high redshift, which is still highly uncertain, in particular at the faint\nend. As an example of future opportunities, we present forecasts for the Deep\nSpectroscopic Survey (DSS) that will be conducted with the Fred Young\nSubmillimeter Telescope at $z \\simeq 3.6$ and also make predictions for\neventual $10\\times$ wider and/or $\\sqrt{10}\\times$ more sensitive surveys. The\nhalo-occupation properties of [CII] emitters in the MARIGOLD simulations\nprovide us with the motivation to abundance match two versions of the ALPINE LF\nagainst the halo mass function. We employ the resulting luminosity-mass\nrelation within the halo model to predict the expected PS signal and its\nuncertainty. Finally, we use Bayesian inference to analyse mock PS data and\nforecast what constraints could be achieved on the first two moments of the LF\nand on Schechter fits. Depending on the actual LF, the DSS will measure the\nclustering and shot-noise amplitudes of the PS with a signal-to-noise ratio of\n$\\sim 3$ or higher. However, degeneracies with the bias parameter and\nredshift-space distortions make it unfeasible to extract the first moment of\nthe LF. Even the widest and most sensitive survey we consider can only\nconstrain it with a $50\\%$ uncertainty. By jointly fitting the PS and the LF,\nwe directly constrain Schechter-function parameters. We find that the\nnormalisation and the cutoff luminosity are precisely and accurately measured\nwhile the faint-end slope remains highly uncertain (unless the true value\napproaches $-2$). Overall, increasing the survey sensitivity at fixed sky\ncoverage yields greater improvements than covering a larger area at fixed\nsensitivity."}, {"arxiv_id": "2504.06265", "title": "GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning\n  through Bayesian Optimization", "authors": ["Bojana Ranković", "Philippe Schwaller"], "published": "2025-04-08T17:59:57Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.06265v1", "categories": ["cs.LG", "cs.AI"], "abstract": "Large Language Models (LLMs) can encode complex relationships in their latent\nspaces, yet harnessing them for optimization under uncertainty remains\nchallenging. We address this gap with a novel architecture that reframes LLM\nfinetuning as Gaussian process (GP) marginal likelihood optimization via deep\nkernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs\nto preserve the benefits of both - LLMs to provide a rich and flexible input\nspace for Bayesian optimization and - GPs to model this space with predictive\nuncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction\noptimization, our method nearly doubles the discovery rate of high-performing\nreactions compared to static LLM embeddings (from 24% to 43% coverage of the\ntop 5% reactions in just 50 optimization iterations). We also observe a 14%\nimprovement over domain-specific representations without requiring specialized\nfeatures. Extensive empirical evaluation across 19 benchmarks - ranging from\ngeneral chemistry to reaction and molecular property optimization -\ndemonstrates our method's robustness, generality, and consistent improvements\nacross: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),\n(3) pretraining domains (chemistry-related or general-purpose) and (4)\nhyperparameter settings (tuned once on a single dataset). Finally, we explain\nthese improvements: joint LLM-GP optimization through marginal likelihood\nimplicitly performs contrastive learning, aligning representations to produce\n(1) better-structured embedding spaces, (2) improved uncertainty calibration,\nand (3) more efficient sampling - without requiring any external loss. This\nwork provides both practical advances in sample-efficient optimization and\ninsights into what makes effective Bayesian optimization."}, {"arxiv_id": "2504.06261", "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention", "authors": ["Gleb Rodionov", "Roman Garipov", "Alina Shutova", "George Yakushev", "Vage Egiazarian", "Anton Sinitsin", "Denis Kuznedelev", "Dan Alistarh"], "published": "2025-04-08T17:59:41Z", "citations": 0, "tweets": 29, "paper_link": "http://arxiv.org/abs/2504.06261v1", "categories": ["cs.LG", "cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."}, {"arxiv_id": "2504.06260", "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "published": "2025-04-08T17:59:39Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.06260v1", "categories": ["cs.AI", "cs.CL", "cs.NA", "math.NA"], "abstract": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench"}, {"arxiv_id": "2504.06257", "title": "PainNet: Statistical Relation Network with Episode-Based Training for\n  Pain Estimation", "authors": ["Mina Bishay", "Graham Page", "Mohammad Mavadati"], "published": "2025-04-08T17:58:52Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.06257v1", "categories": ["cs.CV"], "abstract": "Despite the span in estimating pain from facial expressions, limited works\nhave focused on estimating the sequence-level pain, which is reported by\npatients and used commonly in clinics. In this paper, we introduce a novel\nStatistical Relation Network, referred to as PainNet, designed for the\nestimation of the sequence-level pain. PainNet employs two key modules, the\nembedding and the relation modules, for comparing pairs of pain videos, and\nproducing relation scores indicating if each pair belongs to the same pain\ncategory or not. At the core of the embedding module is a statistical layer\nmounted on the top of a RNN for extracting compact video-level features. The\nstatistical layer is implemented as part of the deep architecture. Doing so,\nallows combining multiple training stages used in previous research, into a\nsingle end-to-end training stage. PainNet is trained using the episode-based\ntraining scheme, which involves comparing a query video with a set of videos\nrepresenting the different pain categories. Experimental results show the\nbenefit of using the statistical layer and the episode-based training in the\nproposed model. Furthermore, PainNet outperforms the state-of-the-art results\non self-reported pain estimation."}, {"arxiv_id": "2504.07097", "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\n  Learning", "authors": ["Nikhil Shivakumar Nayak", "Krishnateja Killamsetty", "Ligong Han", "Abhishek Bhandwaldar", "Prateek Chanda", "Kai Xu", "Hao Wang", "Aldo Pareja", "Oleg Silkin", "Mustafa Eyceoz", "Akash Srivastava"], "published": "2025-04-09T17:59:42Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.07097v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.PR", "stat.ML", "68T50", "I.2.0; G.3"], "abstract": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models."}, {"arxiv_id": "2504.07089", "title": "OmniCaptioner: One Captioner to Rule Them All", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "published": "2025-04-09T17:58:58Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2504.07089v1", "categories": ["cs.CV", "cs.CL"], "abstract": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."}, {"arxiv_id": "2504.07087", "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on\n  Textualized Knowledge Graphs", "authors": ["Elan Markowitz", "Krupa Galiya", "Greg Ver Steeg", "Aram Galstyan"], "published": "2025-04-09T17:58:47Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2504.07087v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abstract": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks."}, {"arxiv_id": "2504.07080", "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning", "authors": ["Atharva Pandey", "Kshitij Dubey", "Rahul Sharma", "Amit Sharma"], "published": "2025-04-09T17:53:55Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.07080v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains."}, {"arxiv_id": "2504.07070", "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large\n  Language Models", "authors": ["Zhouhang Xie", "Junda Wu", "Yiran Shen", "Yu Xia", "Xintong Li", "Aaron Chang", "Ryan Rossi", "Sachin Kumar", "Bodhisattwa Prasad Majumder", "Jingbo Shang", "Prithviraj Ammanabrolu", "Julian McAuley"], "published": "2025-04-09T17:39:58Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.07070v1", "categories": ["cs.CL"], "abstract": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field."}, {"arxiv_id": "2504.07964", "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing", "authors": ["Zhongyang Li", "Ziyue Li", "Tianyi Zhou"], "published": "2025-04-10T17:59:56Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.07964v1", "categories": ["cs.LG"], "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely\nsub-optimal expert pathways-our study reveals that naive expert selection\nlearned from pretraining leaves a surprising 10-20% accuracy gap for\nimprovement. Motivated by this observation, we develop a novel class of\ntest-time optimization methods to re-weight or \"re-mixing\" the experts in\ndifferent layers jointly for each test sample. Since the test sample's ground\ntruth is unknown, we propose to optimize a surrogate objective defined by the\nsample's \"successful neighbors\" from a reference set of samples. We introduce\nthree surrogates and algorithms based on mode-finding, kernel regression, and\nthe average loss of similar reference samples/tasks. To reduce the cost of\noptimizing whole pathways, we apply our algorithms merely to the core experts'\nmixing weights in critical layers, which enjoy similar performance but save\nsignificant computation. This leads to \"Critical-Layer, Core-Expert,\nCollaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE\nLLMs and examine it on six widely-used benchmarks. It consistently improves the\nbase model by 7-15% in accuracy and outperforms widely used test-time learning\nbaselines, e.g., in-context learning and prompt/prefix tuning, by a large\nmargin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to\noutperform LLMs of 7-9B parameters, hence improving MoE's advantages on\nefficiency. Our thorough ablation study further sheds novel insights on\nachieving test-time improvement on MoE."}, {"arxiv_id": "2504.07956", "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning", "authors": ["Yukun Qi", "Yiming Zhao", "Yu Zeng", "Xikun Bao", "Wenxuan Huang", "Lin Chen", "Zehui Chen", "Jie Zhao", "Zhongang Qi", "Feng Zhao"], "published": "2025-04-10T17:59:03Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2504.07956v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly\nenhanced the capabilities of large language models (LLMs) and large\nvision-language models (LVLMs). However, a rigorous evaluation framework for\nvideo CoT reasoning remains absent. Current video benchmarks fail to adequately\nassess the reasoning process and expose whether failures stem from deficiencies\nin perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a\nnovel benchmark designed to comprehensively evaluate LVLMs' Video\nChain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos\nspanning a variety of video content and durations, along with 1,034\nhigh-quality question-answer pairs. Each pair is manually annotated with a\nstepwise CoT rationale, where every step is tagged to indicate its association\nwith the perception or reasoning capabilities. Furthermore, we design seven\ndistinct task dimensions and propose the CoT score to assess the entire CoT\nprocess based on the stepwise tagged CoT rationals. Extensive experiments on\nVCR-Bench highlight substantial limitations in current LVLMs. Even the\ntop-performing model, o1, only achieves a 62.8% CoT score and an 56.7%\naccuracy, while most models score below 40%. Experiments show most models score\nlower on perception than reasoning steps, revealing LVLMs' key bottleneck in\ntemporal-spatial information processing for complex video reasoning. A robust\npositive correlation between the CoT score and accuracy confirms the validity\nof our evaluation framework and underscores the critical role of CoT reasoning\nin solving complex video reasoning tasks. We hope VCR-Bench to serve as a\nstandardized evaluation framework and expose the actual drawbacks in complex\nvideo reasoning task."}, {"arxiv_id": "2504.07951", "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models", "authors": ["Mustafa Shukor", "Enrico Fini", "Victor Guilherme Turrisi da Costa", "Matthieu Cord", "Joshua Susskind", "Alaaeldin El-Nouby"], "published": "2025-04-10T17:57:28Z", "citations": 0, "tweets": 9, "paper_link": "http://arxiv.org/abs/2504.07951v1", "categories": ["cs.CV"], "abstract": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."}, {"arxiv_id": "2504.07938", "title": "Development of a Quantum-Resistant File Transfer System with Blockchain\n  Audit Trail", "authors": ["Ernesto Sola-Thomas", "Masudul H Imtiaz"], "published": "2025-04-10T17:51:14Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.07938v1", "categories": ["cs.CR"], "abstract": "This paper presents a condensed system architecture for a file transfer\nsolution that leverages post quantum cryptography and blockchain to secure data\nagainst quantum threats. The architecture integrates NIST standardized\nalgorithms CRYSTALS Kyber for encryption and CRYSTALS Dilithium for digital\nsignatures with an immutable blockchain ledger to provide an auditable,\ndecentralized storage mechanism. The system is modular, comprising a Sender\nmodule for secure encryption and signing, a central User Storage module for\ndecryption, reencryption, and blockchain logging, and a Requestor module for\nauthenticated data access. We include detailed pseudocode, analyze security\nrisks, and offer performance insights to demonstrate the system's robustness,\nscalability, and transparency."}, {"arxiv_id": "2504.07936", "title": "We Are All Creators: Generative AI, Collective Knowledge, and the Path\n  Towards Human-AI Synergy", "authors": ["Jordi Linares-Pellicer", "Juan Izquierdo-Domenech", "Isabel Ferri-Molla", "Carlos Aliaga-Torro"], "published": "2025-04-10T17:50:17Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2504.07936v1", "categories": ["cs.AI"], "abstract": "Generative AI presents a profound challenge to traditional notions of human\nuniqueness, particularly in creativity. Fueled by neural network based\nfoundation models, these systems demonstrate remarkable content generation\ncapabilities, sparking intense debates about authorship, copyright, and\nintelligence itself. This paper argues that generative AI represents an\nalternative form of intelligence and creativity, operating through mathematical\npattern synthesis rather than biological understanding or verbatim replication.\nThe fundamental differences between artificial and biological neural networks\nreveal AI learning as primarily statistical pattern extraction from vast\ndatasets crystallized forms of collective human knowledge scraped from the\ninternet. This perspective complicates copyright theft narratives and\nhighlights practical challenges in attributing AI outputs to individual\nsources. Rather than pursuing potentially futile legal restrictions, we\nadvocate for human AI synergy. By embracing generative AI as a complementary\ntool alongside human intuition, context, and ethical judgment, society can\nunlock unprecedented innovation, democratize creative expression, and address\ncomplex challenges. This collaborative approach, grounded in realistic\nunderstanding of AIs capabilities and limitations, offers the most promising\npath forward. Additionally, recognizing these models as products of collective\nhuman knowledge raises ethical questions about accessibility ensuring equitable\naccess to these tools could prevent widening societal divides and leverage\ntheir full potential for collective benefit."}, {"arxiv_id": "2504.08729", "title": "Steering CLIP's vision transformer with sparse autoencoders", "authors": ["Sonia Joseph", "Praneet Suresh", "Ethan Goldfarb", "Lorenz Hufe", "Yossi Gandelsman", "Robert Graham", "Danilo Bzdok", "Wojciech Samek", "Blake Aaron Richards"], "published": "2025-04-11T17:56:09Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2504.08729v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks."}, {"arxiv_id": "2504.08728", "title": "End-to-End Demonstration of Quantum Generative Adversarial Networks for\n  Steel Microstructure Image Augmentation on a Trapped-Ion Quantum Computer", "authors": ["Samwel Sekwao", "Jason Iaconis", "Claudio Girotto", "Martin Roetteler", "Minwoo Kang", "Donghwi Kim", "Seunghyo Noh", "Woomin Kyoung", "Kyujin Shin"], "published": "2025-04-11T17:55:58Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.08728v1", "categories": ["quant-ph"], "abstract": "Generative adversarial networks (GANs) are a machine learning technique\ncapable of producing high-quality synthetic images. In the field of materials\nscience, when a crystallographic dataset includes inadequate or\ndifficult-to-obtain images, synthetic images can be used for image augmentation\nto mitigate data scarcity and streamline the preparation of datasets for\nhigh-throughput analysis. We integrate quantum computing with GANs into a\nhybrid quantum-classical GAN to generate complex 5-channel electron backscatter\ndiffraction (EBSD) images of two distinct microstructure phases of steel. By\ntraining a quantum circuit at the input layer of a large classical Wasserstein\nGAN (WGAN) model, we mitigate mode collapse and achieve higher image quality\ncompared to a baseline classical GAN. We generate images from both ferrite and\nbainite microstructure phases in an end-to-end workflow. With respect to\nmaximum mean discrepancy score, we find that the hybrid quantum-classical WGAN\nimproves over classical Bernoulli GANs in 70% of samples. As the quantum\ncomputer is part of the training procedure, our method has potential to scale\nto larger number of qubits. Our results indicate that the WGAN model based on\nthe quantum circuit ansatz may be effectively leveraged to enhance the quality\nof synthetic EBSD images on both quantum simulators and actual quantum\nhardware."}, {"arxiv_id": "2504.08727", "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images", "authors": ["Boyang Deng", "Songyou Peng", "Kyle Genova", "Gordon Wetzstein", "Noah Snavely", "Leonidas Guibas", "Thomas Funkhouser"], "published": "2025-04-11T17:55:45Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.08727v1", "categories": ["cs.CV", "cs.AI", "cs.CY"], "abstract": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."}, {"arxiv_id": "2504.08725", "title": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation", "authors": ["Dayu Yang", "Antoine Simoulin", "Xin Qian", "Xiaoyi Liu", "Yuwei Cao", "Zhaopu Teng", "Grey Yang"], "published": "2025-04-11T17:50:08Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.08725v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "abstract": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."}, {"arxiv_id": "2504.08714", "title": "Generating Fine Details of Entity Interactions", "authors": ["Xinyi Gu", "Jiayuan Mao"], "published": "2025-04-11T17:24:58Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.08714v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation."}, {"arxiv_id": "2504.10484", "title": "Generalized Symmetries of Non-SUSY and Discrete Torsion String\n  Backgrounds", "authors": ["Noah Braeger", "Vivek Chakrabhavi", "Jonathan J. Heckman", "Max Hübner"], "published": "2025-04-14T17:59:55Z", "citations": 0, "tweets": 7, "paper_link": "http://arxiv.org/abs/2504.10484v1", "categories": ["hep-th", "math.AT", "math.RT"], "abstract": "String / M-theory backgrounds with degrees of freedom at a localized\nsingularity provide a general template for generating strongly correlated\nsystems decoupled from lower-dimensional gravity. There are by now several\ncomplementary procedures for extracting the associated generalized symmetry\ndata from orbifolds of the form $\\mathbb{R}^6 / \\Gamma$, including methods\nbased on the boundary topology of the asymptotic geometry, as well as the\nadjacency matrix for fermionic degrees of freedom in the quiver gauge theory of\nprobe branes. In this paper we show that this match between the two methods\nalso works in non-supersymmetric and discrete torsion backgrounds. In\nparticular, a refinement of geometric boundary data based on Chen-Ruan\ncohomology matches the expected answer based on quiver data. Additionally, we\nalso show that free (i.e., non-torsion) factors count the number of\nhigher-dimensional branes which couple to the localized singularity. We use\nthis to also extract quadratic pairing terms in the associated symmetry theory\n(SymTh) for these systems, and explain how these considerations generalize to a\nbroader class of backgrounds."}, {"arxiv_id": "2504.10481", "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations", "authors": ["Ding Chen", "Qingchen Yu", "Pengyuan Wang", "Wentao Zhang", "Bo Tang", "Feiyu Xiong", "Xinchi Li", "Minchuan Yang", "Zhiyu Li"], "published": "2025-04-14T17:59:36Z", "citations": 0, "tweets": 7, "paper_link": "http://arxiv.org/abs/2504.10481v1", "categories": ["cs.CL"], "abstract": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify."}, {"arxiv_id": "2504.10479", "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "published": "2025-04-14T17:59:25Z", "citations": 0, "tweets": 13, "paper_link": "http://arxiv.org/abs/2504.10479v1", "categories": ["cs.CV"], "abstract": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."}, {"arxiv_id": "2504.10473", "title": "Rotatable Antenna-Enabled Secure Wireless Communication", "authors": ["Liang Dai", "Beixiong Zheng", "Qingjie Wu"], "published": "2025-04-14T17:55:27Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.10473v1", "categories": ["eess.SP"], "abstract": "Rotatable antenna (RA) is a promising technology that exploits new spatial\ndegrees of freedom (DoFs) to improve wireless communication and sensing\nperformance. In this letter, we investigate an RA-enabled secure communication\nsystem where confidential information is transmitted from an RA-based access\npoint (AP) to a single-antenna legitimate user in the presence of multiple\neavesdroppers. We aim to maximize the achievable secrecy rate by jointly\noptimizing the transmit beamforming and the deflection angles of all RAs.\nAccordingly, we propose an efficient alternating optimization (AO) algorithm to\nobtain a high-quality suboptimal solution in an iterative manner, where the\ngeneralized Rayleigh quotient-based beamforming is applied and the RAs'\ndeflection angles are optimized by the successive convex approximation (SCA).\nSimulation results show that the proposed RA-enabled secure communication\nsystem achieves significant improvement in achievable secrecy rate as compared\nto various benchmark schemes."}, {"arxiv_id": "2504.10469", "title": "Bounds as blueprints: towards optimal and accelerated photonic inverse\n  design", "authors": ["Pengning Chao", "Alessio Amaolo", "Sean Molesky", "Alejandro W. Rodriguez"], "published": "2025-04-14T17:53:34Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.10469v1", "categories": ["physics.optics", "math.OC"], "abstract": "Our ability to structure materials at the nanoscale has, and continues to,\nenable key advances in optical control. In pursuit of optimal photonic designs,\nsubstantial progress has been made on two complementary fronts: bottom-up\nstructural optimizations (inverse design) discover complex high-performing\nstructures but offer no guarantees of optimality; top-down field optimizations\n(convex relaxations) reveal fundamental performance limits but offer no\nguarantees that structures meeting the limits exist. We bridge the gap between\nthese two parallel paradigms by introducing a ``verlan'' initialization method\nthat exploits the encoded local and global wave information in duality-based\nconvex relaxations to guide inverse design towards better-performing\nstructures. We illustrate this technique via the challenging problem of Purcell\nenhancement, maximizing the power extracted from a small emitter in the\nvicinity of a photonic structure, where ill-conditioning and the presence of\ncompeting local maxima lead to sub-optimal designs for adjoint optimization.\nStructures discovered by our verlan method outperform standard (random)\ninitializations by close to an order of magnitude and approach fundamental\nperformance limits within a factor of two, highlighting the possibility of\naccessing significant untapped performance improvements."}, {"arxiv_id": "2504.11456", "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and\n  Verifiable Mathematical Dataset for Advancing Reasoning", "authors": ["Zhiwei He", "Tian Liang", "Jiahao Xu", "Qiuzhi Liu", "Xingyu Chen", "Yue Wang", "Linfeng Song", "Dian Yu", "Zhenwen Liang", "Wenxuan Wang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published": "2025-04-15T17:59:51Z", "citations": 0, "tweets": 9, "paper_link": "http://arxiv.org/abs/2504.11456v1", "categories": ["cs.CL", "cs.AI"], "abstract": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath."}, {"arxiv_id": "2504.11446", "title": "eXplainable AI for data driven control: an inverse optimal control\n  approach", "authors": ["Federico Porcari", "Donatello Materassi", "Simone Formentin"], "published": "2025-04-15T17:56:24Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.11446v1", "categories": ["eess.SY", "cs.SY"], "abstract": "Understanding the behavior of black-box data-driven controllers is a key\nchallenge in modern control design. In this work, we propose an eXplainable AI\n(XAI) methodology based on Inverse Optimal Control (IOC) to obtain local\nexplanations for the behavior of a controller operating around a given region.\nSpecifically, we extract the weights assigned to tracking errors and control\neffort in the implicit cost function that a black-box controller is optimizing,\noffering a more transparent and interpretable representation of the\ncontroller's underlying objectives. This approach presents connections with\nwell-established XAI techniques, such as Local Interpretable Model-agnostic\nExplanations (LIME) since it is still based on a local approximation of the\ncontrol policy. However, rather being limited to a standard sensitivity\nanalysis, the explanation provided by our method relies on the solution of an\ninverse Linear Quadratic (LQ) problem, offering a structured and more\ncontrol-relevant perspective. Numerical examples demonstrate that the inferred\ncost function consistently provides a deeper understanding of the controller's\ndecision-making process, shedding light on otherwise counterintuitive or\nunexpected phenomena."}, {"arxiv_id": "2504.11442", "title": "TextArena", "authors": ["Leon Guertler", "Bobby Cheng", "Simon Yu", "Bo Liu", "Leshem Choshen", "Cheston Tan"], "published": "2025-04-15T17:55:20Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2504.11442v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "abstract": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/."}, {"arxiv_id": "2504.11440", "title": "Greedy Restart Schedules: A Baseline for Dynamic Algorithm Selection on\n  Numerical Black-box Optimization Problems", "authors": ["Lennart Schäpermeier"], "published": "2025-04-15T17:54:21Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.11440v1", "categories": ["math.OC", "cs.AI"], "abstract": "In many optimization domains, there are multiple different solvers that\ncontribute to the overall state-of-the-art, each performing better on some, and\nworse on other types of problem instances. Meta-algorithmic approaches, such as\ninstance-based algorithm selection, configuration and scheduling, aim to close\nthis gap by extracting the most performance possible from a set of\n(configurable) optimizers. In this context, the best performing individual\nalgorithms are often hand-crafted hybrid heuristics which perform many restarts\nof fast local optimization approaches. However, data-driven techniques to\ncreate optimized restart schedules have not yet been extensively studied.\n  Here, we present a simple scheduling approach that iteratively selects the\nalgorithm performing best on the distribution of unsolved training problems at\ntime of selection, resulting in a problem-independent solver schedule. We\ndemonstrate our approach using well-known optimizers from numerical black-box\noptimization on the BBOB testbed, bridging much of the gap between single and\nvirtual best solver from the original portfolio across various evaluation\nprotocols. Our greedy restart schedule presents a powerful baseline for more\ncomplex dynamic algorithm selection models."}, {"arxiv_id": "2504.11431", "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language\n  Models", "authors": ["Maria Teleki", "Xiangjue Dong", "Haoran Liu", "James Caverlee"], "published": "2025-04-15T17:41:54Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.11431v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "abstract": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault."}, {"arxiv_id": "2504.12285", "title": "BitNet b1.58 2B4T Technical Report", "authors": ["Shuming Ma", "Hongyu Wang", "Shaohan Huang", "Xingxing Zhang", "Ying Hu", "Ting Song", "Yan Xia", "Furu Wei"], "published": "2025-04-16T17:51:43Z", "citations": 0, "tweets": 12, "paper_link": "http://arxiv.org/abs/2504.12285v1", "categories": ["cs.CL", "cs.LG"], "abstract": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures."}, {"arxiv_id": "2504.12284", "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday\n  Interactions", "authors": ["Aditya Prakash", "Benjamin Lundell", "Dmitry Andreychuk", "David Forsyth", "Saurabh Gupta", "Harpreet Sawhney"], "published": "2025-04-16T17:48:12Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2504.12284v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings."}, {"arxiv_id": "2504.12278", "title": "Wormholes with Ends of the World", "authors": ["Diandian Wang", "Zhencheng Wang", "Zixia Wei"], "published": "2025-04-16T17:38:27Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.12278v1", "categories": ["hep-th", "gr-qc"], "abstract": "We study classical wormhole solutions in 3D gravity with end-of-the-world\n(EOW) branes, conical defects, kinks, and punctures. These solutions compute\nstatistical averages of an ensemble of boundary conformal field theories\n(BCFTs) related to universal asymptotics of OPE data extracted from 2D\nconformal bootstrap. Conical defects connect BCFT bulk operators; branes join\nBCFT boundary intervals with identical boundary conditions; kinks (1D defects\nalong branes) link BCFT boundary operators; and punctures (0D defects) are\nendpoints where conical defects terminate on branes. We provide evidence for a\ncorrespondence between the gravity theory and the ensemble. In particular, the\nagreement of $g$-function dependence results from an underlying topological\naspect of the on-shell EOW brane action, from which a BCFT analogue of the\nSchlenker-Witten theorem also follows."}, {"arxiv_id": "2504.12268", "title": "HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level\n  Synthesis Design Tasks", "authors": ["Stefan Abi-Karam", "Cong Hao"], "published": "2025-04-16T17:30:36Z", "citations": 0, "tweets": 41, "paper_link": "http://arxiv.org/abs/2504.12268v1", "categories": ["cs.AR", "cs.AI"], "abstract": "The rapid scaling of large language model (LLM) training and inference has\ndriven their adoption in semiconductor design across academia and industry.\nWhile most prior work evaluates LLMs on hardware description language (HDL)\ntasks, particularly Verilog, designers are increasingly using high-level\nsynthesis (HLS) to build domain-specific accelerators and complex hardware\nsystems. However, benchmarks and tooling to comprehensively evaluate LLMs for\nHLS design tasks remain scarce.\n  To address this, we introduce HLS-Eval, the first complete benchmark and\nevaluation framework for LLM-driven HLS design. HLS-Eval targets two core\ntasks: (1) generating HLS code from natural language descriptions, and (2)\nperforming HLS-specific code edits to optimize performance and hardware\nefficiency. The benchmark includes 94 unique designs drawn from standard HLS\nbenchmarks and novel sources. Each case is prepared via a semi-automated flow\nthat produces a natural language description and a paired testbench for\nC-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"\n  Beyond the benchmark, HLS-Eval offers a modular Python framework for\nautomated, parallel evaluation of both local and hosted LLMs. It includes a\nparallel evaluation engine, direct HLS tool integration, and abstractions for\nto support different LLM interaction paradigms, enabling rapid prototyping of\nnew benchmarks, tasks, and LLM methods.\n  We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on\nVitis HLS, measuring outputs across four key metrics - parseability,\ncompilability, runnability, and synthesizability - reflecting the iterative HLS\ndesign cycle. We also report pass@k metrics, establishing clear baselines and\nreusable infrastructure for the broader LLM-for-hardware community.\n  All benchmarks, framework code, and results are open-sourced at\nhttps://github.com/stefanpie/hls-eval."}, {"arxiv_id": "2504.12261", "title": "Dependency Dilemmas: A Comparative Study of Independent and Dependent\n  Artifacts in Maven Central Ecosystem", "authors": ["Mehedi Hasan Shanto", "Muhammad Asaduzzaman", "Manishankar Mondal", "Shaiful Chowdhury"], "published": "2025-04-16T17:15:58Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.12261v1", "categories": ["cs.SE"], "abstract": "The Maven Central ecosystem forms the backbone of Java dependency management,\nhosting artifacts that vary significantly in their adoption, security, and\necosystem roles. Artifact reuse is fundamental in software development, with\necosystems like Maven Central facilitating this process. However, prior studies\npredominantly analyzed popular artifacts with numerous dependencies, leaving\nthose without incoming dependencies (independent artifacts) unexplored. In this\nstudy, we analyzed 658,078 artifacts, of which 635,003 had at least one\nrelease. Among these, 93,101 artifacts (15.4%) were identified as independent\n(in-degree = 0), while the rest were classified as dependent. We looked at the\nimpact of separate artifacts using PageRank and out-degree centrality and\ndiscovered that they were very important to the ecosystem. Further analysis\nacross 18 different metrics revealed several advantages and comparability of\nindependent artifacts with dependent artifacts: comparable popularity (25.58\nvs. 7.30), fewer vulnerabilities (60 CVEs vs. 179 CVEs), and zero propagated\nvulnerabilities. Based on these results, it seems that independent artifacts\nmake a big difference in the ecosystem and give developers a safe,\nself-contained alternative to traditional dependencies. These findings suggest\nthat independent artifacts might be a beneficial choice for dependencies but\nhave some maintainability issues. Therefore, developers should carefully\nincorporate independent artifacts into their projects, and artifact maintainers\nshould prioritize this group of artifacts to mitigate the risk of transitive\nvulnerability propagation and improve software sustainability."}, {"arxiv_id": "2504.13178", "title": "Aligning Constraint Generation with Design Intent in Parametric CAD", "authors": ["Evan Casey", "Tianyu Zhang", "Shu Ishida", "John Roger Thompson", "Amir Khasahmadi", "Joseph George Lambourne", "Pradeep Kumar Jayaraman", "Karl D. D. Willis"], "published": "2025-04-17T17:59:54Z", "citations": 0, "tweets": 6, "paper_link": "http://arxiv.org/abs/2504.13178v1", "categories": ["cs.LG"], "abstract": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains."}, {"arxiv_id": "2504.13176", "title": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion\n  Design", "authors": ["Fei Shen", "Jian Yu", "Cong Wang", "Xin Jiang", "Xiaoyu Du", "Jinhui Tang"], "published": "2025-04-17T17:59:47Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.13176v1", "categories": ["cs.CV"], "abstract": "This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1."}, {"arxiv_id": "2504.13171", "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time", "authors": ["Kevin Lin", "Charlie Snell", "Yu Wang", "Charles Packer", "Sarah Wooders", "Ion Stoica", "Joseph E. Gonzalez"], "published": "2025-04-17T17:59:25Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2504.13171v1", "categories": ["cs.AI", "cs.CL"], "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task."}, {"arxiv_id": "2504.13145", "title": "Exploring Expert Failures Improves LLM Agent Tuning", "authors": ["Li-Cheng Lan", "Andrew Bai", "Minhao Cheng", "Ruochen Wang", "Cho-Jui Hsieh", "Tianyi Zhou"], "published": "2025-04-17T17:53:54Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2504.13145v1", "categories": ["cs.AI"], "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld."}, {"arxiv_id": "2504.13134", "title": "Energy-Based Reward Models for Robust Language Model Alignment", "authors": ["Anamika Lochab", "Ruqi Zhang"], "published": "2025-04-17T17:47:15Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.13134v1", "categories": ["cs.CL", "cs.LG"], "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM."}, {"arxiv_id": "2504.13837", "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in\n  LLMs Beyond the Base Model?", "authors": ["Yang Yue", "Zhiqi Chen", "Rui Lu", "Andrew Zhao", "Zhaokai Wang", "Yang Yue", "Shiji Song", "Gao Huang"], "published": "2025-04-18T17:59:56Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.13837v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io"}, {"arxiv_id": "2504.13834", "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "published": "2025-04-18T17:59:29Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2504.13834v1", "categories": ["cs.CL"], "abstract": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$"}, {"arxiv_id": "2504.13827", "title": "Extracting the chiral anomaly from $e^+e^-\\to 3π$", "authors": ["Martin Hoferichter", "Bai-Long Hoid", "Bastian Kubis"], "published": "2025-04-18T17:55:17Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.13827v1", "categories": ["hep-ph", "hep-ex", "hep-lat", "nucl-th"], "abstract": "The strength of the interaction of three pions and a photon, $F_{3\\pi}$ is\npredicted by the axial anomaly in terms of the pion decay constant, a relation\nthat is frequently used to constrain low-energy radiative processes involving\npions, but only tested experimentally at the $10\\%$ level. Here, we present a\nnew avenue to test this prediction, via a fit of a dispersive description of\nthe $\\gamma^*\\to3\\pi$ amplitude to data for $e^+e^-\\to 3\\pi$. From the global\nfit to SND, CMD-2, and BaBar data we obtain\n$F_{3\\pi}=33.1(1.7)\\,\\text{GeV}^{-3}$, in agreement with the chiral prediction\nat the level of $5\\%$. We also consider dispersive fits to the recent data by\nBelle II, in which case we observe tensions with the dispersive constraints,\nthe width parameters of $\\omega$ and $\\phi$, and the chiral anomaly."}, {"arxiv_id": "2504.13818", "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement\n  Learning", "authors": ["Yixuan Even Xu", "Yash Savani", "Fei Fang", "Zico Kolter"], "published": "2025-04-18T17:49:55Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.13818v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark."}, {"arxiv_id": "2504.13816", "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through\n  the Lens of Internal Representations", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Mahani Aljunied", "Lidong Bing", "Noura Al Moubayed", "Yu Rong"], "published": "2025-04-18T17:44:12Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.13816v1", "categories": ["cs.CL"], "abstract": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries."}, {"arxiv_id": "2504.15282", "title": "Multi-Target Rydberg Gates via Spatial Blockade Engineering", "authors": ["Samuel Stein", "Chenxu Liu", "Shuwen Kan", "Eleanor Crane", "Yufei Ding", "Ying Mao", "Alexander Shuckert", "Ang Li"], "published": "2025-04-21T17:59:56Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.15282v1", "categories": ["quant-ph"], "abstract": "Multi-target gates offer the potential to reduce gate depth in syndrome\nextraction for quantum error correction. Although neutral-atom quantum\ncomputers have demonstrated native multi-qubit gates, existing approaches that\navoid additional control or multiple atomic species have been limited to\nsingle-target gates. We propose single-control-multi-target CZ^{\\otimes N})\ngates on a single-species neutral-atom platform that require no extra control\nand have gate durations comparable to standard CZ gates. Our approach leverages\ntailored interatomic distances to create an asymmetric blockade between the\ncontrol and target atoms. Using a GPU-accelerated pulse synthesis protocol, we\ndesign smooth control pulses for CZZ and CZZZ gates, achieving fidelities of up\nto 99.55% and 99.24%, respectively, even in the presence of simulated atom\nplacement errors and Rydberg-state decay. This work presents a practical path\nto implementing multi-target gates in neutral-atom systems, significantly\nreducing the resource overhead for syndrome extraction."}, {"arxiv_id": "2504.15275", "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\n  Needs for Reasoning", "authors": ["Jie Cheng", "Ruixi Qiao", "Lijun Li", "Chao Guo", "Junle Wang", "Gang Xiong", "Yisheng Lv", "Fei-Yue Wang"], "published": "2025-04-21T17:59:02Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.15275v1", "categories": ["cs.AI", "cs.LG"], "abstract": "Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE."}, {"arxiv_id": "2504.15266", "title": "Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "published": "2025-04-21T17:47:46Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.15266v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity"}, {"arxiv_id": "2504.15265", "title": "Realization of maximally-entangling two-qutrit gates using the\n  Cross-Resonance scheme", "authors": ["Yash Saxena", "Sagnik Chatterjee", "Tharrmashastha Sapv"], "published": "2025-04-21T17:47:16Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.15265v1", "categories": ["quant-ph"], "abstract": "Three-level systems have natural advantages over their two-level counterparts\nin quantum information and computation. Although generally used for qubits, the\nexisting superconducting transmon architecture can naturally be extended to\nthree levels, which allows the exploration of the entire qutrit Hilbert space.\nRealizing a maximally entangling two-qutrit gate is necessary for universal\nquantum computation using three-level systems. Our work extends the\nstate-of-the-art approach by providing a theoretical framework for realizing a\nhigh-fidelity two-qutrit generalized CR gate using the cross-resonance\nframework. Using our framework, we experimentally demonstrate two-qutrit\ngeneralized controlled-$\\sqrt{X}$ gates through simulations on QISKIT dynamics,\nwhich in turn allow us to obtain a maximum extracted concurrence of\n$97.2\\pm0.1\\%$, thereby demonstrating that entanglement has been achieved on\nall three levels. Using the above gates, we also prepare a two-qutrit Bell\nstate with a fidelity of $96.9\\pm 0.1\\%$."}, {"arxiv_id": "2504.15263", "title": "Interpretable Locomotion Prediction in Construction Using a\n  Memory-Driven LLM Agent With Chain-of-Thought Reasoning", "authors": ["Ehsan Ahmadi", "Chao Wang"], "published": "2025-04-21T17:45:21Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.15263v1", "categories": ["cs.RO"], "abstract": "Construction tasks are inherently unpredictable, with dynamic environments\nand safety-critical demands posing significant risks to workers. Exoskeletons\noffer potential assistance but falter without accurate intent recognition\nacross diverse locomotion modes. This paper presents a locomotion prediction\nagent leveraging Large Language Models (LLMs) augmented with memory systems,\naimed at improving exoskeleton assistance in such settings. Using multimodal\ninputs - spoken commands and visual data from smart glasses - the agent\nintegrates a Perception Module, Short-Term Memory (STM), Long-Term Memory\n(LTM), and Refinement Module to predict locomotion modes effectively.\nEvaluation reveals a baseline weighted F1-score of 0.73 without memory, rising\nto 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague\nand safety-critical commands. Calibration metrics, including a Brier Score drop\nfrom 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.\nThis framework supports safer, high-level human-exoskeleton collaboration, with\npromise for adaptive assistive systems in dynamic industries."}, {"arxiv_id": "2504.16084", "title": "TTRL: Test-Time Reinforcement Learning", "authors": ["Yuxin Zuo", "Kaiyan Zhang", "Shang Qu", "Li Sheng", "Xuekai Zhu", "Biqing Qi", "Youbang Sun", "Ganqu Cui", "Ning Ding", "Bowen Zhou"], "published": "2025-04-22T17:59:56Z", "citations": 0, "tweets": 19, "paper_link": "http://arxiv.org/abs/2504.16084v1", "categories": ["cs.CL", "cs.LG"], "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the Maj@N metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model, and\napproach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks, and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"}, {"arxiv_id": "2504.16082", "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "published": "2025-04-22T17:59:41Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2504.16082v1", "categories": ["cs.CV"], "abstract": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video"}, {"arxiv_id": "2504.16081", "title": "Survey of Video Diffusion Models: Foundations, Implementations, and\n  Applications", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "published": "2025-04-22T17:59:17Z", "citations": 0, "tweets": 5, "paper_link": "http://arxiv.org/abs/2504.16081v1", "categories": ["cs.CV", "cs.CL"], "abstract": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion."}, {"arxiv_id": "2504.16078", "title": "LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making\n  Abilities", "authors": ["Thomas Schmied", "Jörg Bornschein", "Jordi Grau-Moya", "Markus Wulfmeier", "Razvan Pascanu"], "published": "2025-04-22T17:57:14Z", "citations": 0, "tweets": 9, "paper_link": "http://arxiv.org/abs/2504.16078v1", "categories": ["cs.LG", "cs.AI"], "abstract": "The success of Large Language Models (LLMs) has sparked interest in various\nagentic applications. A key hypothesis is that LLMs, leveraging common sense\nand Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently\nsolve complex domains. However, LLM agents have been found to suffer from\nsub-optimal exploration and the knowing-doing gap, the inability to effectively\nact on knowledge present in the model. In this work, we systematically study\nwhy LLMs perform sub-optimally in decision-making scenarios. In particular, we\nclosely examine three prevalent failure modes: greediness, frequency bias, and\nthe knowing-doing gap. We propose mitigation of these shortcomings by\nfine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.\nOur experiments across multi-armed bandits, contextual bandits, and\nTic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making\nabilities of LLMs by increasing exploration and narrowing the knowing-doing\ngap. Finally, we study both classic exploration mechanisms, such as\n$\\epsilon$-greedy, and LLM-specific approaches, such as self-correction and\nself-consistency, to enable more effective fine-tuning of LLMs for\ndecision-making."}, {"arxiv_id": "2504.16074", "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in\n  Large Language Models", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Muhan Zhang", "Hua Xing Zhu"], "published": "2025-04-22T17:53:29Z", "citations": 0, "tweets": 13, "paper_link": "http://arxiv.org/abs/2504.16074v1", "categories": ["cs.CL"], "abstract": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/."}, {"arxiv_id": "2504.16931", "title": "Lattice study of $cc\\bar u\\bar s$ tetraquark channel in\n  $D^{(*)}D^{(*)}_s$ scattering", "authors": ["Tanishk Shrimal", "Sara Collins", "Priyajit Jana", "M. Padmanath", "Sasa Prelovsek"], "published": "2025-04-23T17:59:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.16931v1", "categories": ["hep-lat", "hep-ex", "hep-ph"], "abstract": "We present the first lattice QCD determination of coupled $DD_s^*$ and\n$D^*D_s$ scattering amplitudes in the $J^{P}=1^{+}$ channel and elastic $DD_s$\nscattering amplitude in the $J^{P}=0^{+}$ channel.The aim is to investigate\nwhether tetraquarks with flavor $cc\\bar u\\bar s$ exist in the region near\nthreshold. Lattice QCD ensembles from the CLS consortium with $m_{\\pi} \\sim\n280$ MeV, $a\\sim0.09$ fm and $L/a = 24, 32$ are utilized. Finite-volume spectra\nare determined via variational analysis of two-point correlation matrices,\ncomputed using large bases of operators resembling bilocal two-meson structures\nwithin the distillation framework. The scattering matrix for partial wave $l=0$\nis determined using lattice eigenenergies from multiple inertial frames\nfollowing L\\\"uscher's formalism as well as following the solutions of\nLippmann-Schwinger Equation in the finite-volume on a plane-wave basis. We\nobserve small nonzero energy shifts in the simulated spectra from the\nnoninteracting scenario in both the channels studied, which points to rather\nweak nontrivial interactions between the mesons involved. Despite the nonzero\nenergy shifts, the lattice-extracted $S$-wave amplitudes do not carry\nsignatures of any hadron pole features in the physical amplitudes in the energy\nregion near the threshold."}, {"arxiv_id": "2504.16921", "title": "IberBench: LLM Evaluation on Iberian Languages", "authors": ["José Ángel González", "Ian Borrego Obrador", "Álvaro Romo Herrero", "Areg Mikael Sarvazyan", "Mara Chinea-Ríos", "Angelo Basile", "Marc Franco-Salvador"], "published": "2025-04-23T17:48:25Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.16921v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) remain difficult to evaluate comprehensively,\nparticularly for languages other than English, where high-quality data is often\nlimited. Existing benchmarks and leaderboards are predominantly\nEnglish-centric, with only a few addressing other languages. These benchmarks\nfall short in several key areas: they overlook the diversity of language\nvarieties, prioritize fundamental Natural Language Processing (NLP)\ncapabilities over tasks of industrial relevance, and are static. With these\naspects in mind, we present IberBench, a comprehensive and extensible benchmark\ndesigned to assess LLM performance on both fundamental and industry-relevant\nNLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.\nIberBench integrates 101 datasets from evaluation campaigns and recent\nbenchmarks, covering 22 task categories such as sentiment and emotion analysis,\ntoxicity detection, and summarization. The benchmark addresses key limitations\nin current evaluation practices, such as the lack of linguistic diversity and\nstatic evaluation setups by enabling continual updates and community-driven\nmodel and dataset submissions moderated by a committee of experts. We evaluate\n23 LLMs ranging from 100 million to 14 billion parameters and provide empirical\ninsights into their strengths and limitations. Our findings indicate that (i)\nLLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)\nperformance is on average lower for Galician and Basque, (iii) some tasks show\nresults close to random, and (iv) in other tasks LLMs perform above random but\nbelow shared task systems. IberBench offers open-source implementations for the\nentire evaluation pipeline, including dataset normalization and hosting,\nincremental evaluation of LLMs, and a publicly accessible leaderboard."}, {"arxiv_id": "2504.16918", "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents", "authors": ["Raghav Thind", "Youran Sun", "Ling Liang", "Haizhao Yang"], "published": "2025-04-23T17:45:05Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2504.16918v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Optimization plays a vital role in scientific research and practical\napplications, but formulating a concrete optimization problem described in\nnatural language into a mathematical form and selecting a suitable solver to\nsolve the problem requires substantial domain expertise. We introduce\n\\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems\ndescribed in natural language by leveraging LLM-powered \\underline{AI} agents,\nachieving superior performance over current state-of-the-art methods. Our\nframework is built upon four key roles: (1) a \\emph{formulator} that translates\nnatural language problem descriptions into precise mathematical formulations;\n(2) a \\emph{planner} that constructs a high-level solution strategy prior to\nexecution; and (3) a \\emph{coder} and a \\emph{code critic} capable of\ninteracting with the environment and reflecting on outcomes to refine future\nactions. Ablation studies confirm that all roles are essential; removing the\nplanner or code critic results in $5.8\\times$ and $3.1\\times$ drops in\nproductivity, respectively. Furthermore, we introduce UCB-based debug\nscheduling to dynamically switch between alternative plans, yielding an\nadditional $3.3\\times$ productivity gain. Our design emphasizes multi-agent\ncollaboration, allowing us to conveniently explore the synergistic effect of\ncombining diverse models within a unified system. Our approach attains 88.1\\%\naccuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o\ntable) subset, reducing error rates by 58\\% and 50\\% respectively over prior\nbest results."}, {"arxiv_id": "2504.16913", "title": "Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM\n  Behind AI-Generated Text", "authors": ["Shifali Agrahari", "Sanasam Ranbir Singh"], "published": "2025-04-23T17:39:49Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2504.16913v1", "categories": ["cs.CL", "cs.AI"], "abstract": "In recent years, the detection of AI-generated text has become a critical\narea of research due to concerns about academic integrity, misinformation, and\nethical AI deployment. This paper presents COT Fine-tuned, a novel framework\nfor detecting AI-generated text and identifying the specific language model.\nresponsible for generating the text. We propose a dual-task approach, where\nTask A involves classifying text as AI-generated or human-written, and Task B\nidentifies the specific LLM behind the text. The key innovation of our method\nlies in the use of Chain-of-Thought reasoning, which enables the model to\ngenerate explanations for its predictions, enhancing transparency and\ninterpretability. Our experiments demonstrate that COT Fine-tuned achieves high\naccuracy in both tasks, with strong performance in LLM identification and\nhuman-AI classification. We also show that the CoT reasoning process\ncontributes significantly to the models effectiveness and interpretability."}, {"arxiv_id": "2504.16907", "title": "BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation", "authors": ["Ruotong Wang", "Mingli Zhu", "Jiarong Ou", "Rui Chen", "Xin Tao", "Pengfei Wan", "Baoyuan Wu"], "published": "2025-04-23T17:34:48Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.16907v1", "categories": ["cs.CV", "cs.AI"], "abstract": "Text-to-video (T2V) generative models have rapidly advanced and found\nwidespread applications across fields like entertainment, education, and\nmarketing. However, the adversarial vulnerabilities of these models remain\nrarely explored. We observe that in T2V generation tasks, the generated videos\noften contain substantial redundant information not explicitly specified in the\ntext prompts, such as environmental elements, secondary objects, and additional\ndetails, providing opportunities for malicious attackers to embed hidden\nharmful content. Exploiting this inherent redundancy, we introduce BadVideo,\nthe first backdoor attack framework tailored for T2V generation. Our attack\nfocuses on designing target adversarial outputs through two key strategies: (1)\nSpatio-Temporal Composition, which combines different spatiotemporal features\nto encode malicious information; (2) Dynamic Element Transformation, which\nintroduces transformations in redundant elements over time to convey malicious\ninformation. Based on these strategies, the attacker's malicious target\nseamlessly integrates with the user's textual instructions, providing high\nstealthiness. Moreover, by exploiting the temporal dimension of videos, our\nattack successfully evades traditional content moderation systems that\nprimarily analyze spatial information within individual frames. Extensive\nexperiments demonstrate that BadVideo achieves high attack success rates while\npreserving original semantics and maintaining excellent performance on clean\ninputs. Overall, our work reveals the adversarial vulnerability of T2V models,\ncalling attention to potential risks and misuse. Our project page is at\nhttps://wrt2000.github.io/BadVideo2025/."}, {"arxiv_id": "2504.17785", "title": "Silenzio: Secure Non-Interactive Outsourced MLP Training", "authors": ["Jonas Sander", "Thomas Eisenbarth"], "published": "2025-04-24T17:59:22Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.17785v1", "categories": ["cs.CR"], "abstract": "Outsourcing the ML training to cloud providers presents a compelling\nopportunity for resource constrained clients, while it simultaneously bears\ninherent privacy risks, especially for highly sensitive training data. We\nintroduce Silenzio, the first fully non-interactive outsourcing scheme for the\ntraining of multi-layer perceptrons that achieves 128 bit security using FHE.\nUnlike traditional MPC based protocols that necessitate interactive\ncommunication between the client and server(s) or non-collusion assumptions\namong multiple servers, Silenzio enables the fire-and-forget paradigm without\nsuch assumptions. In this approach, the client encrypts the training data once,\nand the cloud server performs the training without any further interaction.\n  Silenzio operates over low bitwidth integers - never exceeding 8 bit - to\nmitigate the computational overhead of FHE. Our approach features a novel\nlow-bitwidth matrix multiplication that leverages input-dependent residue\nnumber systems and a Karatsuba-inspired multiplication routine, ensuring that\nno intermediate FHE-processed value overflows 8 bit. Starting from an\nRNS-to-MRNS conversion process, we propose an efficient block-scaling\nmechanism, which approximately shifts encrypted tensor values to the\nuser-specified most significant bits. To instantiate the backpropagation of the\nerror, Silenzio introduces a low-bitwidth and TFHE friendly gradient\ncomputation for the cross entropy loss.\n  Implemented using the state-of-the-art Concrete library, we evaluate Silenzio\non standard MLP training tasks regarding runtime as well as model performance\nand achieve similar classification accuracy as MLPs trained using standard\nPyTorch with 32 bit floating-point computations. Our open-source implementation\nrepresents a significant advancement in privacy-preserving ML, providing a new\nbaseline for secure and non-interactive outsourced MLP training."}, {"arxiv_id": "2504.17782", "title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "authors": ["Xize Cheng", "Slytherin Wang", "Zehan Wang", "Rongjie Huang", "Tao Jin", "Zhou Zhao"], "published": "2025-04-24T17:58:21Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.17782v1", "categories": ["cs.SD", "cs.LG"], "abstract": "Universal sound separation aims to extract clean audio tracks corresponding\nto distinct events from mixed audio, which is critical for artificial auditory\nperception. However, current methods heavily rely on artificially mixed audio\nfor training, which limits their ability to generalize to naturally mixed audio\ncollected in real-world environments. To overcome this limitation, we propose\nClearSep, an innovative framework that employs a data engine to decompose\ncomplex naturally mixed audio into multiple independent tracks, thereby\nallowing effective sound separation in real-world scenarios. We introduce two\nremix-based evaluation metrics to quantitatively assess separation quality and\nuse these metrics as thresholds to iteratively apply the data engine alongside\nmodel training, progressively optimizing separation performance. In addition,\nwe propose a series of training strategies tailored to these separated\nindependent tracks to make the best use of them. Extensive experiments\ndemonstrate that ClearSep achieves state-of-the-art performance across multiple\nsound separation tasks, highlighting its potential for advancing sound\nseparation in natural audio scenarios. For more examples and detailed results,\nplease visit our demo page at https://clearsep.github.io."}, {"arxiv_id": "2504.17780", "title": "Replay to Remember: Retaining Domain Knowledge in Streaming Language\n  Models", "authors": ["Sneh Pillai"], "published": "2025-04-24T17:56:22Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.17780v1", "categories": ["cs.LG"], "abstract": "Continual learning in large language models (LLMs) typically encounters the\ncritical challenge of catastrophic forgetting, where previously acquired\nknowledge deteriorates upon exposure to new data. While techniques like replay\nbuffers and parameter-efficient tuning (e.g., Low-Rank Adaptation or LoRA) have\nbeen proposed, few studies investigate real-time domain adaptation under strict\ncomputational and data-stream constraints. In this paper, we demonstrate a\nlightweight method combining LoRA and a minimal replay mechanism in a realistic\nstreaming setting across three diverse knowledge domains: medical question\nanswering, genetics, and law. Using perplexity, semantic similarity, and\nGPT-based human-like evaluation metrics, we quantify the model's adaptation,\nforgetting, and recovery over time. Our experiments reveal that while\ncatastrophic forgetting naturally occurs, even minimal replay significantly\nstabilizes and partially restores domain-specific knowledge. This study\ncontributes practical insights for deploying adaptable LLMs in\nresource-constrained, real-world scenarios."}, {"arxiv_id": "2504.17768", "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "published": "2025-04-24T17:39:25Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.17768v1", "categories": ["cs.CL", "cs.LG"], "abstract": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications."}, {"arxiv_id": "2504.17761", "title": "Step1X-Edit: A Practical Framework for General Image Editing", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "published": "2025-04-24T17:25:12Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.17761v1", "categories": ["cs.CV"], "abstract": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."}, {"arxiv_id": "2504.18535", "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to\n  Controllable Language Generation", "authors": ["Gwen Yidou Weng", "Benjie Wang", "Guy Van den Broeck"], "published": "2025-04-25T17:59:13Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.18535v1", "categories": ["cs.CL", "cs.LG"], "abstract": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes."}, {"arxiv_id": "2504.18529", "title": "Practical Type-Based Taint Checking and Inference (Extended Version)", "authors": ["Nima Karimipour", "Kanak Das", "Manu Sridharan", "Behnaz Hassanshahi"], "published": "2025-04-25T17:53:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.18529v1", "categories": ["cs.PL"], "abstract": "Many important security properties can be formulated in terms of flows of\ntainted data, and improved taint analysis tools to prevent such flows are of\ncritical need. Most existing taint analyses use whole-program static analysis,\nleading to scalability challenges. Type-based checking is a promising\nalternative, as it enables modular and incremental checking for fast\nperformance. However, type-based approaches have not been widely adopted in\npractice, due to challenges with false positives and annotating existing\ncodebases. In this paper, we present a new approach to type-based checking of\ntaint properties that addresses these challenges, based on two key techniques.\nFirst, we present a new type-based tainting checker with significantly reduced\nfalse positives, via more practical handling of third-party libraries and other\nlanguage constructs. Second, we present a novel technique to automatically\ninfer tainting type qualifiers for existing code. Our technique supports\ninference of generic type argument annotations, crucial for tainting\nproperties. We implemented our techniques in a tool TaintTyper and evaluated it\non real-world benchmarks. TaintTyper exceeds the recall of a state-of-the-art\nwhole-program taint analyzer, with comparable precision, and 2.93X-22.9X faster\nchecking time. Further, TaintTyper infers annotations comparable to those\nwritten by hand, suitable for insertion into source code. TaintTyper is a\npromising new approach to efficient and practical taint checking."}, {"arxiv_id": "2504.18524", "title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors", "authors": ["Fengjia Zhang", "Samrudhdhi B. Rangrej", "Tristan Aumentado-Armstrong", "Afsaneh Fazly", "Alex Levinshtein"], "published": "2025-04-25T17:47:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.18524v1", "categories": ["cs.CV"], "abstract": "Super-resolution (SR), a classical inverse problem in computer vision, is\ninherently ill-posed, inducing a distribution of plausible solutions for every\ninput. However, the desired result is not simply the expectation of this\ndistribution, which is the blurry image obtained by minimizing pixelwise error,\nbut rather the sample with the highest image quality. A variety of techniques,\nfrom perceptual metrics to adversarial losses, are employed to this end. In\nthis work, we explore an alternative: utilizing powerful non-reference image\nquality assessment (NR-IQA) models in the SR context. We begin with a\ncomprehensive analysis of NR-IQA metrics on human-derived SR data, identifying\nboth the accuracy (human alignment) and complementarity of different metrics.\nThen, we explore two methods of applying NR-IQA models to SR learning: (i)\naltering data sampling, by building on an existing multi-ground-truth SR\nframework, and (ii) directly optimizing a differentiable quality score. Our\nresults demonstrate a more human-centric perception-distortion tradeoff,\nfocusing less on non-perceptual pixel-wise distortion, instead improving the\nbalance between perceptual fidelity and human-tuned NR-IQA measures."}, {"arxiv_id": "2504.18519", "title": "Intelligent Attacks and Defense Methods in Federated Learning-enabled\n  Energy-Efficient Wireless Networks", "authors": ["Han Zhang", "Hao Zhou", "Medhat Elsayed", "Majid Bavand", "Raimundas Gaigalas", "Yigit Ozcan", "Melike Erol-Kantarci"], "published": "2025-04-25T17:40:35Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.18519v1", "categories": ["cs.LG"], "abstract": "Federated learning (FL) is a promising technique for learning-based functions\nin wireless networks, thanks to its distributed implementation capability. On\nthe other hand, distributed learning may increase the risk of exposure to\nmalicious attacks where attacks on a local model may spread to other models by\nparameter exchange. Meanwhile, such attacks can be hard to detect due to the\ndynamic wireless environment, especially considering local models can be\nheterogeneous with non-independent and identically distributed (non-IID) data.\nTherefore, it is critical to evaluate the effect of malicious attacks and\ndevelop advanced defense techniques for FL-enabled wireless networks. In this\nwork, we introduce a federated deep reinforcement learning-based cell sleep\ncontrol scenario that enhances the energy efficiency of the network. We propose\nmultiple intelligent attacks targeting the learning-based approach and we\npropose defense methods to mitigate such attacks. In particular, we have\ndesigned two attack models, generative adversarial network (GAN)-enhanced model\npoisoning attack and regularization-based model poisoning attack. As a\ncounteraction, we have proposed two defense schemes, autoencoder-based defense,\nand knowledge distillation (KD)-enabled defense. The autoencoder-based defense\nmethod leverages an autoencoder to identify the malicious participants and only\naggregate the parameters of benign local models during the global aggregation,\nwhile KD-based defense protects the model from attacks by controlling the\nknowledge transferred between the global model and local models."}, {"arxiv_id": "2504.18516", "title": "Highly Accurate Expectation Values Using High-Order Relativistic Coupled\n  Cluster Theory", "authors": ["Gabriele Fabbro", "Jan Brandejs", "Trond Saue"], "published": "2025-04-25T17:34:07Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.18516v1", "categories": ["physics.chem-ph"], "abstract": "This work presents the automatic generation of analytic first derivatives of\nthe energy for general coupled-cluster models using the \\text{tenpi} toolchain.\nWe report the first implementation of expectation values for CCSDT and CCSDTQ\nmethods within the DIRAC program package for relativistic molecular\ncalculations. As pivotal calculations, we focus on the electric field gradient\n(EFG) evaluated at the lithium nucleus in LiX (X = H, F, Cl) compounds,\nenabling the extraction of the nuclear electric quadrupole moment\n$Q({}^{7}Li)$, and at the aluminum nucleus in AlY (Y =H, F, Cl, Br) compounds,\nfor the determination of $Q({}^{27}Al)$. These high-order methods are applied\nto compute corrections for triple and quadruple excitations for the EFG, a\ncrucial quantity for determining nuclear quadrupole moments. We obtain\n$Q({}^{27}Al)$ = 0.1466 b, in excellent agreement with the recommended value,\nand $Q({}^{7}Li)$ = -0.0386 b, which is smaller than the currently recommended\nvalue, that indicates the need for further investigation."}, {"arxiv_id": "2504.20039", "title": "AutoJudge: Judge Decoding Without Manual Annotation", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "published": "2025-04-28T17:59:28Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20039v1", "categories": ["cs.CL", "cs.LG"], "abstract": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."}, {"arxiv_id": "2504.20022", "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual\n  LLMs in English and Low-Resource Languages", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "published": "2025-04-28T17:48:13Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20022v1", "categories": ["cs.CL", "cs.LG"], "abstract": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."}, {"arxiv_id": "2504.20020", "title": "Modular Machine Learning: An Indispensable Path towards New-Generation\n  Large Language Models", "authors": ["Xin Wang", "Haoyang Li", "Zeyang Zhang", "Haibo Chen", "Wenwu Zhu"], "published": "2025-04-28T17:42:02Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20020v1", "categories": ["cs.LG", "cs.AI"], "abstract": "Large language models (LLMs) have dramatically advanced machine learning\nresearch including natural language processing, computer vision, data mining,\netc., yet they still exhibit critical limitations in reasoning, factual\nconsistency, and interpretability. In this paper, we introduce a novel learning\nparadigm -- Modular Machine Learning (MML) -- as an essential approach toward\nnew-generation LLMs. MML decomposes the complex structure of LLMs into three\ninterdependent components: modular representation, modular model, and modular\nreasoning, aiming to enhance LLMs' capability of counterfactual reasoning,\nmitigating hallucinations, as well as promoting fairness, safety, and\ntransparency. Specifically, the proposed MML paradigm can: i) clarify the\ninternal working mechanism of LLMs through the disentanglement of semantic\ncomponents; ii) allow for flexible and task-adaptive model design; iii) enable\ninterpretable and logic-driven decision-making process. We present a feasible\nimplementation of MML-based LLMs via leveraging advanced techniques such as\ndisentangled representation learning, neural architecture search and\nneuro-symbolic learning. We critically identify key challenges, such as the\nintegration of continuous neural and discrete symbolic processes, joint\noptimization, and computational scalability, present promising future research\ndirections that deserve further exploration. Ultimately, the integration of the\nMML paradigm with LLMs has the potential to bridge the gap between statistical\n(deep) learning and formal (logical) reasoning, thereby paving the way for\nrobust, adaptable, and trustworthy AI systems across a wide range of real-world\napplications."}, {"arxiv_id": "2504.20016", "title": "Applying LLM-Powered Virtual Humans to Child Interviews in\n  Child-Centered Design", "authors": ["Linshi Li", "Hanlin Cai"], "published": "2025-04-28T17:35:46Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20016v1", "categories": ["cs.HC", "cs.CY", "cs.MM"], "abstract": "In child-centered design, directly engaging children is crucial for deeply\nunderstanding their experiences. However, current research often prioritizes\nadult perspectives, as interviewing children involves unique challenges such as\nenvironmental sensitivities and the need for trust-building. AI-powered virtual\nhumans (VHs) offer a promising approach to facilitate engaging and multimodal\ninteractions with children. This study establishes key design guidelines for\nLLM-powered virtual humans tailored to child interviews, standardizing\nmultimodal elements including color schemes, voice characteristics, facial\nfeatures, expressions, head movements, and gestures. Using ChatGPT-based prompt\nengineering, we developed three distinct Human-AI workflows (LLM-Auto,\nLLM-Interview, and LLM-Analyze) and conducted a user study involving 15\nchildren aged 6 to 12. The results indicated that the LLM-Analyze workflow\noutperformed the others by eliciting longer responses, achieving higher user\nexperience ratings, and promoting more effective child engagement."}, {"arxiv_id": "2504.20013", "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case\n  Study on Neural News Recommendation", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "published": "2025-04-28T17:32:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20013v1", "categories": ["cs.CL", "cs.CY", "cs.IR"], "abstract": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}, {"arxiv_id": "2504.20997", "title": "Toward Efficient Exploration by Large Language Model Agents", "authors": ["Dilip Arumugam", "Thomas L. Griffiths"], "published": "2025-04-29T17:59:48Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20997v1", "categories": ["cs.LG", "cs.AI"], "abstract": "A burgeoning area within reinforcement learning (RL) is the design of\nsequential decision-making agents centered around large language models (LLMs).\nWhile autonomous decision-making agents powered by modern LLMs could facilitate\nnumerous real-world applications, such successes demand agents that are capable\nof data-efficient RL. One key obstacle to achieving data efficiency in RL is\nexploration, a challenge that we demonstrate many recent proposals for LLM\nagent designs struggle to contend with. Meanwhile, classic algorithms from the\nRL literature known to gracefully address exploration require technical\nmachinery that can be challenging to operationalize in purely natural language\nsettings. In this work, rather than relying on finetuning or in-context\nlearning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\nhow LLMs can be used to explicitly implement an existing RL algorithm\n(Posterior Sampling for Reinforcement Learning) whose capacity for\nstatistically-efficient exploration is already well-studied. We offer empirical\nresults demonstrating how our LLM-based implementation of a known,\ndata-efficient RL algorithm can be considerably more effective in natural\nlanguage tasks that demand prudent exploration."}, {"arxiv_id": "2504.20996", "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models", "authors": ["Sicheng Mo", "Thao Nguyen", "Xun Huang", "Siddharth Srinivasan Iyer", "Yijun Li", "Yuchen Liu", "Abhishek Tandon", "Eli Shechtman", "Krishna Kumar Singh", "Yong Jae Lee", "Bolei Zhou", "Yuheng Li"], "published": "2025-04-29T17:59:45Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20996v1", "categories": ["cs.CV"], "abstract": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels."}, {"arxiv_id": "2504.20989", "title": "Photonic Quantum Convolutional Neural Networks with Adaptive State\n  Injection", "authors": ["Léo Monbroussou", "Beatrice Polacchi", "Verena Yacoub", "Eugenio Caruccio", "Giovanni Rodari", "Francesco Hoch", "Gonzalo Carvacho", "Nicolò Spagnolo", "Taira Giordani", "Mattia Bossi", "Abhiram Rajan", "Niki Di Giano", "Riccardo Albiero", "Francesco Ceccarelli", "Roberto Osellame", "Elham Kashefi", "Fabio Sciarrino"], "published": "2025-04-29T17:57:01Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20989v1", "categories": ["quant-ph"], "abstract": "Linear optical architectures have been extensively investigated for quantum\ncomputing and quantum machine learning applications. Recently, proposals for\nphotonic quantum machine learning have combined linear optics with resource\nadaptivity, such as adaptive circuit reconfiguration, which promises to enhance\nexpressivity and improve algorithm performances and scalability. Moreover,\nlinear optical platforms preserve some subspaces due to the fixed number of\nparticles during the computation, a property recently exploited to design a\nnovel quantum convolutional neural networks. This last architecture has shown\nan advantage in terms of running time complexity and of the number of\nparameters needed with respect to other quantum neural network proposals. In\nthis work, we design and experimentally implement the first photonic quantum\nconvolutional neural network (PQCNN) architecture based on particle-number\npreserving circuits equipped with state injection, an approach recently\nproposed to increase the controllability of linear optical circuits.\nSubsequently, we experimentally validate the PQCNN for a binary image\nclassification on a photonic platform utilizing a semiconductor quantum\ndot-based single-photon source and programmable integrated photonic\ninterferometers comprising 8 and 12 modes. In order to investigate the\nscalability of the PQCNN design, we have performed numerical simulations on\ndatasets of different sizes. We highlight the potential utility of a simple\nadaptive technique for a nonlinear Boson Sampling task, compatible with\nnear-term quantum devices."}, {"arxiv_id": "2504.20984", "title": "ACE: A Security Architecture for LLM-Integrated App Systems", "authors": ["Evan Li", "Tushin Mallick", "Evan Rose", "William Robertson", "Alina Oprea", "Cristina Nita-Rotaru"], "published": "2025-04-29T17:55:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20984v1", "categories": ["cs.CR", "cs.LG"], "abstract": "LLM-integrated app systems extend the utility of Large Language Models (LLMs)\nwith third-party apps that are invoked by a system LLM using interleaved\nplanning and execution phases to answer user queries. These systems introduce\nnew attack vectors where malicious apps can cause integrity violation of\nplanning or execution, availability breakdown, or privacy compromise during\nexecution.\n  In this work, we identify new attacks impacting the integrity of planning, as\nwell as the integrity and availability of execution in LLM-integrated apps, and\ndemonstrate them against IsolateGPT, a recent solution designed to mitigate\nattacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new\nsecure architecture for LLM-integrated app systems that provides security\nguarantees for system planning and execution. Specifically, ACE decouples\nplanning into two phases by first creating an abstract execution plan using\nonly trusted information, and then mapping the abstract plan to a concrete plan\nusing installed system apps. We verify that the plans generated by our system\nsatisfy user-specified secure information flow constraints via static analysis\non the structured plan output. During execution, ACE enforces data and\ncapability barriers between apps, and ensures that the execution is conducted\naccording to the trusted abstract plan. We show experimentally that our system\nis secure against attacks from the INJECAGENT benchmark, a standard benchmark\nfor control flow integrity in the face of indirect prompt injection attacks,\nand our newly introduced attacks. Our architecture represents a significant\nadvancement towards hardening LLM-based systems containing system facilities of\nvarying levels of trustworthiness."}, {"arxiv_id": "2504.20980", "title": "Jekyll-and-Hyde Tipping Point in an AI's Behavior", "authors": ["Neil F. Johnson", "Frank Yingjie Huo"], "published": "2025-04-29T17:50:29Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.20980v1", "categories": ["cs.AI", "cs.CY", "nlin.AO", "physics.comp-ph", "physics.soc-ph"], "abstract": "Trust in AI is undermined by the fact that there is no science that predicts\n-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is\nlikely to tip mid-response to become wrong, misleading, irrelevant or\ndangerous. With deaths and trauma already being blamed on LLMs, this\nuncertainty is even pushing people to treat their 'pet' LLM more politely to\n'dissuade' it (or its future Artificial General Intelligence offspring) from\nsuddenly turning on them. Here we address this acute need by deriving from\nfirst principles an exact formula for when a Jekyll-and-Hyde tipping point\noccurs at LLMs' most basic level. Requiring only secondary school mathematics,\nit shows the cause to be the AI's attention spreading so thin it suddenly\nsnaps. This exact formula provides quantitative predictions for how the\ntipping-point can be delayed or prevented by changing the prompt and the AI's\ntraining. Tailored generalizations will provide policymakers and the public\nwith a firm platform for discussing any of AI's broader uses and risks, e.g. as\na personal counselor, medical advisor, decision-maker for when to use force in\na conflict situation. It also meets the need for clear and transparent answers\nto questions like ''should I be polite to my LLM?''"}, {"arxiv_id": "2504.21855", "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction", "authors": ["Qihao Liu", "Ju He", "Qihang Yu", "Liang-Chieh Chen", "Alan Yuille"], "published": "2025-04-30T17:59:56Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.21855v1", "categories": ["cs.CV"], "abstract": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration."}, {"arxiv_id": "2504.21851", "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and\n  Structured Assessments", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "published": "2025-04-30T17:58:06Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.21851v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability."}, {"arxiv_id": "2504.21847", "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors", "authors": ["Derong Jin", "Ruohan Gao"], "published": "2025-04-30T17:55:29Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.21847v1", "categories": ["cs.CV", "cs.SD"], "abstract": "An immersive acoustic experience enabled by spatial audio is just as crucial\nas the visual aspect in creating realistic virtual environments. However,\nexisting methods for room impulse response estimation rely either on\ndata-demanding learning-based models or computationally expensive physics-based\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\nRendering (AV-DAR), a framework that leverages visual cues extracted from\nmulti-view images and acoustic beam tracing for physics-based room acoustic\nrendering. Experiments across six real-world environments from two datasets\ndemonstrate that our multimodal, physics-based approach is efficient,\ninterpretable, and accurate, significantly outperforming a series of prior\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\ncomparable performance to models trained on 10 times more data while delivering\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale."}, {"arxiv_id": "2504.21846", "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "authors": ["Hadleigh Schwartz", "Xiaofeng Yan", "Charles J. Carver", "Xia Zhou"], "published": "2025-04-30T17:55:24Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.21846v1", "categories": ["cs.CV", "cs.AI", "cs.CR"], "abstract": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies."}, {"arxiv_id": "2504.21842", "title": "Cryptography without Long-Term Quantum Memory and Global Entanglement", "authors": ["Lev Stambler"], "published": "2025-04-30T17:51:25Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2504.21842v1", "categories": ["quant-ph", "cs.CR"], "abstract": "We show how oracles which only allow for classical query access can be used\nto construct a variety of quantum cryptographic primitives which do not require\nlong-term quantum memory or global entanglement. Specifically, if a quantum\nparty can execute a semi-quantum token scheme (Shmueli 2022) with probability\nof success $1/2 + \\delta$, we can build powerful cryptographic primitives with\na multiplicative logarithmic overhead for the desired correctness error. Our\nscheme makes no assumptions about the quantum party's noise model except for a\nsimple independence requirement: noise on two sets of non-entangled hardware\nmust be independent.\n  Using semi-quantum tokens and oracles which can only be queried classically,\nwe first show how to construct a \"short-lived\" semi-quantum one-time program\n(OTP) which allows a classical sending party to prepare a one-time program on\nthe receiving party's quantum computer. We then show how to use this\nsemi-quantum OTP to construct a semi-quantum \"stateful obfuscation\" scheme\n(which we term \"RAM obfuscation\"). Importantly, the RAM obfuscation scheme does\nnot require long-term quantum memory or global entanglement. Finally, we show\nhow RAM obfuscation can be used to build long-lived one-time programs and\ncopy-protection schemes."}, {"arxiv_id": "2505.00693", "title": "Robotic Visual Instruction", "authors": ["Yanbang Li", "Ziyang Gong", "Haoyang Li", "Haoyang Li", "Xiaoqi Huang", "Haolan Kang", "Guangping Bai", "Xianzheng Ma"], "published": "2025-05-01T17:55:05Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.00693v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "abstract": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision for robotic\ncontrol introduces challenges such as ambiguity and verbosity. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment, enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Code and\nDatasets in this paper will be released soon."}, {"arxiv_id": "2505.00688", "title": "The Architecture Tradeoff and Risk Analysis Framework (ATRAF): A Unified\n  Approach for Evaluating Software Architectures, Reference Architectures, and\n  Architectural Frameworks", "authors": ["Amine Ben Hassouna"], "published": "2025-05-01T17:48:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.00688v1", "categories": ["cs.SE"], "abstract": "Modern software systems are guided by hierarchical architectural concepts --\nsoftware architectures, reference architectures, and architectural frameworks\n-- each operating at a distinct level of abstraction. These artifacts promote\nreuse, scalability, and consistency, but also embed tradeoffs that shape\ncritical quality attributes such as modifiability, performance, and security.\nExisting evaluation methods, such as the Architecture Tradeoff Analysis Method\n(ATAM), focus on system-specific architectures and are not designed to address\nthe broader generality and variability of higher-level architectural forms. To\nclose this gap, we introduce the Architecture Tradeoff and Risk Analysis\nFramework (ATRAF) -- a unified, scenario-driven framework for evaluating\ntradeoffs and risks across architectural levels. ATRAF encompasses three\nmethods: the Architecture Tradeoff and Risk Analysis Method (ATRAM), extending\nATAM with enhanced risk identification for concrete systems; the Reference\nArchitecture Tradeoff and Risk Analysis Method (RATRAM), adapting ATRAM to the\nevaluation of domain-level reference architectures; and the Architectural\nFramework Tradeoff and Risk Analysis Method (AFTRAM), supporting the evaluation\nof architectural frameworks that guide entire system families. All three\nmethods follow an iterative spiral process that enables the identification of\nsensitivities, tradeoffs, and risks while supporting continuous refinement of\narchitectural artifacts. We demonstrate ATRAF through progressively abstracted\nexamples derived from the Remote Temperature Sensor (RTS) case, originally\nintroduced in the ATAM literature. ATRAF equips architects, reference modelers,\nand framework designers with a practical, systematic approach for analyzing\ndesign alternatives and managing quality attribute tradeoffs early in the\nlifecycle and across all levels of architectural abstraction."}, {"arxiv_id": "2505.00686", "title": "Quantum information engines: Bounds on performance metrics by\n  measurement time", "authors": ["Henning Kirchberg", "Abraham Nitzan"], "published": "2025-05-01T17:48:00Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.00686v1", "categories": ["quant-ph"], "abstract": "Information engines, sometimes referred to as Maxwell Demon engines, utilize\ninformation obtained through measurement to control the conversion of energy\ninto useful work. Discussions around such devices often assume the measurement\nstep to be instantaneous, assessing its cost by Landauer's information erasure\nwithin the measurement device. While this simplified perspective is sufficient\nfor classical feedback-controlled engines, for nanoengines that often operate\nin the quantum realm, the overall performance may be significantly affected by\nthe measurement duration (which may be comparable to the engine's cycle time)\nand cost (energy needed to create the system-meter correlation). In this study,\nwe employ a generalized von-Neumann measurement model to highlight that\nobtaining a finite amount of information requires a finite measurement time and\nincurs an energetic cost. We investigate the crucial role of these factors in\ndetermining the engine's performance, particularly in terms of efficiency and\npower output. Furthermore, for the information engine model under\nconsideration, we establish a precise relationship between the acquired\ninformation in the measurement process and the maximum energy extractable\nthrough the measurement. We also discuss ways to extend our considerations\nusing these concepts, such as in measurement-enhanced photochemical reactions."}, {"arxiv_id": "2505.00681", "title": "MINERVA: Evaluating Complex Video Reasoning", "authors": ["Arsha Nagrani", "Sachit Menon", "Ahmet Iscen", "Shyamal Buch", "Ramin Mehran", "Nilpa Jha", "Anja Hauth", "Yukun Zhu", "Carl Vondrick", "Mikhail Sirotenko", "Cordelia Schmid", "Tobias Weyand"], "published": "2025-05-01T17:41:49Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.00681v1", "categories": ["cs.LG", "cs.CV"], "abstract": "Multimodal LLMs are turning their focus to video benchmarks, however most\nvideo benchmarks only provide outcome supervision, with no intermediate or\ninterpretable reasoning steps. This makes it challenging to assess if models\nare truly able to combine perceptual and temporal information to reason about\nvideos, or simply get the correct answer by chance or by exploiting linguistic\nbiases. To remedy this, we provide a new video reasoning dataset called MINERVA\nfor modern multimodal models. Each question in the dataset comes with 5 answer\nchoices, as well as detailed, hand-crafted reasoning traces. Our dataset is\nmultimodal, diverse in terms of video domain and length, and consists of\ncomplex multi-step questions. Extensive benchmarking shows that our dataset\nprovides a challenge for frontier open-source and proprietary models. We\nperform fine-grained error analysis to identify common failure modes across\nvarious models, and create a taxonomy of reasoning errors. We use this to\nexplore both human and LLM-as-a-judge methods for scoring video reasoning\ntraces, and find that failure modes are primarily related to temporal\nlocalization, followed by visual perception errors, as opposed to logical or\ncompleteness errors. The dataset, along with questions, answer candidates and\nreasoning traces will be publicly available under\nhttps://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva."}, {"arxiv_id": "2505.00679", "title": "Steering Large Language Models with Register Analysis for Arbitrary\n  Style Transfer", "authors": ["Xinchen Yang", "Marine Carpuat"], "published": "2025-05-01T17:39:02Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.00679v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in\nrewriting text across various styles. However, effectively leveraging this\nability for example-based arbitrary style transfer, where an input text is\nrewritten to match the style of a given exemplar, remains an open challenge. A\nkey question is how to describe the style of the exemplar to guide LLMs toward\nhigh-quality rewrites. In this work, we propose a prompting method based on\nregister analysis to guide LLMs to perform this task. Empirical evaluations\nacross multiple style transfer tasks show that our prompting approach enhances\nstyle transfer strength while preserving meaning more effectively than existing\nprompting strategies."}, {"arxiv_id": "2505.01408", "title": "Carbon fiber damage evolution under flame attack and the role of\n  impurities", "authors": ["Pablo Chavez-Gomez", "Tanja Pelzmann", "Darren R. Hall", "Cornelia Chilian", "Louis Laberge Lebel", "Etienne Robert"], "published": "2025-05-02T17:37:25Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.01408v1", "categories": ["physics.flu-dyn"], "abstract": "Carbon fibers (CFs) are prone to extensive oxidation under fire attack, for\ninstance, in an aircraft fire scenario. This work addresses the damage\nmechanisms observed on polyacrylonitrile (PAN)-based CFs with different\nmicrostructure exposed to open flames. A fixed-point technique was developed to\nfollow up individual CFs by means of time-controlled insertion into premixed\nmethane/air flames, followed by scanning electron microscopy (SEM) and\nenergy-dispersive X-ray spectroscopy (EDS) analyses. Besides diameter\nreduction, three localized damage mechanisms were discerned in presence of\nimpurities, which were quantified by neutron activation analysis (NAA). Severe\npitting was ascribed to catalytic oxidation mainly caused by alkali and\nalkaline earth metals. After an initial period where catalytic reactions\nbetween impurities and the carbon surface dominate, the flame stoichiometry\ngoverned the CF gasification process, with lean flames being much more\naggressive than rich ones. A second mechanism, channelling, was caused by\nmobile metallic impurities. Some impurities showed an opposite effect, lowering\nreactivity and thus preventing further catalysis. Amorphous damage with a\nskin-peeling effect is believed to be the result of localized impurities at\nhigh concentrations and microstructural variations. Hindrance or synergistic\neffects between impurities are discussed. Finally, apparent axial pit growth\nrates were determined and compared with other carbonaceous materials, revealing\na strong influence of impurities and the flame reactive atmosphere on CF\noxidation."}, {"arxiv_id": "2505.01406", "title": "VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in\n  Video Diffusion Models", "authors": ["Mohammadreza Teymoorianfard", "Shiqing Ma", "Amir Houmansadr"], "published": "2025-05-02T17:35:03Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.01406v1", "categories": ["cs.CV", "cs.CR", "cs.LG"], "abstract": "The rapid rise of video diffusion models has enabled the generation of highly\nrealistic and temporally coherent videos, raising critical concerns about\ncontent authenticity, provenance, and misuse. Existing watermarking approaches,\nwhether passive, post-hoc, or adapted from image-based techniques, often\nstruggle to withstand video-specific manipulations such as frame insertion,\ndropping, or reordering, and typically degrade visual quality. In this work, we\nintroduce VIDSTAMP, a watermarking framework that embeds per-frame or\nper-segment messages directly into the latent space of temporally-aware video\ndiffusion models. By fine-tuning the model's decoder through a two-stage\npipeline, first on static image datasets to promote spatial message separation,\nand then on synthesized video sequences to restore temporal consistency,\nVIDSTAMP learns to embed high-capacity, flexible watermarks with minimal\nperceptual impact. Leveraging architectural components such as 3D convolutions\nand temporal attention, our method imposes no additional inference cost and\noffers better perceptual quality than prior methods, while maintaining\ncomparable robustness against common distortions and tampering. VIDSTAMP embeds\n768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a\nlog P-value of -166.65 (lower is better), and maintains a video quality score\nof 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior\nmethods in capacity-quality tradeoffs. Code: Code:\n\\url{https://github.com/SPIN-UMass/VidStamp}"}, {"arxiv_id": "2505.01404", "title": "Quasiparticle Interference of Spin-Triplet Superconductors: Application\n  to UTe$_2$", "authors": ["Hans Christiansen", "Brian M. Andersen", "P. J. Hirschfeld", "Andreas Kreisel"], "published": "2025-05-02T17:28:10Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.01404v1", "categories": ["cond-mat.supr-con", "cond-mat.str-el"], "abstract": "Quasiparticle interference (QPI) obtained from scanning tunneling microscopy\n(STM) is a powerful method to help extract the pairing symmetry of\nunconventional superconductors. We examine the general properties of QPI on\nsurfaces of spin-triplet superconductors, where the properties of the $\\vec\nd$-vector order parameter and topological surface bound states offer important\ndifferences from QPI on spin-singlet superconducting materials. We then apply\nthe theory to a model specific to UTe$_2$, and compare the resulting QPI with\nrecent STM measurements. We conclude that the two candidate Cooper pair\ninstabilities $B_{2u}$ and $B_{3u}$ exhibit distinct features in the QPI\nintensity to discriminate these using the experimental data. Characteristic\nfeatures of the emergent topological surface states protected by mirror\nsymmetries provide further unique signatures to help pinpointing the pairing\nsymmetry channel of UTe$_2$."}, {"arxiv_id": "2505.01384", "title": "Experiments in a novel quasi-1D diffusion flame with variable bulk flow", "authors": ["Etienne Robert", "Peter A. Monkewitz"], "published": "2025-05-02T16:48:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.01384v1", "categories": ["physics.flu-dyn"], "abstract": "The novel species injector of a recently developed research burner,\nconsisting of an array of hypodermic needles, which allows to produce quasi\none-dimensional unstrained diffusion flames has been improved. It is used in a\nnew symmetric design with fuel and oxidizer injected through needle arrays\nwhich allows to independently choose both the magnitude and direction of the\nbulk flow through the flame. A simplified theoretical model for the flame\nposition with variable bulk flow is presented which accounts for the transport\nproperties of both reactants. The model results are compared to experiments\nwith a CO2-diluted H2-O2 flame and variable bulk flow. The mixture composition\nthroughout the burning chamber is monitored by mass spectrometry. The resulting\nconcentration profiles are also compared to the simplified theory and\ndemonstrate that the new burner configuration produces a good approximation of\nthe 1-D chambered diffusion flame, which has been used extensively for the\nstability analysis of diffusion flames. Hence, the new research burner opens up\nnew possibilities for the experimental validation of theoretical models\ndeveloped in the idealized unstrained 1-D chambered flame configuration, in\nparticular models concerning the effect of bulk flow magnitude and direction on\nflame stability. Some preliminary results are presented on the effect of bulk\nflow direction on the thermal-diffusive cellular flame instability."}, {"arxiv_id": "2505.01379", "title": "Observation of Full Hierarchy of Temporal Quantum Correlations with a\n  Superconducting Qubit", "authors": ["Hao-Cheng Weng", "Chen-Yeh Wei", "Huan-Yu Ku", "Shin-Liang Chen", "Yueh-Nan Chen", "Chih-Sung Chuu"], "published": "2025-05-02T16:37:14Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.01379v1", "categories": ["quant-ph"], "abstract": "Temporal quantum correlations provide an intriguing way of testing\nquantumness at the macroscopic level, with a logical hierarchy present among\nthe quantum correlations associated with nonmacrorealism, temporal steering,\nand temporal inseparability. By manipulating the dynamics of a superconducting\nqubit, we observe the full hierarchy of temporal quantum correlations.\nMoreover, we show that the rich dynamics of the temporal quantum correlations,\nsuch as sudden death or revival of temporal steering, provides a useful and\nunique measure for benchmarking qubits on a realistic circuit. Our work finds\napplications in identifying the casual structure in a quantum network, the\nnon-Markovianity of open quantum systems, and the security bounds of quantum\nkey distribution. As an example, we demonstrate the non-Markovianity of a\nsingle superconducting qubit on the quantum circuit."}, {"arxiv_id": "2505.02836", "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene\n  Generation", "authors": ["Lu Ling", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yifan Ding", "Yu Zeng", "Yichen Sheng", "Yunhao Ge", "Ming-Yu Liu", "Aniket Bera", "Zhaoshuo Li"], "published": "2025-05-05T17:59:58Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.02836v1", "categories": ["cs.CV"], "abstract": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch."}, {"arxiv_id": "2505.02828", "title": "Privacy Risks and Preservation Methods in Explainable Artificial\n  Intelligence: A Scoping Review", "authors": ["Sonal Allana", "Mohan Kankanhalli", "Rozita Dara"], "published": "2025-05-05T17:53:28Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.02828v1", "categories": ["cs.AI", "cs.CR", "cs.ET"], "abstract": "Explainable Artificial Intelligence (XAI) has emerged as a pillar of\nTrustworthy AI and aims to bring transparency in complex models that are opaque\nby nature. Despite the benefits of incorporating explanations in models, an\nurgent need is found in addressing the privacy concerns of providing this\nadditional information to end users. In this article, we conduct a scoping\nreview of existing literature to elicit details on the conflict between privacy\nand explainability. Using the standard methodology for scoping review, we\nextracted 57 articles from 1,943 studies published from January 2019 to\nDecember 2024. The review addresses 3 research questions to present readers\nwith more understanding of the topic: (1) what are the privacy risks of\nreleasing explanations in AI systems? (2) what current methods have researchers\nemployed to achieve privacy preservation in XAI systems? (3) what constitutes a\nprivacy preserving explanation? Based on the knowledge synthesized from the\nselected studies, we categorize the privacy risks and preservation methods in\nXAI and propose the characteristics of privacy preserving explanations to aid\nresearchers and practitioners in understanding the requirements of XAI that is\nprivacy compliant. Lastly, we identify the challenges in balancing privacy with\nother system desiderata and provide recommendations for achieving privacy\npreserving XAI. We expect that this review will shed light on the complex\nrelationship of privacy and explainability, both being the fundamental\nprinciples of Trustworthy AI."}, {"arxiv_id": "2505.02824", "title": "Towards Dataset Copyright Evasion Attack against Personalized\n  Text-to-Image Diffusion Models", "authors": ["Kuofeng Gao", "Yufei Zhu", "Yiming Li", "Jiawang Bai", "Yong Yang", "Zhifeng Li", "Shu-Tao Xia"], "published": "2025-05-05T17:51:55Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.02824v1", "categories": ["cs.CV", "cs.AI", "cs.CR"], "abstract": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance."}, {"arxiv_id": "2505.02820", "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback", "authors": ["Hao Zhu", "Phil Cuvin", "Xinkai Yu", "Charlotte Ka Yee Yan", "Jason Zhang", "Diyi Yang"], "published": "2025-05-05T17:47:49Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.02820v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."}, {"arxiv_id": "2505.02819", "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear\n  Transformations", "authors": ["Dmitriy Shopkhoev", "Ammar Ali", "Magauiya Zhussip", "Valentin Malykh", "Stamatios Lefkimmiatis", "Nikos Komodakis", "Sergey Zagoruyko"], "published": "2025-05-05T17:47:42Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.02819v1", "categories": ["cs.CL"], "abstract": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository."}, {"arxiv_id": "2505.03733", "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "published": "2025-05-06T17:59:15Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.03733v1", "categories": ["cs.CL"], "abstract": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model."}, {"arxiv_id": "2505.03730", "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios", "authors": ["Shiyi Zhang", "Junhao Zhuang", "Zhaoyang Zhang", "Ying Shan", "Yansong Tang"], "published": "2025-05-06T17:58:02Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.03730v1", "categories": ["cs.CV", "cs.AI", "cs.MM"], "abstract": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/"}, {"arxiv_id": "2505.03721", "title": "Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency\n  with Decision Theory-Guided Deep Reinforcement Learning", "authors": ["Dian Chen", "Zelin Wan", "Dong Sam Ha", "Jin-Hee Cho"], "published": "2025-05-06T17:49:06Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.03721v1", "categories": ["cs.LG", "cs.MA"], "abstract": "Solar sensor-based monitoring systems have become a crucial agricultural\ninnovation, advancing farm management and animal welfare through integrating\nsensor technology, Internet-of-Things, and edge and cloud computing. However,\nthe resilience of these systems to cyber-attacks and their adaptability to\ndynamic and constrained energy supplies remain largely unexplored. To address\nthese challenges, we propose a sustainable smart farm network designed to\nmaintain high-quality animal monitoring under various cyber and adversarial\nthreats, as well as fluctuating energy conditions. Our approach utilizes deep\nreinforcement learning (DRL) to devise optimal policies that maximize both\nmonitoring effectiveness and energy efficiency. To overcome DRL's inherent\nchallenge of slow convergence, we integrate transfer learning (TL) and decision\ntheory (DT) to accelerate the learning process. By incorporating DT-guided\nstrategies, we optimize monitoring quality and energy sustainability,\nsignificantly reducing training time while achieving comparable performance\nrewards. Our experimental results prove that DT-guided DRL outperforms\nTL-enhanced DRL models, improving system performance and reducing training\nruntime by 47.5%."}, {"arxiv_id": "2505.03715", "title": "DISARM++: Beyond scanner-free harmonization", "authors": ["Luca Caldera", "Lara Cavinato", "Alessio Cirone", "Isabella Cama", "Sara Garbarino", "Raffaele Lodi", "Fabrizio Tagliavini", "Anna Nigri", "Silvia De Francesco", "Andrea Cappozzo", "Michele Piana", "Francesca Ieva"], "published": "2025-05-06T17:36:49Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.03715v1", "categories": ["cs.CV"], "abstract": "Harmonization of T1-weighted MR images across different scanners is crucial\nfor ensuring consistency in neuroimaging studies. This study introduces a novel\napproach to direct image harmonization, moving beyond feature standardization\nto ensure that extracted features remain inherently reliable for downstream\nanalysis. Our method enables image transfer in two ways: (1) mapping images to\na scanner-free space for uniform appearance across all scanners, and (2)\ntransforming images into the domain of a specific scanner used in model\ntraining, embedding its unique characteristics. Our approach presents strong\ngeneralization capability, even for unseen scanners not included in the\ntraining phase. We validated our method using MR images from diverse cohorts,\nincluding healthy controls, traveling subjects, and individuals with\nAlzheimer's disease (AD). The model's effectiveness is tested in multiple\napplications, such as brain age prediction (R2 = 0.60 \\pm 0.05), biomarker\nextraction, AD classification (Test Accuracy = 0.86 \\pm 0.03), and diagnosis\nprediction (AUC = 0.95). In all cases, our harmonization technique outperforms\nstate-of-the-art methods, showing improvements in both reliability and\npredictive accuracy. Moreover, our approach eliminates the need for extensive\npreprocessing steps, such as skull-stripping, which can introduce errors by\nmisclassifying brain and non-brain structures. This makes our method\nparticularly suitable for applications that require full-head analysis,\nincluding research on head trauma and cranial deformities. Additionally, our\nharmonization model does not require retraining for new datasets, allowing\nsmooth integration into various neuroimaging workflows. By ensuring\nscanner-invariant image quality, our approach provides a robust and efficient\nsolution for improving neuroimaging studies across diverse settings. The code\nis available at this link."}, {"arxiv_id": "2505.03704", "title": "Multi-modal cascade feature transfer for polymer property prediction", "authors": ["Kiichi Obuchi", "Yuta Yahagi", "Kiyohiko Toyama", "Shukichi Tanaka", "Kota Matsui"], "published": "2025-05-06T17:24:43Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.03704v1", "categories": ["stat.ML", "cs.LG"], "abstract": "In this paper, we propose a novel transfer learning approach called\nmulti-modal cascade model with feature transfer for polymer property\nprediction.Polymers are characterized by a composite of data in several\ndifferent formats, including molecular descriptors and additive information as\nwell as chemical structures. However, in conventional approaches, prediction\nmodels were often constructed using each type of data separately. Our model\nenables more accurate prediction of physical properties for polymers by\ncombining features extracted from the chemical structure by graph convolutional\nneural networks (GCN) with features such as molecular descriptors and additive\ninformation. The predictive performance of the proposed method is empirically\nevaluated using several polymer datasets. We report that the proposed method\nshows high predictive performance compared to the baseline conventional\napproach using a single feature."}, {"arxiv_id": "2505.04623", "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning", "authors": ["Zhenghao Xing", "Xiaowei Hu", "Chi-Wing Fu", "Wenhai Wang", "Jifeng Dai", "Pheng-Ann Heng"], "published": "2025-05-07T17:59:49Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.04623v1", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.MM", "cs.SD"], "abstract": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research."}, {"arxiv_id": "2505.04620", "title": "On Path to Multimodal Generalist: General-Level and General-Bench", "authors": ["Hao Fei", "Yuan Zhou", "Juncheng Li", "Xiangtai Li", "Qingshan Xu", "Bobo Li", "Shengqiong Wu", "Yaoting Wang", "Junbao Zhou", "Jiahao Meng", "Qingyu Shi", "Zhiyuan Zhou", "Liangtao Shi", "Minghe Gao", "Daoan Zhang", "Zhiqi Ge", "Weiming Wu", "Siliang Tang", "Kaihang Pan", "Yaobo Ye", "Haobo Yuan", "Tao Zhang", "Tianjie Ju", "Zixiang Meng", "Shilin Xu", "Liyu Jia", "Wentao Hu", "Meng Luo", "Jiebo Luo", "Tat-Seng Chua", "Shuicheng Yan", "Hanwang Zhang"], "published": "2025-05-07T17:59:32Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.04620v1", "categories": ["cs.CV"], "abstract": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/"}, {"arxiv_id": "2505.04606", "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution", "authors": ["Lianghong Guo", "Wei Tao", "Runhan Jiang", "Yanlin Wang", "Jiachi Chen", "Xilin Liu", "Yuchi Ma", "Mingzhi Mao", "Hongyu Zhang", "Zibin Zheng"], "published": "2025-05-07T17:51:10Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.04606v1", "categories": ["cs.SE"], "abstract": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements."}, {"arxiv_id": "2505.04602", "title": "Extracting local velocity from cosmic dipole using simulations", "authors": ["Mohit Panwar", "Akash Gandhi", "Pankaj Jain"], "published": "2025-05-07T17:48:56Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.04602v1", "categories": ["astro-ph.CO"], "abstract": "Our velocity with respect to the cosmic frame of rest leads to a dipole in\nthe number count distribution of galaxies. The dipole depends on the source\nspectrum, which is usually assumed to be a power law, $S(\\nu) \\propto\n\\nu^{-\\alpha}$ and on the flux dependence of the number density of sources. The\nlatter is also generally assumed to be a power law, parametrised with exponent\n$x$. The velocity can be extracted from the observed dipole once the two\nparameters $x$ and $\\alpha$ are known. The standard procedure uses the mean\nvalue of $\\alpha$ across the entire sample, and the parameter $x$ is inferred\nby fitting the cumulative number count, $\\frac{dN}{d\\Omega}(>S_*) \\propto\nS_*^{-x}$, near the flux limit $S_*$ of the survey. Here, we introduce a\nsimulation procedure to extract the velocity which directly uses the $\\alpha$\nvalues of each source rather than their mean and does not rely on the\nfunctional form of the cumulative number count near the flux limit. We apply\nthis to the quasar sample in CatWISE2020 data and find that the final results\ndiffer from the standard procedure by approximately one sigma."}, {"arxiv_id": "2505.04598", "title": "Post-selection free time-bin entanglement on a thin-film lithium niobate\n  photonic chip", "authors": ["Marcello Bacchi", "Andrea Bernardi", "Marco Clementi", "Sara Congia", "Francesco Garrisi", "Andrea Martellosio", "Marco Passoni", "Alexander Wrobel", "Federico Andrea Sabattoli", "Matteo Galli", "Daniele Bajoni"], "published": "2025-05-07T17:39:03Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.04598v1", "categories": ["quant-ph"], "abstract": "Time-bin entanglement is the most commonly used form of entanglement for\nquantum communication protocols over fiber networks, due to the natural\nresilience of this encoding scheme to thermal phase fluctuations in optical\nfibers. Projective measurements on some bases in the time-bin encoding need,\nhowever, post-selection of the measured events, introducing a loophole in Bell\ntests and requiring high temporal resolution. In this work, we demonstrate\nchip-integrated receivers for time-bin entanglement certification including a\nhigh-speed optical switch to remove such post-selection loophole. The receivers\nare realized using thin-film lithium niobate and operate at a switching\nfrequency of 5 GHz, enabling high secure key rates. We demonstrate a Bell\ninequality violation by more than 24 standard deviations without the need for\ntime resolution at the time-bin separation level."}, {"arxiv_id": "2505.05467", "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant", "authors": ["Haibo Wang", "Bo Feng", "Zhengfeng Lai", "Mingze Xu", "Shiyu Li", "Weifeng Ge", "Afshin Dehghan", "Meng Cao", "Ping Huang"], "published": "2025-05-08T17:57:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.05467v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks."}, {"arxiv_id": "2505.05465", "title": "ComPO: Preference Alignment via Comparison Oracles", "authors": ["Peter Chen", "Xi Chen", "Wotao Yin", "Tianyi Lin"], "published": "2025-05-08T17:56:57Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.05465v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}."}, {"arxiv_id": "2505.05464", "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging", "authors": ["Shiqi Chen", "Jinghan Zhang", "Tongyao Zhu", "Wei Liu", "Siyang Gao", "Miao Xiong", "Manling Li", "Junxian He"], "published": "2025-05-08T17:56:23Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.05464v1", "categories": ["cs.CL"], "abstract": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation."}, {"arxiv_id": "2505.05453", "title": "Conversational Process Model Redesign", "authors": ["Nataliia Klievtsova", "Timotheus Kampik", "Juergen Mangler", "Stefanie Rinderle-Ma"], "published": "2025-05-08T17:44:45Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.05453v1", "categories": ["cs.AI"], "abstract": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria."}, {"arxiv_id": "2505.05445", "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations", "authors": ["Chalamalasetti Kranti", "Sherzod Hakimov", "David Schlangen"], "published": "2025-05-08T17:36:36Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.05445v1", "categories": ["cs.CL"], "abstract": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems."}, {"arxiv_id": "2505.06217", "title": "Adapting a Segmentation Foundation Model for Medical Image\n  Classification", "authors": ["Pengfei Gu", "Haoteng Tang", "Islam A. Ebeid", "Jose A. Nunez", "Fabian Vazquez", "Diego Adame", "Marcus Zhan", "Huimin Li", "Bin Fu", "Danny Z. Chen"], "published": "2025-05-09T17:51:51Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.06217v1", "categories": ["cs.CV"], "abstract": "Recent advancements in foundation models, such as the Segment Anything Model\n(SAM), have shown strong performance in various vision tasks, particularly\nimage segmentation, due to their impressive zero-shot segmentation\ncapabilities. However, effectively adapting such models for medical image\nclassification is still a less explored topic. In this paper, we introduce a\nnew framework to adapt SAM for medical image classification. First, we utilize\nthe SAM image encoder as a feature extractor to capture segmentation-based\nfeatures that convey important spatial and contextual details of the image,\nwhile freezing its weights to avoid unnecessary overhead during training. Next,\nwe propose a novel Spatially Localized Channel Attention (SLCA) mechanism to\ncompute spatially localized attention weights for the feature maps. The\nfeatures extracted from SAM's image encoder are processed through SLCA to\ncompute attention weights, which are then integrated into deep learning\nclassification models to enhance their focus on spatially relevant or\nmeaningful regions of the image, thus improving classification performance.\nExperimental results on three public medical image classification datasets\ndemonstrate the effectiveness and data-efficiency of our approach."}, {"arxiv_id": "2505.06211", "title": "Alternating Methods for Large-Scale AC Optimal Power Flow with Unit\n  Commitment", "authors": ["Matthew Brun", "Thomas Lee", "Dirk Lauinger", "Xin Chen", "Xu Andy Sun"], "published": "2025-05-09T17:41:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.06211v1", "categories": ["math.OC", "49M27, 90C06, 90B99"], "abstract": "Security-constrained unit commitment with alternating current optimal power\nflow (SCUC-ACOPF) is a central problem in power grid operations that optimizes\ncommitment and dispatch of generators under a physically accurate power\ntransmission model while encouraging robustness against component failures.\nSCUC-ACOPF requires solving large-scale problems that involve multiple time\nperiods and networks with thousands of buses within strict time limits. In this\nwork, we study a detailed SCUC-ACOPF model with a rich set of features of\nmodern power grids, including price-sensitive load, reserve products,\ntransformer controls, and energy-limited devices. We propose a decomposition\nscheme and a penalty alternating direction method to find high-quality\nsolutions to this model. Our methodology leverages spatial and temporal\ndecomposition, separating the problem into a set of mixed-integer linear\nprograms for each bus and a set of continuous nonlinear programs for each time\nperiod. To improve the performance of the algorithm, we introduce a variety of\nheuristics, including restrictions of temporal linking constraints, a\nsecond-order cone relaxation, and a contingency screening algorithm. We\nquantify the quality of feasible solutions through a dual bound from a convex\nsecond-order cone program. To evaluate our algorithm, we use large-scale test\ncases from Challenge 3 of the U.S. Department of Energy's Grid Optimization\nCompetition that resemble real power grid data under a variety of operating\nconditions and decision horizons. The experiments yield feasible solutions with\nan average optimality gap of 1.33%, demonstrating that this approach generates\nnear-optimal solutions within stringent time limits."}, {"arxiv_id": "2505.06207", "title": "Leveraging Multi-Task Learning for Multi-Label Power System Security\n  Assessment", "authors": ["Muhy Eddin Za'ter", "Amir Sajad", "Bri-Mathias Hodge"], "published": "2025-05-09T17:36:59Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.06207v1", "categories": ["eess.SY", "cs.LG", "cs.SY"], "abstract": "This paper introduces a novel approach to the power system security\nassessment using Multi-Task Learning (MTL), and reformulating the problem as a\nmulti-label classification task. The proposed MTL framework simultaneously\nassesses static, voltage, transient, and small-signal stability, improving both\naccuracy and interpretability with respect to the most state of the art machine\nlearning methods. It consists of a shared encoder and multiple decoders,\nenabling knowledge transfer between stability tasks. Experiments on the IEEE\n68-bus system demonstrate a measurable superior performance of the proposed\nmethod compared to the extant state-of-the-art approaches."}, {"arxiv_id": "2505.06203", "title": "Auto Tensor Singular Value Thresholding: A Non-Iterative and Rank-Free\n  Framework for Tensor Denoising", "authors": ["Hiroki Hasegawa", "Yukihiko Okada"], "published": "2025-05-09T17:30:16Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.06203v1", "categories": ["cs.LG"], "abstract": "In modern data-driven tasks such as classification, optimization, and\nforecasting, mitigating the effects of intrinsic noise is crucial for improving\npredictive accuracy. While numerous denoising techniques have been developed,\nthe rising dimensionality of real-world datasets limits conventional\nmatrix-based methods in preserving data structure and accuracy. This challenge\nhas led to increasing interest in tensor-based approaches, which naturally\ncapture multi-way data relationships. However, classical tensor decomposition\nmethods (e.g., HOSVD, HOOI) typically require pre-specified ranks and iterative\noptimization, making them computationally expensive and less practical. In this\nwork, we propose a novel low-rank approximation method for tensor data that\navoids these limitations. Our approach applies statistically grounded singular\nvalue thresholding to mode-wise matricizations, enabling automatic extraction\nof significant components without requiring prior rank specification or\niterative refinement. Experiments on synthetic and real-world tensors show that\nour method consistently outperforms existing techniques in terms of estimation\naccuracy and computational efficiency, especially in noisy high-dimensional\nsettings."}, {"arxiv_id": "2505.06201", "title": "Decoding Algorithms for Two-dimensional Constacyclic Codes over\n  $\\mathbb{F}_q$", "authors": ["Vidya Sagar", "Shikha Patel", "Shayan Srinivasa Garani"], "published": "2025-05-09T17:28:51Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.06201v1", "categories": ["cs.IT", "math.IT"], "abstract": "We derive the spectral domain properties of two-dimensional (2-D)\n$(\\lambda_1, \\lambda_2)$-constacyclic codes over $\\mathbb{F}_q$ using the 2-D\nfinite field Fourier transform (FFFT). Based on the spectral nulls of 2-D\n$(\\lambda_1, \\lambda_2)$-constacyclic codes, we characterize the structure of\n2-D constacyclic coded arrays. The proposed 2-D construction has flexible code\nrates and works for any code areas, be it odd or even area. We present an\nalgorithm to detect the location of 2-D errors. Further, we also propose\ndecoding algorithms for extracting the error values using both time and\nfrequency domain properties by exploiting the sparsity that arises due to\nduality in the time and frequency domains. Through several illustrative\nexamples, we demonstrate the working of the proposed decoding algorithms."}, {"arxiv_id": "2505.07817", "title": "Pixel Motion as Universal Representation for Robot Control", "authors": ["Kanchana Ranasinghe", "Xiang Li", "Cristina Mata", "Jongwoo Park", "Michael S Ryoo"], "published": "2025-05-12T17:59:32Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.07817v1", "categories": ["cs.RO", "cs.CV"], "abstract": "We present LangToMo, a vision-language-action framework structured as a\ndual-system architecture that uses pixel motion forecasts as intermediate\nrepresentations. Our high-level System 2, an image diffusion model, generates\ntext-conditioned pixel motion sequences from a single frame to guide robot\ncontrol. Pixel motion-a universal, interpretable, and motion-centric\nrepresentation-can be extracted from videos in a self-supervised manner,\nenabling diffusion model training on web-scale video-caption data. Treating\ngenerated pixel motion as learned universal representations, our low level\nSystem 1 module translates these into robot actions via motion-to-action\nmapping functions, which can be either hand-crafted or learned with minimal\nsupervision. System 2 operates as a high-level policy applied at sparse\ntemporal intervals, while System 1 acts as a low-level policy at dense temporal\nintervals. This hierarchical decoupling enables flexible, scalable, and\ngeneralizable robot control under both unsupervised and supervised settings,\nbridging the gap between language, motion, and action. Checkout\nhttps://kahnchana.github.io/LangToMo for visualizations."}, {"arxiv_id": "2505.07809", "title": "A Comparative Analysis of Static Word Embeddings for Hungarian", "authors": ["Máté Gedeon"], "published": "2025-05-12T17:57:11Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.07809v1", "categories": ["cs.CL", "cs.AI"], "abstract": "This paper presents a comprehensive analysis of various static word\nembeddings for Hungarian, including traditional models such as Word2Vec,\nFastText, as well as static embeddings derived from BERT-based models using\ndifferent extraction methods. We evaluate these embeddings on both intrinsic\nand extrinsic tasks to provide a holistic view of their performance. For\nintrinsic evaluation, we employ a word analogy task, which assesses the\nembeddings ability to capture semantic and syntactic relationships. Our results\nindicate that traditional static embeddings, particularly FastText, excel in\nthis task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among\nthe BERT-based models, the X2Static method for extracting static embeddings\ndemonstrates superior performance compared to decontextualized and aggregate\nmethods, approaching the effectiveness of traditional static embeddings. For\nextrinsic evaluation, we utilize a bidirectional LSTM model to perform Named\nEntity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results\nreveal that embeddings derived from dynamic models, especially those extracted\nusing the X2Static method, outperform purely static embeddings. Notably, ELMo\nembeddings achieve the highest accuracy in both NER and POS tagging tasks,\nunderscoring the benefits of contextualized representations even when used in a\nstatic form. Our findings highlight the continued relevance of static word\nembeddings in NLP applications and the potential of advanced extraction methods\nto enhance the utility of BERT-based models. This piece of research contributes\nto the understanding of embedding performance in the Hungarian language and\nprovides valuable insights for future developments in the field. The training\nscripts, evaluation codes, restricted vocabulary, and extracted embeddings will\nbe made publicly available to support further research and reproducibility."}, {"arxiv_id": "2505.07793", "title": "Overflow Prevention Enhances Long-Context Recurrent LLMs", "authors": ["Assaf Ben-Kish", "Itamar Zimerman", "M. Jehanzeb Mirza", "James Glass", "Leonid Karlinsky", "Raja Giryes"], "published": "2025-05-12T17:45:05Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.07793v1", "categories": ["cs.LG", "cs.AI"], "abstract": "A recent trend in LLMs is developing recurrent sub-quadratic models that\nimprove long-context processing efficiency. We investigate leading large\nlong-context models, focusing on how their fixed-size recurrent memory affects\ntheir performance. Our experiments reveal that, even when these models are\ntrained for extended contexts, their use of long contexts remains\nunderutilized. Specifically, we demonstrate that a chunk-based inference\nprocedure, which identifies and processes only the most relevant portion of the\ninput can mitigate recurrent memory failures and be effective for many\nlong-context tasks: On LongBench, our method improves the overall performance\nof Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%,\nRecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this\nsimple approach also leads to state-of-the-art results in the challenging\nLongBench v2 benchmark, showing competitive performance with equivalent size\nTransformers. Furthermore, our findings raise questions about whether recurrent\nmodels genuinely exploit long-range dependencies, as our single-chunk strategy\ndelivers stronger performance - even in tasks that presumably require\ncross-context relations."}, {"arxiv_id": "2505.07784", "title": "Domain Regeneration: How well do LLMs match syntactic properties of text\n  domains?", "authors": ["Da Ju", "Hagen Blix", "Adina Williams"], "published": "2025-05-12T17:37:17Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.07784v1", "categories": ["cs.CL"], "abstract": "Recent improvement in large language model performance have, in all\nlikelihood, been accompanied by improvement in how well they can approximate\nthe distribution of their training data. In this work, we explore the following\nquestion: which properties of text domains do LLMs faithfully approximate, and\nhow well do they do so? Applying observational approaches familiar from corpus\nlinguistics, we prompt a commonly used, opensource LLM to regenerate text from\ntwo domains of permissively licensed English text which are often contained in\nLLM training data -- Wikipedia and news text. This regeneration paradigm allows\nus to investigate whether LLMs can faithfully match the original human text\ndomains in a fairly semantically-controlled setting. We investigate varying\nlevels of syntactic abstraction, from more simple properties like sentence\nlength, and article readability, to more complex and higher order properties\nsuch as dependency tag distribution, parse depth, and parse complexity. We find\nthat the majority of the regenerated distributions show a shifted mean, a lower\nstandard deviation, and a reduction of the long tail, as compared to the human\noriginals."}, {"arxiv_id": "2505.07783", "title": "Relative Overfitting and Accept-Reject Framework", "authors": ["Yanxin Liu", "Yunqi Zhang"], "published": "2025-05-12T17:36:14Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.07783v1", "categories": ["cs.LG"], "abstract": "Currently, the scaling law of Large Language Models (LLMs) faces challenges\nand bottlenecks. This paper posits that noise effects, stemming from changes in\nthe signal-to-noise ratio under diminishing marginal returns, are the root\ncause of these issues. To control this noise, we investigated the differences\nbetween models with performance advantages and disadvantages, introducing the\nconcept of \"relative overfitting.\" Based on their complementary strengths, we\nhave proposed an application framework, Accept-Reject (AR). In Natural Language\nProcessing (NLP), we use LLMs and Small Language Models (SLMs) as the medium\nfor discussion. This framework enables SLMs to exert a universal positive\ninfluence on LLM decision outputs, rather than the intuitively expected\nnegative influence. We validated our approach using self-built models based on\nmainstream architectures and pre-trained mainstream models across multiple\ndatasets, including basic language modeling, long-context tasks, subject\nexamination, and question-answering (QA) benchmarks. The results demonstrate\nthat through our structure, compared to increasing the LLM's parameters, we can\nachieve better performance improvements with significantly lower parameter and\ncomputational costs in many scenarios. These improvements are universal,\nstable, and effective. Furthermore, we explore the potential of \"relative\noverfitting\" and the AR framework in other machine learning domains, such as\ncomputer vision (CV) and AI for science. We hope the proposed approach can help\nscale laws overcome existing bottlenecks."}, {"arxiv_id": "2505.08787", "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill\n  Representations", "authors": ["Hanjung Kim", "Jaehyun Kang", "Hyolim Kang", "Meedeum Cho", "Seon Joo Kim", "Youngwoon Lee"], "published": "2025-05-13T17:59:22Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.08787v1", "categories": ["cs.RO", "cs.CV"], "abstract": "Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill."}, {"arxiv_id": "2505.08783", "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation", "authors": ["Shanda Li", "Tanya Marwah", "Junhong Shen", "Weiwei Sun", "Andrej Risteski", "Yiming Yang", "Ameet Talwalkar"], "published": "2025-05-13T17:58:08Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.08783v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "abstract": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE."}, {"arxiv_id": "2505.08772", "title": "Blockchain Technology: Core Mechanisms, Evolution, and Future\n  Implementation Challenges", "authors": ["Aditya Pratap Singh"], "published": "2025-05-13T17:50:31Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.08772v1", "categories": ["cs.CR", "cs.ET", "68M10, 94A60", "C.2.4; D.4.4; K.4.4"], "abstract": "Blockchain technology has emerged as one of the most transformative digital\ninnovations of the 21st century. This paper presents a comprehensive review of\nblockchain's fundamental architecture, tracing its development from Bitcoin's\ninitial implementation to current enterprise applications. We examine the core\ntechnical components including distributed consensus algorithms, cryptographic\nprinciples, and smart contract functionality that enable blockchain's unique\nproperties. The historical progression from cryptocurrency-focused systems to\nrobust platforms for decentralized applications is analyzed, highlighting\npivotal developments in scalability, privacy, and interoperability.\nAdditionally, we identify critical challenges facing widespread blockchain\nadoption, including technical limitations, regulatory hurdles, and integration\ncomplexities with existing systems. By providing this foundational\nunderstanding of blockchain technology, this paper contributes to ongoing\nresearch efforts addressing blockchain's potential to revolutionize data\nmanagement across industries."}, {"arxiv_id": "2505.08771", "title": "Kudzu: Fast and Simple High-Throughput BFT", "authors": ["Victor Shoup", "Jakub Sliwinski", "Yann Vonlanthen"], "published": "2025-05-13T17:50:05Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.08771v1", "categories": ["cs.DC", "cs.CR"], "abstract": "We present Kudzu, a high-throughput atomic broadcast protocol with an\nintegrated fast path. Our contribution is based on the combination of two lines\nof work. Firstly, our protocol achieves finality in just two rounds of\ncommunication if all but $p$ out of $n = 3f + 2p + 1$ participating replicas\nbehave correctly, where $f$ is the number of Byzantine faults that are\ntolerated. Due to the seamless integration of the fast path, even in the\npresence of more than $p$ faults, our protocol maintains state-of-the-art\ncharacteristics. Secondly, our protocol utilizes the bandwidth of participating\nreplicas in a balanced way, alleviating the bottleneck at the leader, and thus\nenabling high throughput. This is achieved by disseminating blocks using\nerasure codes. Despite combining a novel set of advantages, Kudzu is remarkably\nsimple: intricacies such as progress certificates, complex view changes, and\nspeculative execution are avoided."}, {"arxiv_id": "2505.08768", "title": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models", "authors": ["Suhan Guo", "Jiahong Deng", "Mengjun Yi", "Furao Shen", "Jian Zhao"], "published": "2025-05-13T17:39:31Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.08768v1", "categories": ["cs.LG"], "abstract": "Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042."}, {"arxiv_id": "2505.09610", "title": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors", "authors": ["Nicolas Dupuis", "Ravi Nair", "Shyam Ramji", "Sean McClintock", "Nishant Chauhan", "Priyanka Nagpal", "Bart Blaner", "Ken Valk", "Leon Stok", "Ruchir Puri"], "published": "2025-05-14T17:58:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.09610v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "abstract": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world."}, {"arxiv_id": "2505.09602", "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs", "authors": ["David Khachaturov", "Robert Mullins"], "published": "2025-05-14T17:52:10Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.09602v1", "categories": ["cs.LG", "cs.CR"], "abstract": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios."}, {"arxiv_id": "2505.09598", "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference", "authors": ["Nidhal Jegham", "Marwen Abdelatti", "Lassad Elmoubarki", "Abdeltawab Hendawi"], "published": "2025-05-14T17:47:00Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.09598v1", "categories": ["cs.CY", "cs.AI"], "abstract": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards."}, {"arxiv_id": "2505.09595", "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models", "authors": ["Abdullah Mushtaq", "Imran Taj", "Rafay Naeem", "Ibrahim Ghaznavi", "Junaid Qadir"], "published": "2025-05-14T17:43:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.09595v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "abstract": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems."}, {"arxiv_id": "2505.09586", "title": "Rhomboid Tiling for Geometric Graph Deep Learning", "authors": ["Yipeng Zhang", "Longlong Li", "Kelin Xia"], "published": "2025-05-14T17:37:15Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.09586v1", "categories": ["cs.LG"], "abstract": "Graph Neural Networks (GNNs) have proven effective for learning from\ngraph-structured data through their neighborhood-based message passing\nframework. Many hierarchical graph clustering pooling methods modify this\nframework by introducing clustering-based strategies, enabling the construction\nof more expressive and powerful models. However, all of these message passing\nframework heavily rely on the connectivity structure of graphs, limiting their\nability to capture the rich geometric features inherent in geometric graphs. To\naddress this, we propose Rhomboid Tiling (RT) clustering, a novel clustering\nmethod based on the rhomboid tiling structure, which performs clustering by\nleveraging the complex geometric information of the data and effectively\nextracts its higher-order geometric structures. Moreover, we design RTPool, a\nhierarchical graph clustering pooling model based on RT clustering for graph\nclassification tasks. The proposed model demonstrates superior performance,\noutperforming 21 state-of-the-art competitors on all the 7 benchmark datasets."}, {"arxiv_id": "2505.10559", "title": "Neural Thermodynamic Laws for Large Language Model Training", "authors": ["Ziming Liu", "Yizhou Liu", "Jeff Gore", "Max Tegmark"], "published": "2025-05-15T17:59:22Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.10559v1", "categories": ["cs.LG", "cs.AI", "physics.data-an", "stat.ML"], "abstract": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules."}, {"arxiv_id": "2505.10556", "title": "An AI-driven framework for the prediction of personalised health\n  response to air pollution", "authors": ["Nazanin Zounemat Kermani", "Sadjad Naderi", "Claire H. Dilliway", "Claire E. Heaney", "Shrreya Behll", "Boyang Chen", "Hisham Abubakar-Waziri", "Alexandra E. Porter", "Marc Chadeau-Hyam", "Fangxin Fang", "Ian M. Adcock", "Kian Fan Chung", "Christopher C. Pain"], "published": "2025-05-15T17:59:07Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.10556v1", "categories": ["cs.LG", "physics.ao-ph"], "abstract": "Air pollution poses a significant threat to public health, causing or\nexacerbating many respiratory and cardiovascular diseases. In addition, climate\nchange is bringing about more extreme weather events such as wildfires and\nheatwaves, which can increase levels of pollution and worsen the effects of\npollution exposure. Recent advances in personal sensing have transformed the\ncollection of behavioural and physiological data, leading to the potential for\nnew improvements in healthcare. We wish to capitalise on this data, alongside\nnew capabilities in AI for making time series predictions, in order to monitor\nand predict health outcomes for an individual. Thus, we present a novel\nworkflow for predicting personalised health responses to pollution by\nintegrating physiological data from wearable fitness devices with real-time\nenvironmental exposures. The data is collected from various sources in a secure\nand ethical manner, and is used to train an AI model to predict individual\nhealth responses to pollution exposure within a cloud-based, modular framework.\nWe demonstrate that the AI model -- an Adversarial Autoencoder neural network\nin this case -- accurately reconstructs time-dependent health signals and\ncaptures nonlinear responses to pollution. Transfer learning is applied using\ndata from a personal smartwatch, which increases the generalisation abilities\nof the AI model and illustrates the adaptability of the approach to real-world,\nuser-generated data."}, {"arxiv_id": "2505.10551", "title": "Does Feasibility Matter? Understanding the Impact of Feasibility on\n  Synthetic Training Data", "authors": ["Yiwen Liu", "Jessica Bader", "Jae Myung Kim"], "published": "2025-05-15T17:57:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.10551v1", "categories": ["cs.CV", "cs.AI"], "abstract": "With the development of photorealistic diffusion models, models trained in\npart or fully on synthetic data achieve progressively better results. However,\ndiffusion models still routinely generate images that would not exist in\nreality, such as a dog floating above the ground or with unrealistic texture\nartifacts. We define the concept of feasibility as whether attributes in a\nsynthetic image could realistically exist in the real-world domain; synthetic\nimages containing attributes that violate this criterion are considered\ninfeasible. Intuitively, infeasible images are typically considered\nout-of-distribution; thus, training on such images is expected to hinder a\nmodel's ability to generalize to real-world data, and they should therefore be\nexcluded from the training set whenever possible. However, does feasibility\nreally matter? In this paper, we investigate whether enforcing feasibility is\nnecessary when generating synthetic training data for CLIP-based classifiers,\nfocusing on three target attributes: background, color, and texture. We\nintroduce VariReal, a pipeline that minimally edits a given source image to\ninclude feasible or infeasible attributes given by the textual prompt generated\nby a large language model. Our experiments show that feasibility minimally\naffects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference\nin top-1 accuracy across three fine-grained datasets. Also, the attribute\nmatters on whether the feasible/infeasible images adversarially influence the\nclassification performance. Finally, mixing feasible and infeasible images in\ntraining datasets does not significantly impact performance compared to using\npurely feasible or infeasible datasets."}, {"arxiv_id": "2505.10538", "title": "S3C2 Summit 2024-09: Industry Secure Software Supply Chain Summit", "authors": ["Imranur Rahman", "Yasemin Acar", "Michel Cukier", "William Enck", "Christian Kastner", "Alexandros Kapravelos", "Dominik Wermke", "Laurie Williams"], "published": "2025-05-15T17:48:14Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.10538v1", "categories": ["cs.CR"], "abstract": "While providing economic and software development value, software supply\nchains are only as strong as their weakest link. Over the past several years,\nthere has been an exponential increase in cyberattacks, specifically targeting\nvulnerable links in critical software supply chains. These attacks disrupt the\nday-to-day functioning and threaten the security of nearly everyone on the\ninternet, from billion-dollar companies and government agencies to hobbyist\nopen-source developers. The ever-evolving threat of software supply chain\nattacks has garnered interest from the software industry and the US government\nin improving software supply chain security.\n  On September 20, 2024, three researchers from the NSF-backed Secure Software\nSupply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with\na diverse set of 12 practitioners from 9 companies. The goals of the Summit\nwere to: (1) to enable sharing between individuals from different companies\nregarding practical experiences and challenges with software supply chain\nsecurity, (2) to help form new collaborations, (3) to share our observations\nfrom our previous summits with industry, and (4) to learn about practitioners'\nchallenges to inform our future research direction. The summit consisted of\ndiscussions of six topics relevant to the companies represented, including\nupdating vulnerable dependencies, component and container choice, malicious\ncommits, building infrastructure, large language models, and reducing entire\nclasses of vulnerabilities."}, {"arxiv_id": "2505.10537", "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps", "authors": ["Filippo Olimpieri", "Noemi Giustini", "Andrea Lacava", "Salvatore D'Oro", "Tommaso Melodia", "Francesca Cuomo"], "published": "2025-05-15T17:47:30Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.10537v1", "categories": ["cs.NI", "cs.AI"], "abstract": "The O-RAN architecture is transforming cellular networks by adopting RAN\nsoftwarization and disaggregation concepts to enable data-driven monitoring and\ncontrol of the network. Such management is enabled by RICs, which facilitate\nnear-real-time and non-real-time network control through xApps and rApps.\nHowever, they face limitations, including latency overhead in data exchange\nbetween the RAN and RIC, restricting real-time monitoring, and the inability to\naccess user plain data due to privacy and security constraints, hindering use\ncases like beamforming and spectrum classification. In this paper, we leverage\nthe dApps concept to enable real-time RF spectrum classification with LibIQ, a\nnovel library for RF signals that facilitates efficient spectrum monitoring and\nsignal classification by providing functionalities to read I/Q samples as\ntime-series, create datasets and visualize time-series data through plots and\nspectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to\ndetect external RF signals, which are subsequently classified using a CNN\ninside the library. To achieve accurate spectrum analysis, we created an\nextensive dataset of time-series-based I/Q samples, representing distinct\nsignal types captured using a custom dApp running on a 5G deployment over the\nColosseum network emulator and an OTA testbed. We evaluate our model by\ndeploying LibIQ in heterogeneous scenarios with varying center frequencies,\ntime windows, and external RF signals. In real-time analysis, the model\nclassifies the processed I/Q samples, achieving an average accuracy of\napproximately 97.8\\% in identifying signal types across all scenarios. We\npledge to release both LibIQ and the dataset created as a publicly available\nframework upon acceptance."}, {"arxiv_id": "2505.11484", "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "published": "2025-05-16T17:47:50Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.11484v1", "categories": ["cs.CL"], "abstract": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT."}, {"arxiv_id": "2505.11480", "title": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning", "authors": ["Anjiang Wei", "Tarun Suresh", "Huanmi Tan", "Yinglun Xu", "Gagandeep Singh", "Ke Wang", "Alex Aiken"], "published": "2025-05-16T17:40:45Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.11480v1", "categories": ["cs.CL", "cs.AI", "cs.PF", "cs.PL", "cs.SE"], "abstract": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance."}, {"arxiv_id": "2505.11475", "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "published": "2025-05-16T17:31:19Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.11475v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference"}, {"arxiv_id": "2505.11467", "title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views", "authors": ["Abhishek Kashyap", "Henrik Andreasson", "Todor Stoyanov"], "published": "2025-05-16T17:23:09Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.11467v1", "categories": ["cs.RO", "cs.CV"], "abstract": "Vision based robot manipulation uses cameras to capture one or more images of\na scene containing the objects to be manipulated. Taking multiple images can\nhelp if any object is occluded from one viewpoint but more visible from another\nviewpoint. However, the camera has to be moved to a sequence of suitable\npositions for capturing multiple images, which requires time and may not always\nbe possible, due to reachability constraints. So while additional images can\nproduce more accurate grasp poses due to the extra information available, the\ntime-cost goes up with the number of additional views sampled. Scene\nrepresentations like Gaussian Splatting are capable of rendering accurate\nphotorealistic virtual images from user-specified novel viewpoints. In this\nwork, we show initial results which indicate that novel view synthesis can\nprovide additional context in generating grasp poses. Our experiments on the\nGraspnet-1billion dataset show that novel views contributed force-closure\ngrasps in addition to the force-closure grasps obtained from sparsely sampled\nreal views while also improving grasp coverage. In the future we hope this work\ncan be extended to improve grasp extraction from radiance fields constructed\nwith a single input image, using for example diffusion models or generalizable\nradiance fields."}, {"arxiv_id": "2505.11462", "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "published": "2025-05-16T17:16:27Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.11462v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios."}, {"arxiv_id": "2505.13446", "title": "Unlocking Non-Invasive Brain-to-Text", "authors": ["Dulhan Jayalath", "Gilad Landau", "Oiwi Parker Jones"], "published": "2025-05-19T17:59:35Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.13446v1", "categories": ["cs.LG"], "abstract": "Despite major advances in surgical brain-to-text (B2T), i.e. transcribing\nspeech from invasive brain recordings, non-invasive alternatives have yet to\nsurpass even chance on standard metrics. This remains a barrier to building a\nnon-invasive brain-computer interface (BCI) capable of restoring communication\nin paralysed individuals without surgery. Here, we present the first\nnon-invasive B2T result that significantly exceeds these critical baselines,\nraising BLEU by $1.4\\mathrm{-}2.6\\times$ over prior work. This result is driven\nby three contributions: (1) we extend recent word-classification models with\nLLM-based rescoring, transforming single-word predictors into closed-vocabulary\nB2T systems; (2) we introduce a predictive in-filling approach to handle\nout-of-vocabulary (OOV) words, substantially expanding the effective\nvocabulary; and (3) we demonstrate, for the first time, how to scale\nnon-invasive B2T models across datasets, unlocking deep learning at scale and\nimproving accuracy by $2.1\\mathrm{-}2.3\\times$. Through these contributions, we\noffer new insights into the roles of data quality and vocabulary size.\nTogether, our results remove a major obstacle to realising practical\nnon-invasive B2T systems."}, {"arxiv_id": "2505.13445", "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published": "2025-05-19T17:59:31Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.13445v1", "categories": ["cs.AI", "cs.CL"], "abstract": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners."}, {"arxiv_id": "2505.13438", "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "published": "2025-05-19T17:58:44Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.13438v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency."}, {"arxiv_id": "2505.13431", "title": "A Practical Guide for Incorporating Symmetry in Diffusion Policy", "authors": ["Dian Wang", "Boce Hu", "Shuran Song", "Robin Walters", "Robert Platt"], "published": "2025-05-19T17:55:28Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.13431v1", "categories": ["cs.RO"], "abstract": "Recently, equivariant neural networks for policy learning have shown\npromising improvements in sample efficiency and generalization, however, their\nwide adoption faces substantial barriers due to implementation complexity.\nEquivariant architectures typically require specialized mathematical\nformulations and custom network design, posing significant challenges when\nintegrating with modern policy frameworks like diffusion-based models. In this\npaper, we explore a number of straightforward and practical approaches to\nincorporate symmetry benefits into diffusion policies without the overhead of\nfull equivariant designs. Specifically, we investigate (i) invariant\nrepresentations via relative trajectory actions and eye-in-hand perception,\n(ii) integrating equivariant vision encoders, and (iii) symmetric feature\nextraction with pretrained encoders using Frame Averaging. We first prove that\ncombining eye-in-hand perception with relative or delta action parameterization\nyields inherent SE(3)-invariance, thus improving policy generalization. We then\nperform a systematic experimental study on those design choices for integrating\nsymmetry in diffusion policies, and conclude that an invariant representation\nwith equivariant feature extraction significantly improves the policy\nperformance. Our method achieves performance on par with or exceeding fully\nequivariant architectures while greatly simplifying implementation."}, {"arxiv_id": "2505.13430", "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "published": "2025-05-19T17:55:15Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.13430v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}, {"arxiv_id": "2505.14684", "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "published": "2025-05-20T17:59:31Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.14684v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits."}, {"arxiv_id": "2505.14679", "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in\n  Large Language Models", "authors": ["Xiaojie Gu", "Guangxu Chen", "Jungang Li", "Jia-Chen Gu", "Xuming Hu", "Kai Zhang"], "published": "2025-05-20T17:59:04Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.14679v1", "categories": ["cs.CL"], "abstract": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit."}, {"arxiv_id": "2505.14677", "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with\n  Reinforcement Learning", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "published": "2025-05-20T17:58:35Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.14677v1", "categories": ["cs.CV"], "abstract": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks."}, {"arxiv_id": "2505.14673", "title": "Training-Free Watermarking for Autoregressive Image Generation", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "published": "2025-05-20T17:58:02Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.14673v1", "categories": ["cs.CV", "cs.AI", "cs.CR"], "abstract": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression."}, {"arxiv_id": "2505.14669", "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models", "authors": ["Roberto L. Castro", "Andrei Panferov", "Soroush Tabesh", "Oliver Sieberling", "Jiale Chen", "Mahdi Nikdan", "Saleh Ashkboos", "Dan Alistarh"], "published": "2025-05-20T17:55:50Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.14669v1", "categories": ["cs.LG"], "abstract": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet."}, {"arxiv_id": "2505.15817", "title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning", "authors": ["Tong Zheng", "Lichang Chen", "Simeng Han", "R. Thomas McCoy", "Heng Huang"], "published": "2025-05-21T17:59:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.15817v1", "categories": ["cs.CL"], "abstract": "Human beings naturally utilize multiple reasoning modalities to learn and\nsolve logical problems, i.e., different representational formats such as\nnatural language, code, and symbolic logic. In contrast, most existing\nLLM-based approaches operate with a single reasoning modality during training,\ntypically natural language. Although some methods explored modality selection\nor augmentation at inference time, the training process remains modality-blind,\nlimiting synergy among modalities. To fill in this gap, we propose\nMixture-of-Thought (MoT), a framework that enables LLMs to reason across three\ncomplementary modalities: natural language, code, and a newly introduced\nsymbolic modality, truth-table, which systematically enumerates logical cases\nand partially mitigates key failure modes in natural language reasoning. MoT\nadopts a two-phase design: (1) self-evolving MoT training, which jointly learns\nfrom filtered, self-generated rationales across modalities; and (2) MoT\ninference, which fully leverages the synergy of three modalities to produce\nbetter predictions. Experiments on logical reasoning benchmarks including FOLIO\nand ProofWriter demonstrate that our MoT framework consistently and\nsignificantly outperforms strong LLM baselines with single-modality\nchain-of-thought approaches, achieving up to +11.7pp average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and\ninference stages; that it is particularly effective on harder logical reasoning\nproblems; and that different modalities contribute complementary strengths,\nwith truth-table reasoning helping to overcome key bottlenecks in natural\nlanguage inference."}, {"arxiv_id": "2505.15807", "title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context\n  Retrieval Augmentation", "authors": ["Patrick Kahardipraja", "Reduan Achtibat", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "published": "2025-05-21T17:59:01Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.15807v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "abstract": "Large language models are able to exploit in-context learning to access\nexternal knowledge beyond their training data through retrieval-augmentation.\nWhile promising, its inner workings remain unclear. In this work, we shed light\non the mechanism of in-context retrieval augmentation for question answering by\nviewing a prompt as a composition of informational components. We propose an\nattribution-based method to identify specialized attention heads, revealing\nin-context heads that comprehend instructions and retrieve relevant contextual\ninformation, and parametric heads that store entities' relational knowledge. To\nbetter understand their roles, we extract function vectors and modify their\nattention weights to show how they can influence the answer generation process.\nFinally, we leverage the gained insights to trace the sources of knowledge used\nduring inference, paving the way towards more safe and transparent language\nmodels."}, {"arxiv_id": "2505.15805", "title": "Keep Security! Benchmarking Security Policy Preservation in Large\n  Language Model Contexts Against Indirect Attacks in Question Answering", "authors": ["Hwan Chang", "Yumin Kim", "Yonghyun Jun", "Hwanhee Lee"], "published": "2025-05-21T17:58:11Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.15805v1", "categories": ["cs.CL"], "abstract": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security."}, {"arxiv_id": "2505.15804", "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "authors": ["Zongzhao Li", "Zongyang Ma", "Mingze Li", "Songyou Li", "Yu Rong", "Tingyang Xu", "Ziqi Zhang", "Deli Zhao", "Wenbing Huang"], "published": "2025-05-21T17:57:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.15804v1", "categories": ["cs.CV"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1."}, {"arxiv_id": "2505.15798", "title": "Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds\n  for Low-Shot Learning", "authors": ["Taehoon Kim", "Henry Gouk", "Minyoung Kim", "Timothy Hospedales"], "published": "2025-05-21T17:51:05Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.15798v1", "categories": ["cs.LG"], "abstract": "Certifying the IID generalisation ability of deep networks is the first of\nmany requirements for trusting AI in high-stakes applications from medicine to\nsecurity. However, when instantiating generalisation bounds for deep networks\nit remains challenging to obtain non-vacuous guarantees, especially when\napplying contemporary large models on the small scale data prevalent in such\nhigh-stakes fields. In this paper, we draw a novel connection between a family\nof learning methods based on model fusion and generalisation certificates, and\nsurprisingly show that with minor adjustment several existing learning\nstrategies already provide non-trivial generalisation guarantees. Essentially,\nby focusing on data-driven learning of downstream tasks by fusion rather than\nfine-tuning, the certified generalisation gap becomes tiny and independent of\nthe base network size, facilitating its certification. Our results show for the\nfirst time non-trivial generalisation guarantees for learning with as low as\n100 examples, while using vision models such as VIT-B and language models such\nas mistral-7B. This observation is significant as it has immediate implications\nfor facilitating the certification of existing systems as trustworthy, and\nopens up new directions for research at the intersection of practice and\ntheory."}, {"arxiv_id": "2505.17020", "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual\n  Cross-Attention Mechanisms", "authors": ["Shilin Yan", "Jiaming Han", "Joey Tsai", "Hongwei Xue", "Rongyao Fang", "Lingyi Hong", "Ziyu Guo", "Ray Zhang"], "published": "2025-05-22T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.17020v1", "categories": ["cs.CV"], "abstract": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources."}, {"arxiv_id": "2505.17017", "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "published": "2025-05-22T17:59:49Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.17017v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"}, {"arxiv_id": "2505.17013", "title": "When Are Concepts Erased From Diffusion Models?", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published": "2025-05-22T17:59:09Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.17013v1", "categories": ["cs.LG", "cs.CV"], "abstract": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models."}, {"arxiv_id": "2505.17005", "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "published": "2025-05-22T17:58:26Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.17005v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "abstract": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus."}, {"arxiv_id": "2505.16998", "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "published": "2025-05-22T17:57:23Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.16998v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval."}, {"arxiv_id": "2505.18154", "title": "The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step\n  Induction to Complex Moral Dilemmas", "authors": ["Ya Wu", "Qiang Sheng", "Danding Wang", "Guang Yang", "Yifan Sun", "Zhengjia Wang", "Yuyan Bu", "Juan Cao"], "published": "2025-05-23T17:59:50Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.18154v1", "categories": ["cs.CL", "cs.CY"], "abstract": "Ethical decision-making is a critical aspect of human judgment, and the\ngrowing use of LLMs in decision-support systems necessitates a rigorous\nevaluation of their moral reasoning capabilities. However, existing assessments\nprimarily rely on single-step evaluations, failing to capture how models adapt\nto evolving ethical challenges. Addressing this gap, we introduce the\nMulti-step Moral Dilemmas (MMDs), the first dataset specifically constructed to\nevaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.\nThis framework enables a fine-grained, dynamic analysis of how LLMs adjust\ntheir moral reasoning across escalating dilemmas. Our evaluation of nine widely\nused LLMs reveals that their value preferences shift significantly as dilemmas\nprogress, indicating that models recalibrate moral judgments based on scenario\ncomplexity. Furthermore, pairwise value comparisons demonstrate that while LLMs\noften prioritize the value of care, this value can sometimes be superseded by\nfairness in certain contexts, highlighting the dynamic and context-dependent\nnature of LLM ethical reasoning. Our findings call for a shift toward dynamic,\ncontext-aware evaluation paradigms, paving the way for more human-aligned and\nvalue-sensitive development of LLMs."}, {"arxiv_id": "2505.18152", "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry\n  Understanding in LLMs", "authors": ["Wafa Alghallabi", "Ritesh Thawkar", "Sara Ghaboura", "Ketan More", "Omkar Thawakar", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published": "2025-05-23T17:59:29Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.18152v1", "categories": ["cs.CL"], "abstract": "Arabic poetry stands as one of the most sophisticated and culturally embedded\nforms of expression in the Arabic language, known for its layered meanings,\nstylistic diversity, and deep historical continuity. Although large language\nmodels (LLMs) have demonstrated strong performance across languages and tasks,\ntheir ability to understand Arabic poetry remains largely unexplored. In this\nwork, we introduce `Fann or Flop`, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in twelve historical eras, covering 21\ncore poetic genres and a variety of metrical forms, from classical structures\nto contemporary free verse. The benchmark comprises a curated corpus of poems\nwith explanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM is in understanding\nclassical Arabic through the Arabic poetry. Unlike surface-level tasks, this\ndomain demands deeper interpretive reasoning and cultural sensitivity. Our\nevaluation of state-of-the-art LLMs shows that most models struggle with poetic\nunderstanding despite strong results on standard Arabic benchmarks. We release\n`Fann or Flop` along with the evaluation suite as an open-source resource to\nenable rigorous evaluation and advancement for Arabic language models. Code is\navailable at: https://github.com/mbzuai-oryx/FannOrFlop."}, {"arxiv_id": "2505.18148", "title": "Lost in the Haystack: Smaller Needles are More Difficult for LLMs to\n  Find", "authors": ["Owen Bianchi", "Mathew J. Koretsky", "Maya Willey", "Chelsea X. Alvarado", "Tanay Nayak", "Adi Asija", "Nicole Kuznetsov", "Mike A. Nalls", "Faraz Faghri", "Daniel Khashabi"], "published": "2025-05-23T17:57:42Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.18148v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems."}, {"arxiv_id": "2505.18135", "title": "Gaming Tool Preferences in Agentic LLMs", "authors": ["Kazem Faghih", "Wenxiao Wang", "Yize Cheng", "Siddhant Bharti", "Gaurang Sriramanan", "Sriram Balasubramanian", "Parsa Hosseini", "Soheil Feizi"], "published": "2025-05-23T17:43:48Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.18135v1", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "abstract": "Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources."}, {"arxiv_id": "2505.18133", "title": "Joint Encryption and Error Correction for Secure Quantum Communication", "authors": ["Nitin Jha", "Abhishek Parakh", "Mahadevan Subramaniam"], "published": "2025-05-23T17:42:03Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.18133v1", "categories": ["quant-ph", "cs.NI"], "abstract": "Secure quantum networks are a bedrock requirement for developing a future\nquantum internet. However, quantum channels are susceptible to channel noise\nthat introduce errors in the transmitted data. The traditional approach to\nproviding error correction typically encapsulates the message in an error\ncorrection code after encryption. Such separate processes incur overhead that\nmust be avoided when possible. We, consequently, provide a single integrated\nprocess that allows for encryption as well as error correction. This is a first\nattempt to do so for secure quantum communication and combines the\nCalderbank-Shor-Steane (CSS) code with the three-stage secure quantum\ncommunication protocol. Lastly, it allows for arbitrary qubits to be\ntransmitted from sender to receiver making the proposed protocol general\npurpose."}, {"arxiv_id": "2505.20150", "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling", "authors": ["Ilai Reshef", "Nadav Dym"], "published": "2025-05-26T15:53:09Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20150v1", "categories": ["cs.LG", "cs.AI"], "abstract": "Multiset functions, which are functions that map multisets to vectors, are a\nfundamental tool in the construction of neural networks for multisets and\ngraphs. To guarantee that the vector representation of the multiset is\nfaithful, it is often desirable to have multiset mappings that are both\ninjective and bi-Lipschitz. Currently, there are several constructions of\nmultiset functions achieving both these guarantees, leading to improved\nperformance in some tasks but often also to higher compute time than standard\nconstructions. Accordingly, it is natural to inquire whether simpler multiset\nfunctions achieving the same guarantees are available. In this paper, we make a\nlarge step towards giving a negative answer to this question. We consider the\nfamily of k-ary Janossy pooling, which includes many of the most popular\nmultiset models, and prove that no piecewise linear Janossy pooling function\ncan be injective. On the positive side, we show that when restricted to\nmultisets without multiplicities, even simple deep-sets models suffice for\ninjectivity and bi-Lipschitzness."}, {"arxiv_id": "2505.20147", "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via\n  Kinetic-Optimal Velocities", "authors": ["Jin Wang", "Yao Lai", "Aoxue Li", "Shifeng Zhang", "Jiacheng Sun", "Ning Kang", "Chengyue Wu", "Zhenguo Li", "Ping Luo"], "published": "2025-05-26T15:46:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20147v1", "categories": ["cs.CV"], "abstract": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning."}, {"arxiv_id": "2505.20146", "title": "On the Robustness of RSMA to Adversarial BD-RIS-Induced Interference", "authors": ["Arthur S. de Sena", "Jacek Kibilda", "Nurul H. Mahmood", "Andre Gomes", "Luiz A. DaSilva", "Matti Latva-aho"], "published": "2025-05-26T15:46:24Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20146v1", "categories": ["eess.SP"], "abstract": "This article investigates the robustness of rate-splitting multiple access\n(RSMA) in multi-user multiple-input multiple-output (MIMO) systems to\ninterference attacks against channel acquisition induced by beyond-diagonal\nRISs (BD-RISs). Two primary attack strategies, random and aligned interference,\nare proposed for fully connected and group-connected BD-RIS architectures.\nValid random reflection coefficients are generated exploiting the Takagi\nfactorization, while potent aligned interference attacks are achieved through\noptimization strategies based on a quadratically constrained quadratic program\n(QCQP) reformulation followed by projections onto the unitary manifold. Our\nnumerical findings reveal that, when perfect channel state information (CSI) is\navailable, RSMA behaves similarly to space-division multiple access (SDMA) and\nthus is highly susceptible to the attack, with BD-RIS inducing severe\nperformance loss and significantly outperforming diagonal RIS. However, under\nimperfect CSI, RSMA consistently demonstrates significantly greater robustness\nthan SDMA, particularly as the system's transmit power increases."}, {"arxiv_id": "2505.20142", "title": "Model Stitching by Functional Latent Alignment", "authors": ["Ioannis Athanasiadis", "Anmar Karmush", "Michael Felsberg"], "published": "2025-05-26T15:44:26Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20142v1", "categories": ["cs.LG"], "abstract": "Evaluating functional similarity involves quantifying the degree to which\nindependently trained neural networks learn functionally similar\nrepresentations. Reliably inferring the functional similarity of these networks\nremains an open problem with far-reaching implications for AI. Model stitching\nhas emerged as a promising paradigm, where an optimal affine transformation\naligns two models to solve a task, with the stitched model serving as a proxy\nfor functional similarity. In this work, we draw inspiration from the knowledge\ndistillation literature and propose Functional Latent Alignment (FuLA) as a\nnovel optimality condition for model stitching. We revisit previously explored\nfunctional similarity testbeds and introduce a new one, based on which FuLA\nemerges as an overall more reliable method of functional similarity.\nSpecifically, our experiments in (a) adversarial training, (b) shortcut\ntraining and, (c) cross-layer stitching, reveal that FuLA is less prone to\nartifacts tied to training on task cues while achieving non-trivial alignments\nthat are missed by stitch-level matching."}, {"arxiv_id": "2505.20139", "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "published": "2025-05-26T15:40:42Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20139v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "abstract": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures."}, {"arxiv_id": "2505.20156", "title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for\n  Multiple Characters", "authors": ["Yi Chen", "Sen Liang", "Zixiang Zhou", "Ziyao Huang", "Yifeng Ma", "Junshu Tang", "Qin Lin", "Yuan Zhou", "Qinglin Lu"], "published": "2025-05-26T15:57:27Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20156v1", "categories": ["cs.CV"], "abstract": "Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios."}, {"arxiv_id": "2505.20155", "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs", "authors": ["Hanting Chen", "Jiarui Qin", "Jialong Guo", "Tao Yuan", "Yichun Yin", "Huiling Zhen", "Yasheng Wang", "Jinpeng Li", "Xiaojun Meng", "Meng Zhang", "Rongju Ruan", "Zheyuan Bai", "Yehui Tang", "Can Chen", "Xinghao Chen", "Fisher Yu", "Ruiming Tang", "Yunhe Wang"], "published": "2025-05-26T15:57:08Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20155v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s."}, {"arxiv_id": "2505.20154", "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in\n  Parameter-Efficient Fine-Tuning of Large Models", "authors": ["Xueyan Zhang", "Jinman Zhao", "Zhifei Yang", "Yibo Zhong", "Shuhao Guan", "Linbo Cao", "Yining Wang"], "published": "2025-05-26T15:56:40Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.20154v1", "categories": ["cs.CL"], "abstract": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs."}, {"arxiv_id": "2505.21505", "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective", "authors": ["Shimao Zhang", "Zhejian Lai", "Xiang Liu", "Shuaijie She", "Xiao Liu", "Yeyun Gong", "Shujian Huang", "Jiajun Chen"], "published": "2025-05-27T17:59:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.21505v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs."}, {"arxiv_id": "2505.21503", "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs\n  via Catfish Agent for Clinical Decision Making", "authors": ["Yihan Wang", "Qiao Yan", "Zhenghao Xing", "Lihao Liu", "Junjun He", "Chi-Wing Fu", "Xiaowei Hu", "Pheng-Ann Heng"], "published": "2025-05-27T17:59:50Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.21503v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.OT"], "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1."}, {"arxiv_id": "2505.21499", "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery", "authors": ["Haowei Wang", "Junjie Wang", "Xiaojun Jia", "Rupeng Zhang", "Mingyang Li", "Zhe Liu", "Yang Liu", "Qing Wang"], "published": "2025-05-27T17:59:05Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.21499v1", "categories": ["cs.CR", "cs.AI"], "abstract": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject."}, {"arxiv_id": "2505.21494", "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment", "authors": ["Xiaojun Jia", "Sensen Gao", "Simeng Qin", "Tianyu Pang", "Chao Du", "Yihao Huang", "Xinfeng Li", "Yiming Li", "Bo Li", "Yang Liu"], "published": "2025-05-27T17:56:57Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.21494v1", "categories": ["cs.CV"], "abstract": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."}, {"arxiv_id": "2505.21493", "title": "Reinforcing General Reasoning without Verifiers", "authors": ["Xiangxin Zhou", "Zichen Liu", "Anya Sims", "Haonan Wang", "Tianyu Pang", "Chongxuan Li", "Liang Wang", "Min Lin", "Chao Du"], "published": "2025-05-27T17:56:27Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.21493v1", "categories": ["cs.LG", "cs.CL"], "abstract": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree."}, {"arxiv_id": "2505.22664", "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates", "authors": ["Kaiyu Yue", "Vasu Singla", "Menglin Jia", "John Kirchenbauer", "Rifaa Qadri", "Zikui Cai", "Abhinav Bhatele", "Furong Huang", "Tom Goldstein"], "published": "2025-05-28T17:59:59Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.22664v1", "categories": ["cs.CV"], "abstract": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder."}, {"arxiv_id": "2505.22663", "title": "Training Free Stylized Abstraction", "authors": ["Aimon Rahman", "Kartik Narayan", "Vishal M. Patel"], "published": "2025-05-28T17:59:57Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.22663v1", "categories": ["cs.CV"], "abstract": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup."}, {"arxiv_id": "2505.22662", "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "published": "2025-05-28T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.22662v1", "categories": ["cs.CL", "cs.LG"], "abstract": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."}, {"arxiv_id": "2505.22661", "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating\n  LLMs in Domain-Specific Knowledge and Reasoning", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "published": "2025-05-28T17:59:43Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.22661v1", "categories": ["cs.CL"], "abstract": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."}, {"arxiv_id": "2505.22657", "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "published": "2025-05-28T17:59:13Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.22657v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}, {"arxiv_id": "2505.23765", "title": "From Chat Logs to Collective Insights: Aggregative Question Answering", "authors": ["Wentao Zhang", "Woojeong Kim", "Yuntian Deng"], "published": "2025-05-29T17:59:55Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.23765v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data."}, {"arxiv_id": "2505.23754", "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning", "authors": ["Ziyin Zhang", "Jiahao Xu", "Zhiwei He", "Tian Liang", "Qiuzhi Liu", "Yansi Li", "Linfeng Song", "Zhengwen Liang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "published": "2025-05-29T17:59:39Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.23754v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration."}, {"arxiv_id": "2505.23752", "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks", "authors": ["Akashah Shabbir", "Muhammad Akhtar Munir", "Akshay Dudhane", "Muhammad Umer Sheikh", "Muhammad Haris Khan", "Paolo Fraccaro", "Juan Bernabe Moreno", "Fahad Shahbaz Khan", "Salman Khan"], "published": "2025-05-29T17:59:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.23752v1", "categories": ["cs.CV"], "abstract": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable"}, {"arxiv_id": "2505.23747", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published": "2025-05-29T17:59:04Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.23747v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2"], "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/."}, {"arxiv_id": "2505.23744", "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters\n  is All You Need", "authors": ["Qiang Wang", "Xiang Song", "Yuhang He", "Jizhou Han", "Chenhao Ding", "Xinyuan Gao", "Yihong Gong"], "published": "2025-05-29T17:58:57Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.23744v1", "categories": ["cs.CV", "cs.AI"], "abstract": "Deep neural networks (DNNs) often underperform in real-world, dynamic\nsettings where data distributions change over time. Domain Incremental Learning\n(DIL) offers a solution by enabling continual model adaptation, with\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\nselection accuracy, especially as the number of domains and corresponding\nclasses grows. To address this, we propose SOYO, a lightweight framework that\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\n(MDFN) enhances domain feature extraction. Our framework supports multiple\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\nsuch as image classification, object detection, and speech enhancement.\nExperimental results on six benchmarks demonstrate SOYO's consistent\nsuperiority over existing baselines, showcasing its robustness and adaptability\nin complex, evolving environments. The codes will be released in\nhttps://github.com/qwangcv/SOYO."}, {"arxiv_id": "2505.24878", "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents", "authors": ["Yaxin Luo", "Zhaoyi Li", "Jiacheng Liu", "Jiacheng Cui", "Xiaohan Zhao", "Zhiqiang Shen"], "published": "2025-05-30T17:59:55Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.24878v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL."}, {"arxiv_id": "2505.24876", "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic\n  Tasks", "authors": ["Tajamul Ashraf", "Amal Saqib", "Hanan Ghani", "Muhra AlMahri", "Yuhao Li", "Noor Ahsan", "Umair Nawaz", "Jean Lahoud", "Hisham Cholakkal", "Mubarak Shah", "Philip Torr", "Fahad Shahbaz Khan", "Rao Muhammad Anwer", "Salman Khan"], "published": "2025-05-30T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.24876v1", "categories": ["cs.CV", "cs.CL"], "abstract": "Deep reasoning is fundamental for solving complex tasks, especially in\nvision-centric scenarios that demand sequential, multimodal understanding.\nHowever, existing benchmarks typically evaluate agents with fully synthetic,\nsingle-turn queries, limited visual modalities, and lack a framework to assess\nreasoning quality over multiple steps as required in real-world settings. To\naddress this, we introduce Agent-X, a large-scale benchmark for evaluating\nvision-centric agents multi-step and deep reasoning capabilities in real-world,\nmultimodal settings. Agent- X features 828 agentic tasks with authentic visual\ncontexts, including images, multi-image comparisons, videos, and instructional\ntext. These tasks span six major agentic environments: general visual\nreasoning, web browsing, security and surveillance, autonomous driving, sports,\nand math reasoning. Our benchmark requires agents to integrate tool use with\nexplicit, stepwise decision-making in these diverse settings. In addition, we\npropose a fine-grained, step-level evaluation framework that assesses the\ncorrectness and logical coherence of each reasoning step and the effectiveness\nof tool usage throughout the task. Our results reveal that even the\nbest-performing models, including GPT, Gemini, and Qwen families, struggle to\nsolve multi-step vision tasks, achieving less than 50% full-chain success.\nThese findings highlight key bottlenecks in current LMM reasoning and tool-use\ncapabilities and identify future research directions in vision-centric agentic\nreasoning models. Our data and code are publicly available at\nhttps://github.com/mbzuai-oryx/Agent-X"}, {"arxiv_id": "2505.24873", "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal", "authors": ["Bojia Zi", "Weixuan Peng", "Xianbiao Qi", "Jianan Wang", "Shihao Zhao", "Rong Xiao", "Kam-Fai Wong"], "published": "2025-05-30T17:59:45Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.24873v1", "categories": ["cs.CV"], "abstract": "Recent advances in video diffusion models have driven rapid progress in video\nediting techniques. However, video object removal, a critical subtask of video\nediting, remains challenging due to issues such as hallucinated objects and\nvisual artifacts. Furthermore, existing methods often rely on computationally\nexpensive sampling procedures and classifier-free guidance (CFG), resulting in\nslow inference. To address these limitations, we propose MiniMax-Remover, a\nnovel two-stage video object removal approach. Motivated by the observation\nthat text condition is not best suited for this task, we simplify the\npretrained video generation model by removing textual input and cross-attention\nlayers, resulting in a more lightweight and efficient model architecture in the\nfirst stage. In the second stage, we distilled our remover on successful videos\nproduced by the stage-1 model and curated by human annotators, using a minimax\noptimization strategy to further improve editing quality and inference speed.\nSpecifically, the inner maximization identifies adversarial input noise (\"bad\nnoise\") that makes failure removals, while the outer minimization step trains\nthe model to generate high-quality removal results even under such challenging\nconditions. As a result, our method achieves a state-of-the-art video object\nremoval results with as few as 6 sampling steps and doesn't rely on CFG,\nsignificantly improving inference efficiency. Extensive experiments demonstrate\nthe effectiveness and superiority of MiniMax-Remover compared to existing\nmethods. Codes and Videos are available at: https://minimax-remover.github.io."}, {"arxiv_id": "2505.24871", "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning", "authors": ["Yiqing Liang", "Jielin Qiu", "Wenhao Ding", "Zuxin Liu", "James Tompkin", "Mengdi Xu", "Mengzhou Xia", "Zhengzhong Tu", "Laixi Shi", "Jiacheng Zhu"], "published": "2025-05-30T17:59:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.24871v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline."}, {"arxiv_id": "2505.24869", "title": "SiLVR: A Simple Language-based Video Reasoning Framework", "authors": ["Ce Zhang", "Yan-Bo Lin", "Ziyang Wang", "Mohit Bansal", "Gedas Bertasius"], "published": "2025-05-30T17:59:19Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2505.24869v1", "categories": ["cs.CV"], "abstract": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR."}, {"arxiv_id": "2506.03147", "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "published": "2025-06-03T17:59:33Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.03147v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets."}, {"arxiv_id": "2506.03145", "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM", "authors": ["Pralaypati Ta", "Sriram Venkatesaperumal", "Keerthi Ram", "Mohanasankar Sivaprakasam"], "published": "2025-06-03T17:59:18Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.03145v1", "categories": ["cs.CL", "cs.AI"], "abstract": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions."}, {"arxiv_id": "2506.03144", "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "published": "2025-06-03T17:59:14Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.03144v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "abstract": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval."}, {"arxiv_id": "2506.03143", "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "published": "2025-06-03T17:59:08Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.03143v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths."}, {"arxiv_id": "2506.03142", "title": "Not All Tokens Are Meant to Be Forgotten", "authors": ["Xiangyu Zhou", "Yao Qiang", "Saleh Zare Zade", "Douglas Zytko", "Prashant Khanduri", "Dongxiao Zhu"], "published": "2025-06-03T17:59:05Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.03142v1", "categories": ["cs.LG"], "abstract": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results."}, {"arxiv_id": "2506.04227", "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos", "authors": ["Zhao-Heng Yin", "Sherry Yang", "Pieter Abbeel"], "published": "2025-06-04T17:59:06Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.04227v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "abstract": "Learning robot control policies from human videos is a promising direction\nfor scaling up robot learning. However, how to extract action knowledge (or\naction representations) from videos for policy learning remains a key\nchallenge. Existing action representations such as video frames, pixelflow, and\npointcloud flow have inherent limitations such as modeling complexity or loss\nof information. In this paper, we propose to use object-centric 3D motion field\nto represent actions for robot learning from human videos, and present a novel\nframework for extracting this representation from videos for zero-shot control.\nWe introduce two novel components in its implementation. First, a novel\ntraining pipeline for training a ''denoising'' 3D motion field estimator to\nextract fine object 3D motions from human videos with noisy depth robustly.\nSecond, a dense object-centric 3D motion field prediction architecture that\nfavors both cross-embodiment transfer and policy generalization to background.\nWe evaluate the system in real world setups. Experiments show that our method\nreduces 3D motion estimation error by over 50% compared to the latest method,\nachieve 55% average success rate in diverse tasks where prior approaches\nfail~($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills\nlike insertion."}, {"arxiv_id": "2506.04211", "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object\n  Detector", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "published": "2025-06-04T17:56:46Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.04211v1", "categories": ["cs.CV"], "abstract": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher."}, {"arxiv_id": "2506.04209", "title": "Language-Image Alignment with Fixed Text Encoders", "authors": ["Jingfeng Yang", "Ziyang Wu", "Yue Zhao", "Yi Ma"], "published": "2025-06-04T17:51:56Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.04209v1", "categories": ["cs.CV"], "abstract": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations."}, {"arxiv_id": "2506.04205", "title": "EPiC: Towards Lossless Speedup for Reasoning Training through\n  Edge-Preserving CoT Condensation", "authors": ["Jinghan Jia", "Hadi Reisizadeh", "Chongyu Fan", "Nathalie Baracaldo", "Mingyi Hong", "Sijia Liu"], "published": "2025-06-04T17:49:10Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.04205v1", "categories": ["cs.LG"], "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities\nwhen trained with chain-of-thought (CoT) supervision. However, the long and\nverbose CoT traces, especially those distilled from large reasoning models\n(LRMs) such as DeepSeek-R1, significantly increase training costs during the\ndistillation process, where a non-reasoning base model is taught to replicate\nthe reasoning behavior of an LRM. In this work, we study the problem of CoT\ncondensation for resource-efficient reasoning training, aimed at pruning\nintermediate reasoning steps (i.e., thoughts) in CoT traces, enabling\nsupervised model training on length-reduced CoT data while preserving both\nanswer accuracy and the model's ability to generate coherent reasoning. Our\nrationale is that CoT traces typically follow a three-stage structure: problem\nunderstanding, exploration, and solution convergence. Through empirical\nanalysis, we find that retaining the structure of the reasoning trace,\nespecially the early stage of problem understanding (rich in reflective cues)\nand the final stage of solution convergence, is sufficient to achieve lossless\nreasoning supervision. To this end, we propose an Edge-Preserving Condensation\nmethod, EPiC, which selectively retains only the initial and final segments of\neach CoT trace while discarding the middle portion. This design draws an\nanalogy to preserving the \"edge\" of a reasoning trajectory, capturing both the\ninitial problem framing and the final answer synthesis, to maintain logical\ncontinuity. Experiments across multiple model families (Qwen and LLaMA) and\nbenchmarks show that EPiC reduces training time by over 34% while achieving\nlossless reasoning accuracy on MATH500, comparable to full CoT supervision. To\nthe best of our knowledge, this is the first study to explore thought-level CoT\ncondensation for efficient reasoning model distillation."}, {"arxiv_id": "2506.04203", "title": "Cascadia: A Cascade Serving System for Large Language Models", "authors": ["Youhe Jiang", "Fangcheng Fu", "Wanru Zhao", "Stephan Rabanser", "Nicholas D. Lane", "Binhang Yuan"], "published": "2025-06-04T17:48:38Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.04203v1", "categories": ["cs.DC"], "abstract": "Recent advances in large language models (LLMs) have intensified the need to\ndeliver both rapid responses and high-quality answers. More powerful models\nyield better results but incur higher inference latency, whereas smaller models\nare faster yet less capable. Recent work proposes balancing this\nlatency-quality trade-off using model cascades, which route simpler queries to\nsmaller models and more complex ones to larger models. However, enabling\nefficient cascade serving remains challenging. Current frameworks lack\neffective mechanisms for handling (i) the huge and varying resource demands of\ndifferent LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the\nco-optimization of system deployment and routing strategy. Motivated by these\nobservations, we introduce Cascadia, a novel cascade serving framework designed\nexplicitly to schedule request routing and deploy model cascades for fast,\nquality-preserving LLM serving. Cascadia employs a bi-level optimization\nmethod: at the inner level, it uses a mixed-integer linear program to select\nresource allocations and parallelism strategies based on LLM information and\nworkload characteristics; at the outer level, it applies a weighted Tchebycheff\nalgorithm to iteratively co-optimize the routing strategy and the system\ndeployment produced by the inner level. Our extensive evaluation on diverse\nworkload traces and different model cascades (DeepSeek and the Llama series)\ndemonstrates that Cascadia significantly outperforms both single-model\ndeployments and the state-of-the-art cascade serving baseline, achieving up to\n4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher\nthroughput while maintaining target answer quality."}, {"arxiv_id": "2506.05344", "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs", "authors": ["Jiahui Wang", "Zuyan Liu", "Yongming Rao", "Jiwen Lu"], "published": "2025-06-05T17:59:55Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.05344v1", "categories": ["cs.CV"], "abstract": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."}, {"arxiv_id": "2506.05345", "title": "Inference-Time Hyper-Scaling with KV Cache Compression", "authors": ["Adrian Łańcucki", "Konrad Staniszewski", "Piotr Nawrot", "Edoardo M. Ponti"], "published": "2025-06-05T17:59:55Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.05345v1", "categories": ["cs.LG", "cs.CL"], "abstract": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."}, {"arxiv_id": "2506.05346", "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity\n  Analysis Between Alignment and Fine-tuning Datasets", "authors": ["Lei Hsiung", "Tianyu Pang", "Yung-Chen Tang", "Linyue Song", "Tsung-Yi Ho", "Pin-Yu Chen", "Yaoqing Yang"], "published": "2025-06-05T17:59:55Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.05346v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "abstract": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers."}, {"arxiv_id": "2506.05341", "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via\n  Spatial Reasoning", "authors": ["Xingjian Ran", "Yixuan Li", "Linning Xu", "Mulin Yu", "Bo Dai"], "published": "2025-06-05T17:59:42Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.05341v1", "categories": ["cs.CV", "cs.AI"], "abstract": "Realistic 3D indoor scene synthesis is vital for embodied AI and digital\ncontent creation. It can be naturally divided into two subtasks: object\ngeneration and layout generation. While recent generative models have\nsignificantly advanced object-level quality and controllability, layout\ngeneration remains challenging due to limited datasets. Existing methods either\noverfit to these datasets or rely on predefined constraints to optimize\nnumerical layout that sacrifice flexibility. As a result, they fail to generate\nscenes that are both open-vocabulary and aligned with fine-grained user\ninstructions. We introduce DirectLayout, a framework that directly generates\nnumerical 3D layouts from text descriptions using generalizable spatial\nreasoning of large language models (LLMs). DirectLayout decomposes the\ngeneration into three stages: producing a Bird's-Eye View (BEV) layout, lifting\nit into 3D space, and refining object placements. To enable explicit spatial\nreasoning and help the model grasp basic principles of object placement, we\nemploy Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.\nAdditionally, we design CoT-Grounded Generative Layout Reward to enhance\ngeneralization and spatial planning. During inference, DirectLayout addresses\nasset-layout mismatches via Iterative Asset-Layout Alignment through in-context\nlearning. Extensive experiments demonstrate that DirectLayout achieves\nimpressive semantic consistency, generalization and physical plausibility."}, {"arxiv_id": "2506.05337", "title": "Momentum fraction and hard scale dependence of double parton scattering", "authors": ["Joao Vitor C. Lovato", "Edgar Huayra", "Emmanuel G. de Oliveira"], "published": "2025-06-05T17:59:30Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.05337v1", "categories": ["hep-ph"], "abstract": "The effective cross section of double parton scattering in high-energy hadron\ncollisions has been measured in proton--proton collisions, with significant\nvariation among final-state observables, contrary to the idea of a universal\nvalue. Building upon our previous work, we incorporate the dependence on both\nthe parton longitudinal momentum fraction $x$ and the process energy hard scale\n$\\mu$ into the transverse part of the double parton distributions, using a\nGaussian profile. Employing the experimental data from the LHC and Tevatron\nexperiments (covering different processes, kinematic configurations, and\ncenter--of--mass energies), we perform a global fit of the model, extracting\nthe parameters that describe the proton structure. With this result, it becomes\npossible to calculate the effective cross section for others observables, and\nwe provide predictions for future measurements at the LHC."}, {"arxiv_id": "2506.06280", "title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias", "authors": ["Yuanzhe Hu", "Kinshuk Goel", "Vlad Killiakov", "Yaoqing Yang"], "published": "2025-06-06T17:59:28Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.06280v1", "categories": ["cs.LG", "cs.AI"], "abstract": "Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\nmatrices has been an active area of research in recent years. At a high level,\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\nempirical spectral densities (ESD) of weight matrices. It provides insight into\nhow well a model is trained and can guide decisions on assigning better\nlayer-wise training hyperparameters. In this paper, we address a challenge\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\nweight matrices on estimated heavytailness metrics. We demonstrate that\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\nof measuring the heavytailness of the original ESD, we measure the average ESD\nof these subsampled submatrices. We show that measuring the heavytailness of\nthese submatrices with the fixed aspect ratio can effectively mitigate the\naspect ratio bias. We validate our approach across various optimization\ntechniques and application domains that involve eigenspectrum analysis of\nweights, including image classification in computer vision (CV) models,\nscientific machine learning (SciML) model training, and large language model\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\nimproves the accuracy of eigenspectrum analysis while enabling more effective\nlayer-wise hyperparameter assignment in these application domains. In one of\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\nby 17.3% when compared with the state-of-the-art method."}, {"arxiv_id": "2506.06279", "title": "CoMemo: LVLMs Need Image Context with Image Memory", "authors": ["Shi Liu", "Weijie Su", "Xizhou Zhu", "Wenhai Wang", "Jifeng Dai"], "published": "2025-06-06T17:59:06Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.06279v1", "categories": ["cs.CV"], "abstract": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/."}, {"arxiv_id": "2506.06278", "title": "Distillation Robustifies Unlearning", "authors": ["Bruce W. Lee", "Addie Foote", "Alex Infanger", "Leni Shor", "Harish Kamath", "Jacob Goldman-Wetzler", "Bryce Woodworth", "Alex Cloud", "Alexander Matt Turner"], "published": "2025-06-06T17:58:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.06278v1", "categories": ["cs.LG", "cs.AI"], "abstract": "Current LLM unlearning methods are not robust: they can be reverted easily\nwith a few steps of finetuning. This is true even for the idealized unlearning\nmethod of training to imitate an oracle model that was never exposed to\nunwanted information, suggesting that output-based finetuning is insufficient\nto achieve robust unlearning. In a similar vein, we find that training a\nrandomly initialized student to imitate an unlearned model transfers desired\nbehaviors while leaving undesired capabilities behind. In other words,\ndistillation robustifies unlearning. Building on this insight, we propose\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\nunlearned model into a partially noised copy of itself. UNDO introduces a\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\nUNDO matches the robustness of a model retrained from scratch with perfect data\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\nthe pretraining data to be labeled. We also show that UNDO robustifies\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\nbenchmark. Since distillation is widely used in practice, incorporating an\nunlearning step beforehand offers a convenient path to robust capability\nremoval."}, {"arxiv_id": "2506.06273", "title": "AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization", "authors": ["Mukur Gupta", "Nikhil Reddy Varimalla", "Nicholas Deas", "Melanie Subbiah", "Kathleen McKeown"], "published": "2025-06-06T17:57:52Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.06273v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) have achieved impressive performance in text\nsummarization and are increasingly deployed in real-world applications.\nHowever, these systems often inherit associative and framing biases from\npre-training data, leading to inappropriate or unfair outputs in downstream\ntasks. In this work, we present AdvSumm (Adversarial Summarization), a\ndomain-agnostic training framework designed to mitigate bias in text\nsummarization through improved generalization. Inspired by adversarial\nrobustness, AdvSumm introduces a novel Perturber component that applies\ngradient-guided perturbations at the embedding level of Sequence-to-Sequence\nmodels, enhancing the model's robustness to input variations. We empirically\ndemonstrate that AdvSumm effectively reduces different types of bias in\nsummarization-specifically, name-nationality bias and political framing\nbias-without compromising summarization quality. Compared to standard\ntransformers and data augmentation techniques like back-translation, AdvSumm\nachieves stronger bias mitigation performance across benchmark datasets."}, {"arxiv_id": "2506.06265", "title": "Integrating Complexity and Biological Realism: High-Performance Spiking\n  Neural Networks for Breast Cancer Detection", "authors": ["Zofia Rudnicka", "Januszcz Szczepanski", "Agnieszka Pregowska"], "published": "2025-06-06T17:47:27Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.06265v1", "categories": ["cs.NE", "eess.IV", "q-bio.NC"], "abstract": "Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding\nof spatial and temporal features, making them suitable for dynamic\ntime-dependent data processing. Despite their biological relevance, SNNs have\nseen limited application in medical image recognition due to difficulties in\nmatching the performance of conventional deep learning models. To address this,\nwe propose a novel breast cancer classification approach that combines SNNs\nwith Lempel-Ziv Complexity (LZC) a computationally efficient measure of\nsequence complexity. LZC enhances the interpretability and accuracy of\nspike-based models by capturing structural patterns in neural activity. Our\nstudy explores both biophysical Leaky Integrate-and-Fire (LIF) and\nprobabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised,\nand hybrid learning regimes. Experiments were conducted on the Breast Cancer\nWisconsin dataset using numerical features derived from medical imaging.\nLB-based models consistently exceeded 90.00% accuracy, while LIF-based models\nreached over 85.00%. The highest accuracy of 98.25% was achieved using an\nANN-to-SNN conversion method applied to both neuron models comparable to\ntraditional deep learning with back-propagation, but at up to 100 times lower\ncomputational cost. This hybrid approach merges deep learning performance with\nthe efficiency and plausibility of SNNs, yielding top results at lower\ncomputational cost. We hypothesize that the synergy between temporal-coding,\nspike-sparsity, and LZC-driven complexity analysis enables more-efficient\nfeature extraction. Our findings demonstrate that SNNs combined with LZC offer\npromising, biologically plausible alternative to conventional neural networks\nin medical diagnostics, particularly for resource-constrained or real-time\nsystems."}, {"arxiv_id": "2506.08008", "title": "Hidden in plain sight: VLMs overlook their visual representations", "authors": ["Stephanie Fu", "Tyler Bonnen", "Devin Guillory", "Trevor Darrell"], "published": "2025-06-09T17:59:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.08008v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs."}, {"arxiv_id": "2506.08005", "title": "ZeroVO: Visual Odometry with Minimal Assumptions", "authors": ["Lei Lai", "Zekai Yin", "Eshed Ohn-Bar"], "published": "2025-06-09T17:59:51Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.08005v1", "categories": ["cs.CV"], "abstract": "We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves\nzero-shot generalization across diverse cameras and environments, overcoming\nlimitations in existing methods that depend on predefined or static camera\ncalibration setups. Our approach incorporates three main innovations. First, we\ndesign a calibration-free, geometry-aware network structure capable of handling\nnoise in estimated depth and camera parameters. Second, we introduce a\nlanguage-based prior that infuses semantic information to enhance robust\nfeature extraction and generalization to previously unseen domains. Third, we\ndevelop a flexible, semi-supervised training paradigm that iteratively adapts\nto new scenes using unlabeled data, further boosting the models' ability to\ngeneralize across diverse real-world scenarios. We analyze complex autonomous\ndriving contexts, demonstrating over 30% improvement against prior methods on\nthree standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly\nintroduced, high-fidelity synthetic dataset derived from Grand Theft Auto\n(GTA). By not requiring fine-tuning or camera calibration, our work broadens\nthe applicability of VO, providing a versatile solution for real-world\ndeployment at scale."}, {"arxiv_id": "2506.08002", "title": "Aligning Text, Images, and 3D Structure Token-by-Token", "authors": ["Aadarsh Sahoo", "Vansh Tibrewal", "Georgia Gkioxari"], "published": "2025-06-09T17:59:37Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.08002v1", "categories": ["cs.CV"], "abstract": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/"}, {"arxiv_id": "2506.08001", "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Schölkopf", "Weiyang Liu"], "published": "2025-06-09T17:59:34Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.08001v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs."}, {"arxiv_id": "2506.07997", "title": "Supporting Construction Worker Well-Being with a Multi-Agent\n  Conversational AI System", "authors": ["Fan Yang", "Yuan Tian", "Jiansong Zhang"], "published": "2025-06-09T17:58:35Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.07997v1", "categories": ["cs.HC"], "abstract": "The construction industry is characterized by both high physical and\npsychological risks, yet supports of mental health remain limited. While\nadvancements in artificial intelligence (AI), particularly large language\nmodels (LLMs), offer promising solutions, their potential in construction\nremains largely underexplored. To bridge this gap, we developed a\nconversational multi-agent system that addresses industry-specific challenges\nthrough an AI-driven approach integrated with domain knowledge. In parallel, it\nfulfills construction workers' basic psychological needs by enabling\ninteractions with multiple agents, each has a distinct persona. This approach\nensures that workers receive both practical problem-solving support and social\nengagement, ultimately contributing to their overall well-being. We evaluate\nits usability and effectiveness through a within-subjects user study with 12\nparticipants. The results show that our system significantly outperforms the\nsingle-agent baseline, achieving improvements of 18% in usability, 40% in\nself-determination, 60% in social presence, and 60% in trust. These findings\nhighlight the promise of LLM-driven AI systems in providing domain-specific\nsupport for construction workers."}, {"arxiv_id": "2506.09050", "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering", "authors": ["Yuki Imajuku", "Kohki Horie", "Yoichi Iwata", "Kensho Aoki", "Naohiro Takahashi", "Takuya Akiba"], "published": "2025-06-10T17:59:56Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09050v1", "categories": ["cs.AI"], "abstract": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements."}, {"arxiv_id": "2506.09049", "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement\n  Learning", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "published": "2025-06-10T17:59:44Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09049v1", "categories": ["cs.AI", "cs.CV", "cs.RO"], "abstract": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems."}, {"arxiv_id": "2506.09048", "title": "Understanding Task Vectors in In-Context Learning: Emergence,\n  Functionality, and Limitations", "authors": ["Yuxin Dong", "Jiachen Jiang", "Zhihui Zhu", "Xia Ning"], "published": "2025-06-10T17:59:31Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09048v1", "categories": ["cs.LG"], "abstract": "Task vectors offer a compelling mechanism for accelerating inference in\nin-context learning (ICL) by distilling task-specific information into a\nsingle, reusable representation. Despite their empirical success, the\nunderlying principles governing their emergence and functionality remain\nunclear. This work proposes the Linear Combination Conjecture, positing that\ntask vectors act as single in-context demonstrations formed through linear\ncombinations of the original ones. We provide both theoretical and empirical\nsupport for this conjecture. First, we show that task vectors naturally emerge\nin linear transformers trained on triplet-formatted prompts through loss\nlandscape analysis. Next, we predict the failure of task vectors on\nrepresenting high-rank mappings and confirm this on practical LLMs. Our\nfindings are further validated through saliency analyses and parameter\nvisualization, suggesting an enhancement of task vectors by injecting multiple\nones into few-shot prompts. Together, our results advance the understanding of\ntask vectors and shed light on the mechanisms underlying ICL in\ntransformer-based models."}, {"arxiv_id": "2506.09046", "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual\n  Backpropagation", "authors": ["Xiaowen Ma", "Chenyang Lin", "Yao Zhang", "Volker Tresp", "Yunpu Ma"], "published": "2025-06-10T17:59:21Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09046v1", "categories": ["cs.LG", "cs.AI", "cs.MA"], "abstract": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework."}, {"arxiv_id": "2506.09040", "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "published": "2025-06-10T17:57:50Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09040v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR."}, {"arxiv_id": "2506.09996", "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via\n  Streaming Content Monitoring", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "published": "2025-06-11T17:59:58Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09996v1", "categories": ["cs.CL", "cs.CY"], "abstract": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO."}, {"arxiv_id": "2506.09998", "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized\n  Rejection Sampling", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Schölkopf"], "published": "2025-06-11T17:59:58Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09998v1", "categories": ["cs.LG", "cs.CL"], "abstract": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering."}, {"arxiv_id": "2506.09995", "title": "PlayerOne: Egocentric World Simulator", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "published": "2025-06-11T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09995v1", "categories": ["cs.CV"], "abstract": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications."}, {"arxiv_id": "2506.09993", "title": "Text-Aware Image Restoration with Diffusion Models", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "published": "2025-06-11T17:59:46Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09993v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/"}, {"arxiv_id": "2506.09991", "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation", "authors": ["Xinyu Yang", "Yuwei An", "Hongyi Liu", "Tianqi Chen", "Beidi Chen"], "published": "2025-06-11T17:59:23Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.09991v1", "categories": ["cs.LG"], "abstract": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes."}, {"arxiv_id": "2506.10979", "title": "How Well Can Reasoning Models Identify and Recover from Unhelpful\n  Thoughts?", "authors": ["Sohee Yang", "Sang-Woo Lee", "Nora Kassner", "Daniela Gottesman", "Sebastian Riedel", "Mor Geva"], "published": "2025-06-12T17:59:53Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.10979v1", "categories": ["cs.CL"], "abstract": "Recent reasoning models show the ability to reflect, backtrack, and\nself-validate their reasoning, which is crucial in spotting mistakes and\narriving at accurate solutions. A natural question that arises is how\neffectively models can perform such self-reevaluation. We tackle this question\nby investigating how well reasoning models identify and recover from four types\nof unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to\nthe question, thoughts misdirecting the question as a slightly different\nquestion, and thoughts that lead to incorrect answers. We show that models are\neffective at identifying most unhelpful thoughts but struggle to recover from\nthe same thoughts when these are injected into their thinking process, causing\nsignificant performance drops. Models tend to naively continue the line of\nreasoning of the injected irrelevant thoughts, which showcases that their\nself-reevaluation abilities are far from a general \"meta-cognitive\" awareness.\nMoreover, we observe non/inverse-scaling trends, where larger models struggle\nmore than smaller ones to recover from short irrelevant thoughts, even when\ninstructed to reevaluate their reasoning. We demonstrate the implications of\nthese findings with a jailbreak experiment using irrelevant thought injection,\nshowing that the smallest models are the least distracted by\nharmful-response-triggering thoughts. Overall, our findings call for\nimprovement in self-reevaluation of reasoning models to develop better\nreasoning and safer systems."}, {"arxiv_id": "2506.10974", "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science", "authors": ["Yixin Ou", "Yujie Luo", "Jingsheng Zheng", "Lanning Wei", "Shuofei Qiao", "Jintian Zhang", "Da Zheng", "Huajun Chen", "Ningyu Zhang"], "published": "2025-06-12T17:59:32Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.10974v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "abstract": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science."}, {"arxiv_id": "2506.10972", "title": "Farseer: A Refined Scaling Law in Large Language Models", "authors": ["Houyi Li", "Wenzhen Zheng", "Qiufeng Wang", "Zhenyu Ding", "Haoying Wang", "Zili Wang", "Shijie Xuyang", "Ning Ding", "Shuigeng Zhou", "Xiangyu Zhang", "Daxin Jiang"], "published": "2025-06-12T17:59:23Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.10972v1", "categories": ["cs.LG", "cs.AI", "I.2"], "abstract": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research."}, {"arxiv_id": "2506.10966", "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following\n  Manipulation", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "published": "2025-06-12T17:59:04Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.10966v1", "categories": ["cs.RO"], "abstract": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/."}, {"arxiv_id": "2506.10963", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for\n  Text-to-Image Reasoning", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "published": "2025-06-12T17:58:09Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2506.10963v1", "categories": ["cs.CV", "cs.CL"], "abstract": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs."}];
    let currentSort = 'date';
    
    function formatDate(dateString) {
        if (!dateString) return "Unknown date";
        const match = dateString.match(/(\d{4}-\d{2}-\d{2})/);
        return match ? match[1] : "Unknown date";
    }
    
    function formatAuthors(authors, full = false) {
        if (!authors || authors.length === 0) return "Unknown authors";
        if (!full && authors.length > 3) {
            return authors.slice(0, 3).join(', ') + ' et al.';
        }
        return authors.join(', ');
    }
    
    function formatCategories(categories) {
        if (!categories || categories.length === 0) return "";
        return categories.join(', ');
    }
    
    function toggleDetails(id) {
        const details = document.getElementById(`paper-details-${id}`);
        if (details.classList.contains('show')) {
            details.classList.remove('show');
        } else {
            details.classList.add('show');
        }
    }
    
    function expandAll() {
        document.querySelectorAll('.paper-details').forEach(el => {
            el.classList.add('show');
        });
    }
    
    function collapseAll() {
        document.querySelectorAll('.paper-details').forEach(el => {
            el.classList.remove('show');
        });
    }
    
    function sortPapers(sortMethod) {
        document.getElementById('sort-date').classList.remove('active');
        document.getElementById('sort-citations').classList.remove('active');
        document.getElementById('sort-tweets').classList.remove('active');
        document.getElementById('sort-' + sortMethod).classList.add('active');
        
        let sortedPapers = [...papers];
        if (sortMethod === 'date') {
            sortedPapers.sort((a, b) => (b.published || '').localeCompare(a.published || ''));
        } else if (sortMethod === 'citations') {
            sortedPapers.sort((a, b) => (b.citations || 0) - (a.citations || 0));
        } else if (sortMethod === 'tweets') {
            sortedPapers.sort((a, b) => (b.tweets || 0) - (a.tweets || 0));
        }
        
        currentSort = sortMethod;
        
        let html = '';
        sortedPapers.forEach((paper, index) => {
            html += `
            <div class="paper-row">
                <div class="paper-main" onclick="toggleDetails(${index})">
                    <div class="rank">${index + 1}</div>
                    <div class="votes">
                        <a href="https://x.com/search?q=${paper.arxiv_id}&src=typed_query&f=top" target="_blank" style="text-decoration:none" onclick="event.stopPropagation()">
                            <strong>${paper.tweets}</strong>
                            <span>tweets</span>
                        </a>
                    </div>
                    <div class="paper-content">
                        <a href="${paper.paper_link}" class="paper-title" target="_blank" onclick="event.stopPropagation()">${paper.title}</a>
                        <div class="paper-meta">
                            ${formatDate(paper.published)} | ${formatAuthors(paper.authors)} | 📚 ${paper.citations} citations
                        </div>
                    </div>
                </div>
                <div class="paper-details" id="paper-details-${index}">
                    <div><strong>Categories:</strong> ${formatCategories(paper.categories)}</div>
                    <div class="abstract">${paper.abstract || 'No abstract available'}</div>
                </div>
            </div>
            `;
        });
        
        document.getElementById('papers-container').innerHTML = html;
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        sortPapers('tweets');
    });
    </script>
</body>
</html>