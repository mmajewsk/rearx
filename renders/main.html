<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv AI Security Papers</title>
    <style>
        body { font-family: Verdana, sans-serif; margin: 0; padding: 0; background-color: #eee; }
        .container { max-width: 950px; margin: 0 auto; padding: 20px; background-color: #fff; }
        .header { background-color: #cee3f8; border-bottom: 1px solid #5f99cf; padding: 10px 20px; margin-bottom: 20px; }
        .header h1 { margin: 0; font-size: 20px; color: #369; }
        .filter-options { display: flex; justify-content: space-between; margin-bottom: 20px; padding: 10px; background-color: #f8f8f8; border: 1px solid #ddd; }
        .filter-options button { color: #369; cursor: pointer; padding: 5px 10px; background: none; border: none; }
        .filter-options button.active { font-weight: bold; background-color: #e2e2e2; border-radius: 3px; }
        .paper-row { padding: 10px; border-bottom: 1px solid #ddd; line-height: 1.4; }
        .paper-main { display: flex; align-items: center; cursor: pointer; }
        .rank { flex: 0 0 30px; color: #888; text-align: right; padding-right: 10px; font-size: 18px; }
        .votes { flex: 0 0 70px; text-align: center; padding: 0 10px; display: flex; flex-direction: column; justify-content: center; align-items: center; }
        .votes strong { color: #1DA1F2; font-size: 15px; }
        .votes a { color: inherit; transition: transform 0.2s; display: flex; flex-direction: column; align-items: center; }
        .votes a:hover { transform: scale(1.1); }
        .paper-content { flex: 1; display: flex; flex-direction: column; }
        .paper-title { color: #0000ff; text-decoration: none; font-weight: bold; font-size: 16px; }
        .paper-meta { color: #888; font-size: 12px; margin-top: 4px; }
        .paper-details { margin-top: 10px; padding: 10px; background-color: #f9f9f9; border-radius: 5px; display: none; }
        .paper-details.show { display: block; }
        .abstract { font-size: 14px; line-height: 1.5; margin-top: 10px; white-space: pre-line; }
        .footer { text-align: center; margin-top: 20px; font-size: 12px; color: #888; padding: 10px; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ArXiv AI Security Papers</h1>
        </div>
        <div class="filter-options">
            <div>
                <button onclick="sortPapers('date')" id="sort-date">Recent</button>
                <button onclick="sortPapers('citations')" id="sort-citations">Most Cited</button>
                <button onclick="sortPapers('tweets')" class="active" id="sort-tweets">Most Tweeted</button>
            </div>
            <div>
                <button onclick="expandAll()" id="expand-all">Expand All</button>
                <button onclick="collapseAll()" id="collapse-all">Collapse All</button>
                <a href="list.html" style="margin-left: 20px; background-color: #5f99cf; color: white; padding: 5px 10px; text-decoration: none; border-radius: 3px;">See Previous Renders</a>
            </div>
        </div>
        
        <div id="papers-container"></div>
        
        <div class="footer">
            Generated on 2025-03-23 15:14:23 | Database contains 20 papers total.
            <div style="margin-top: 10px;">
                <a href="list.html" style="color: #369; text-decoration: none;">View render history</a>
            </div>
        </div>
    </div>
    
    <script>
    const papers = [{"arxiv_id": "2503.16426", "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding", "authors": ["Keyan Chen", "Chenyang Liu", "Bowen Chen", "Wenyuan Li", "Zhengxia Zou", "Zhenwei Shi"], "published": "2025-03-20T17:59:54Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16426v1", "categories": ["cs.CV"], "abstract": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's)."}, {"arxiv_id": "2503.16419", "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", " Shaochen", " Zhong", "Hanjie Chen", "Xia Hu"], "published": "2025-03-20T17:59:38Z", "citations": 0, "tweets": 15, "paper_link": "http://arxiv.org/abs/2503.16419v1", "categories": ["cs.CL"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."}, {"arxiv_id": "2503.16418", "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity", "authors": ["Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Hao Kang", "Xin Lu"], "published": "2025-03-20T17:59:34Z", "citations": 0, "tweets": 8, "paper_link": "http://arxiv.org/abs/2503.16418v1", "categories": ["cs.CV", "cs.LG"], "abstract": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community."}, {"arxiv_id": "2503.16416", "title": "Survey on Evaluation of LLM-based Agents", "authors": ["Asaf Yehudai", "Lilach Eden", "Alan Li", "Guy Uziel", "Yilun Zhao", "Roy Bar-Haim", "Arman Cohan", "Michal Shmueli-Scheuer"], "published": "2025-03-20T17:59:23Z", "citations": 0, "tweets": 12, "paper_link": "http://arxiv.org/abs/2503.16416v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch."}, {"arxiv_id": "2503.16412", "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation", "authors": ["Ananta R. Bhattarai", "Xingzhe He", "Alla Sheffer", "Helge Rhodin"], "published": "2025-03-20T17:59:12Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16412v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation."}, {"arxiv_id": "2503.16413", "title": "M3: 3D-Spatial MultiModal Memory", "authors": ["Xueyan Zou", "Yuchen Song", "Ri-Zhao Qiu", "Xuanbin Peng", "Jianglong Ye", "Sifei Liu", "Xiaolong Wang"], "published": "2025-03-20T17:59:12Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16413v1", "categories": ["cs.CV", "cs.RO"], "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation."}, {"arxiv_id": "2503.16402", "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination", "authors": ["Yifan Sun", "Han Wang", "Dongbai Li", "Gang Wang", "Huan Zhang"], "published": "2025-03-20T17:55:04Z", "citations": 0, "tweets": 0, "paper_link": "http://arxiv.org/abs/2503.16402v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment."}, {"arxiv_id": "2503.16401", "title": "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them", "authors": ["Guanyu Chen", "Peiyang Wang", "Tianren Zhang", "Feng Chen"], "published": "2025-03-20T17:54:42Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.16401v1", "categories": ["cs.LG"], "abstract": "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning."}, {"arxiv_id": "2503.16399", "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World", "authors": ["Chen Chen", "Zhirui Wang", "Taowei Sheng", "Yi Jiang", "Yundu Li", "Peirui Cheng", "Luning Zhang", "Kaiqiang Chen", "Yanfeng Hu", "Xue Yang", "Xian Sun"], "published": "2025-03-20T17:54:29Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16399v1", "categories": ["cs.CV", "cs.AI"], "abstract": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ."}, {"arxiv_id": "2503.16392", "title": "Graph of Effort: Quantifying Risk of AI Usage for Vulnerability\n  Assessment", "authors": ["Anket Mehra", "Andreas Aßmuth", "Malte Prieß"], "published": "2025-03-20T17:52:42Z", "citations": 0, "tweets": 7, "paper_link": "http://arxiv.org/abs/2503.16392v1", "categories": ["cs.CR", "cs.AI", "cs.DC"], "abstract": "With AI-based software becoming widely available, the risk of exploiting its\ncapabilities, such as high automation and complex pattern recognition, could\nsignificantly increase. An AI used offensively to attack non-AI assets is\nreferred to as offensive AI.\n  Current research explores how offensive AI can be utilized and how its usage\ncan be classified. Additionally, methods for threat modeling are being\ndeveloped for AI-based assets within organizations. However, there are gaps\nthat need to be addressed. Firstly, there is a need to quantify the factors\ncontributing to the AI threat. Secondly, there is a requirement to create\nthreat models that analyze the risk of being attacked by AI for vulnerability\nassessment across all assets of an organization. This is particularly crucial\nand challenging in cloud environments, where sophisticated infrastructure and\naccess control landscapes are prevalent. The ability to quantify and further\nanalyze the threat posed by offensive AI enables analysts to rank\nvulnerabilities and prioritize the implementation of proactive countermeasures.\n  To address these gaps, this paper introduces the Graph of Effort, an\nintuitive, flexible, and effective threat modeling method for analyzing the\neffort required to use offensive AI for vulnerability exploitation by an\nadversary. While the threat model is functional and provides valuable support,\nits design choices need further empirical validation in future work."}, {"arxiv_id": "2503.16390", "title": "Phonons in Electron Crystals with Berry Curvature", "authors": ["Junkai Dong", "Ophelia Evelyn Sommer", "Tomohiro Soejima", "Daniel E. Parker", "Ashvin Vishwanath"], "published": "2025-03-20T17:49:59Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16390v1", "categories": ["cond-mat.str-el", "cond-mat.mes-hall"], "abstract": "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology."}, {"arxiv_id": "2503.16385", "title": "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation", "authors": ["Yijia Luo", "Yulin Song", "Xingyao Zhang", "Jiaheng Liu", "Weixun Wang", "GengRu Chen", "Wenbo Su", "Bo Zheng"], "published": "2025-03-20T17:46:38Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16385v1", "categories": ["cs.AI"], "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs."}, {"arxiv_id": "2503.16376", "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images", "authors": ["Leyang Wang", "Joice Lin"], "published": "2025-03-20T17:39:06Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16376v1", "categories": ["cs.CV"], "abstract": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG."}, {"arxiv_id": "2503.16366", "title": "Dynamic Metasurface-Backed Luneburg Lens for Multiplexed Backscatter\n  Communication", "authors": ["Samuel Kim", "Tim Sleasman", "Avrami Rakovsky", "Ra'id Awadallah", "David B. Shrekenhamer"], "published": "2025-03-20T17:22:11Z", "citations": 0, "tweets": 4, "paper_link": "http://arxiv.org/abs/2503.16366v1", "categories": ["physics.optics", "eess.SP"], "abstract": "Backscatter communications is attractive for its low power requirements due\nto the lack of actively radiating components; however, commonly used devices\nare typically limited in range and functionality. Here, we design and\ndemonstrate a flattened Luneburg lens combined with a spatially-tunable dynamic\nmetasurface to create a low-power backscatter communicator. The Luneburg lens\nis a spherically-symmetric lens that focuses a collimated beam from any\ndirection, enabling a wide field-of-view with no aberrations. By applying\nquasi-conformal transformation optics (QCTO), we design a flattened Luneburg\nlens to facilitate its seamless interface with the planar metasurface. The\ngradient index of the Luneburg lens is realized through additive manufacturing.\nWe show that the flattened Luneburg lens with a reflective surface at the\nflattened focal plane is able to achieve diffraction-limited retroreflection,\nenabling long-range backscatter communication. When an interrogator transmits\ntowards the metasurface-backed Luneburg lens, the device can modulate the\nreflected signal phase across a wide field of regard to communicate data. We\nexperimentally show that the spatial control over the metasurface allows\ndifferent bit streams to be simultaneously communicated in different\ndirections. Additionally, we show that the device is able to prevent\neavesdroppers from receiving information, thus securing communications."}, {"arxiv_id": "2503.16356", "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners", "authors": ["Yunzhi Yao", "Jizhan Fang", "Jia-Chen Gu", "Ningyu Zhang", "Shumin Deng", "Huajun Chen", "Nanyun Peng"], "published": "2025-03-20T17:14:34Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16356v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."}, {"arxiv_id": "2503.16350", "title": "An Evaluation Tool for Backbone Extraction Techniques in Weighted\n  Complex Networks", "authors": ["Ali Yassin", "Abbas Haidar", "Hocine Cherifi", "Hamida Seba", "Olivier Togni"], "published": "2025-03-20T17:08:35Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16350v1", "categories": ["cs.SI", "cs.SE"], "abstract": "Networks are essential for analyzing complex systems. However, their growing\nsize necessitates backbone extraction techniques aimed at reducing their size\nwhile retaining critical features. In practice, selecting, implementing, and\nevaluating the most suitable backbone extraction method may be challenging.\nThis paper introduces netbone, a Python package designed for assessing the\nperformance of backbone extraction techniques in weighted networks. Its\ncomparison framework is the standout feature of netbone. Indeed, the tool\nincorporates state-of-the-art backbone extraction techniques. Furthermore, it\nprovides a comprehensive suite of evaluation metrics allowing users to evaluate\ndifferent backbones techniques. We illustrate the flexibility and effectiveness\nof netbone through the US air transportation network analysis. We compare the\nperformance of different backbone extraction techniques using the evaluation\nmetrics. We also show how users can integrate a new backbone extraction method\ninto the comparison framework. netbone is publicly available as an open-source\ntool, ensuring its accessibility to researchers and practitioners. Promoting\nstandardized evaluation practices contributes to the advancement of backbone\nextraction techniques and fosters reproducibility and comparability in research\nefforts. We anticipate that netbone will serve as a valuable resource for\nresearchers and practitioners enabling them to make informed decisions when\nselecting backbone extraction techniques to gain insights into the structural\nand functional properties of complex systems."}, {"arxiv_id": "2503.16345", "title": "Finite-coupling spectrum of O(N) model in AdS", "authors": ["Jonáš Dujava", "Petr Vaško"], "published": "2025-03-20T17:01:22Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16345v1", "categories": ["hep-th"], "abstract": "We determine the scaling dimensions in the boundary $\\mathsf{CFT}_{d}$\ncorresponding to the $\\mathsf{O}(N)$ model in $\\mathsf{EAdS}_{d+1}$. The\n$\\mathsf{CFT}$ data accessible to the 4-point boundary correlator of\nfundamental fields are extracted in $d=2$ and $d=4$, at a finite coupling, and\nto the leading nontrivial order in the $1/N$ expansion. We focus on the\nnon-singlet sectors, namely the anti-symmetric and symmetric traceless\nirreducible representations of the $\\mathsf{O}(N)$ group, extending the\nprevious results that considered only the singlet sector. Studying the\nnon-singlet sector requires an understanding of the crossed-channel diagram\ncontributions to the $s$-channel conformal block decomposition. Building upon\nan existing computation, we present general formulas in $d=2$ and $d=4$ for the\ncontribution of a $t$-channel conformal block to the anomalous dimensions of\n$s$-channel double-twist operators, derived for external scalar operators with\nequal scaling dimensions. Up to some technical details, this eventually leads\nto the complete picture of $1/N$ corrections to the $\\mathsf{CFT}$ data in the\ninteracting theory."}, {"arxiv_id": "2503.16335", "title": "Enhancing Software Quality Assurance with an Adaptive Differential\n  Evolution based Quantum Variational Autoencoder-Transformer Model", "authors": ["Seshu Babu Barma", "Mohanakrishnan Hariharan", "Satish Arvapalli"], "published": "2025-03-20T16:55:38Z", "citations": 0, "tweets": 3, "paper_link": "http://arxiv.org/abs/2503.16335v1", "categories": ["cs.AI", "cs.ET"], "abstract": "An AI-powered quality engineering platform uses artificial intelligence to\nboost software quality assessments through automated defect prediction and\noptimized performance alongside improved feature extraction. Existing models\nresult in difficulties addressing noisy data types together with imbalances,\npattern recognition complexities, ineffective feature extraction, and\ngeneralization weaknesses. To overcome those existing challenges in this\nresearch, we develop a new model Adaptive Differential Evolution based Quantum\nVariational Autoencoder-Transformer Model (ADE-QVAET), that combines a Quantum\nVariational Autoencoder-Transformer (QVAET) to obtain high-dimensional latent\nfeatures and maintain sequential dependencies together with contextual\nrelationships, resulting in superior defect prediction accuracy. Adaptive\nDifferential Evolution (ADE) Optimization utilizes an adaptive parameter tuning\nmethod that enhances model convergence and predictive performance. ADE-QVAET\nintegrates advanced AI techniques to create a robust solution for scalable and\naccurate software defect prediction that represents a top-level AI-driven\ntechnology for quality engineering applications. The proposed ADE-QVAET model\nattains high accuracy, precision, recall, and f1-score during the training\npercentage (TP) 90 of 98.08%, 92.45%, 94.67%, and 98.12%."}, {"arxiv_id": "2503.16334", "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "authors": ["Ying Shen", "Lifu Huang"], "published": "2025-03-20T16:55:26Z", "citations": 0, "tweets": 2, "paper_link": "http://arxiv.org/abs/2503.16334v1", "categories": ["cs.CL"], "abstract": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."}, {"arxiv_id": "2503.16326", "title": "OmniGeo: Towards a Multimodal Large Language Models for Geospatial\n  Artificial Intelligence", "authors": ["Long Yuan", "Fengran Mo", "Kaiyu Huang", "Wenjie Wang", "Wangyuxuan Zhai", "Xiaoyu Zhu", "You Li", "Jinan Xu", "Jian-Yun Nie"], "published": "2025-03-20T16:45:48Z", "citations": 0, "tweets": 1, "paper_link": "http://arxiv.org/abs/2503.16326v1", "categories": ["cs.AI"], "abstract": "The rapid advancement of multimodal large language models (LLMs) has opened\nnew frontiers in artificial intelligence, enabling the integration of diverse\nlarge-scale data types such as text, images, and spatial information. In this\npaper, we explore the potential of multimodal LLMs (MLLM) for geospatial\nartificial intelligence (GeoAI), a field that leverages spatial data to address\nchallenges in domains including Geospatial Semantics, Health Geography, Urban\nGeography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)\ntailored to geospatial applications, capable of processing and analyzing\nheterogeneous data sources, including satellite imagery, geospatial metadata,\nand textual descriptions. By combining the strengths of natural language\nunderstanding and spatial reasoning, our model enhances the ability of\ninstruction following and the accuracy of GeoAI systems. Results demonstrate\nthat our model outperforms task-specific models and existing LLMs on diverse\ngeospatial tasks, effectively addressing the multimodality nature while\nachieving competitive results on the zero-shot geospatial tasks. Our code will\nbe released after publication."}];
    let currentSort = 'date';
    
    function formatDate(dateString) {
        if (!dateString) return "Unknown date";
        const match = dateString.match(/(\d{4}-\d{2}-\d{2})/);
        return match ? match[1] : "Unknown date";
    }
    
    function formatAuthors(authors, full = false) {
        if (!authors || authors.length === 0) return "Unknown authors";
        if (!full && authors.length > 3) {
            return authors.slice(0, 3).join(', ') + ' et al.';
        }
        return authors.join(', ');
    }
    
    function formatCategories(categories) {
        if (!categories || categories.length === 0) return "";
        return categories.join(', ');
    }
    
    function toggleDetails(id) {
        const details = document.getElementById(`paper-details-${id}`);
        if (details.classList.contains('show')) {
            details.classList.remove('show');
        } else {
            details.classList.add('show');
        }
    }
    
    function expandAll() {
        document.querySelectorAll('.paper-details').forEach(el => {
            el.classList.add('show');
        });
    }
    
    function collapseAll() {
        document.querySelectorAll('.paper-details').forEach(el => {
            el.classList.remove('show');
        });
    }
    
    function sortPapers(sortMethod) {
        document.getElementById('sort-date').classList.remove('active');
        document.getElementById('sort-citations').classList.remove('active');
        document.getElementById('sort-tweets').classList.remove('active');
        document.getElementById('sort-' + sortMethod).classList.add('active');
        
        let sortedPapers = [...papers];
        if (sortMethod === 'date') {
            sortedPapers.sort((a, b) => (b.published || '').localeCompare(a.published || ''));
        } else if (sortMethod === 'citations') {
            sortedPapers.sort((a, b) => (b.citations || 0) - (a.citations || 0));
        } else if (sortMethod === 'tweets') {
            sortedPapers.sort((a, b) => (b.tweets || 0) - (a.tweets || 0));
        }
        
        currentSort = sortMethod;
        
        let html = '';
        sortedPapers.forEach((paper, index) => {
            html += `
            <div class="paper-row">
                <div class="paper-main" onclick="toggleDetails(${index})">
                    <div class="rank">${index + 1}</div>
                    <div class="votes">
                        <a href="https://x.com/search?q=${paper.arxiv_id}&src=typed_query&f=top" target="_blank" style="text-decoration:none" onclick="event.stopPropagation()">
                            <strong>${paper.tweets}</strong>
                            <span>tweets</span>
                        </a>
                    </div>
                    <div class="paper-content">
                        <a href="${paper.paper_link}" class="paper-title" target="_blank" onclick="event.stopPropagation()">${paper.title}</a>
                        <div class="paper-meta">
                            ${formatDate(paper.published)} | ${formatAuthors(paper.authors)} | 📚 ${paper.citations} citations
                        </div>
                    </div>
                </div>
                <div class="paper-details" id="paper-details-${index}">
                    <div><strong>Categories:</strong> ${formatCategories(paper.categories)}</div>
                    <div class="abstract">${paper.abstract || 'No abstract available'}</div>
                </div>
            </div>
            `;
        });
        
        document.getElementById('papers-container').innerHTML = html;
    }
    
    document.addEventListener('DOMContentLoaded', function() {
        sortPapers('tweets');
    });
    </script>
</body>
</html>